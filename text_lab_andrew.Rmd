---
title: "Text_lab_Andrew"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidyverse)
#install.packages("tidytext")
library(tidytext)
#install.packages("ggwordcloud")
library(ggwordcloud)
install.packages("gutenbergr") 
library(gutenbergr)
#install.packages('textdata')
library(textdata)
library(striprtf)
library(tibble)
library(dplyr)
```

```{r}
# Read the files into the code
LA_Times<- read_rtf("Los_Angeles_Times_Articles.RTF", skip = 0)
Eurasia<- read_rtf("Eurasia_Review_articles.RTF", skip = 0)
Spokesman<- read_rtf("Spokesman_Review_Articles.RTF", skip = 0)
Combined<- read_rtf("Combined_West_Coast_Articles.RTF", skip = 0)

#Convert the RTF files into tibbles that can be analyzed
LA_Times<- tibble(LA_Times)
Eurasia<- tibble(Eurasia)
Spokesman <- tibble(Spokesman)
Combined <- tibble(Combined)

# View the tibbles to make sure they convert correctly
View(LA_Times)
View(Eurasia)
View(Spokesman)
View(Combined)

#Coerce the arguments to character types that are stripped of attributes
LA_Times$LA_Times <- as.character(LA_Times$LA_Times)
Eurasia$Eurasia <- as.character(Eurasia$Eurasia)
Spokesman$Spokesman <- as.character(Spokesman$Spokesman)
Combined$Combined <- as.character(Combined$Combined)

#View the tibbles again
View(LA_Times)
View(Eurasia)
View(Spokesman)
View(Combined)

#Split the columns into tokens
LA_Times <- LA_Times %>%
  unnest_tokens(word, LA_Times)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)

Eurasia <- Eurasia %>%
  unnest_tokens(word, Eurasia)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)

Spokesman <- Spokesman %>%
  unnest_tokens(word, Spokesman)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)

Combined <- Combined %>%
  unnest_tokens(word, Combined)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)

#View the files
View(LA_Times)
View(Eurasia)
View(Spokesman)
View(Combined)

```

```{r}
get_sentiments('afinn')# we see a list of words and there classification, 2,467 - not really that many overall. 

get_sentiments('nrc')# looks like a good amount more 13,891, but as we can see words are classified in several different categories. 

get_sentiments('bing')# looks like a good amount more 6,776, but as we can see just negative and positive. 

LA_sentiment_affin <- LA_Times %>%
  inner_join(get_sentiments("afinn"))#using a inner join to match words and add the sentiment variable

LA_sentiment_nrc <- LA_Times %>%
  inner_join(get_sentiments("nrc"))

LA_sentiment_bing <- LA_Times %>%
  inner_join(get_sentiments("bing"))

Spokesman_sentiment_affin <- Spokesman %>%
  inner_join(get_sentiments("afinn"))#using a inner join to match words and add the sentiment variable

Spokesman_sentiment_nrc <- Spokesman %>%
  inner_join(get_sentiments("nrc"))

Spokesman_sentiment_bing <- Spokesman %>%
  inner_join(get_sentiments("bing"))

View(LA_sentiment_bing)
View(Spokesman_sentiment_bing)

```

```{r}
table(LA_sentiment_bing$sentiment)

ggplot(data = LA_sentiment_affin, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("LA Times Sentiment Range")+
  theme_minimal()


set.seed(42)
ggplot(LA_Times[1:50,], aes(label = word, size = n)
       ) +
  geom_text_wordcloud() +
  theme_minimal()
```

  git config --global user.email "asp3ud@virginia.edu"
  git config --global user.name "andrewporter1212"
