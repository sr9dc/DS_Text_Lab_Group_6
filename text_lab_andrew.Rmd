---
title: "Text_lab_Andrew"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidyverse)
#install.packages("tidytext")
library(tidytext)
#install.packages("ggwordcloud")
library(ggwordcloud)
install.packages("gutenbergr") 
library(gutenbergr)
#install.packages('textdata')
library(textdata)
library(striprtf)
library(tibble)
library(dplyr)
```

```{r}
# Read the files into the code
LA_Times<- readLines("LA_Times.txt")
Eurasia<- readLines("Eurasia_Review.txt")
Spokesman<- readLines("Spokesman_Review.txt")
Combined<- readLines("West_Coast.txt")


#Convert the RTF files into tibbles that can be analyzed
LA_Times<- tibble(LA_Times)
Eurasia<- tibble(Eurasia)
Spokesman <- tibble(Spokesman)
Combined <- tibble(Combined)

# View the tibbles to make sure they convert correctly
View(LA_Times)
View(Eurasia)
View(Spokesman)
View(Combined)

#Coerce the arguments to character types that are stripped of attributes
LA_Times$LA_Times <- as.character(LA_Times$LA_Times)
Eurasia$Eurasia <- as.character(Eurasia$Eurasia)
Spokesman$Spokesman <- as.character(Spokesman$Spokesman)
Combined$Combined <- as.character(Combined$Combined)

#View the tibbles again
View(LA_Times)
View(Eurasia)
View(Spokesman)
View(Combined)

#Split the columns into tokens
LA_Times <- LA_Times %>%
  unnest_tokens(word, LA_Times)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)

Eurasia <- Eurasia %>%
  unnest_tokens(word, Eurasia)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)

Spokesman <- Spokesman %>%
  unnest_tokens(word, Spokesman)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)

Combined <- Combined %>%
  unnest_tokens(word, Combined)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)


#View the files
View(LA_Times)
View(Eurasia)
View(Spokesman)
View(Combined)

```

```{r}
get_sentiments('afinn')# we see a list of words and there classification, 2,467 - not really that many overall. 

get_sentiments('nrc')# looks like a good amount more 13,891, but as we can see words are classified in several different categories. 

get_sentiments('bing')# looks like a good amount more 6,776, but as we can see just negative and positive. 

LA_sentiment_affin <- LA_Times %>%
  inner_join(get_sentiments("afinn"))#using a inner join to match words and add the sentiment variable

LA_sentiment_nrc <- LA_Times %>%
  inner_join(get_sentiments("nrc"))

LA_sentiment_bing <- LA_Times %>%
  inner_join(get_sentiments("bing"))

Spokesman_sentiment_affin <- Spokesman %>%
  inner_join(get_sentiments("afinn"))#using a inner join to match words and add the sentiment variable

Spokesman_sentiment_nrc <- Spokesman %>%
  inner_join(get_sentiments("nrc"))

Spokesman_sentiment_bing <- Spokesman %>%
  inner_join(get_sentiments("bing"))

Eurasia_sentiment_affin <- Eurasia %>%
  inner_join(get_sentiments("afinn"))#using a inner join to match words and add the sentiment variable

Eurasia_sentiment_nrc <- Eurasia %>%
  inner_join(get_sentiments("nrc"))

Eurasia_sentiment_bing <- Eurasia %>%
  inner_join(get_sentiments("bing"))

Combined_sentiment_affin <- Combined %>%
  inner_join(get_sentiments("afinn"))#using a inner join to match words and add the sentiment variable

Combined_sentiment_nrc <- Combined %>%
  inner_join(get_sentiments("nrc"))

Combined_sentiment_bing <- Combined %>%
  inner_join(get_sentiments("bing"))



View(LA_sentiment_bing)
View(Spokesman_sentiment_bing)
View(Eurasia_sentiment_bing)
View(Combined_sentiment_bing)

```

```{r}
table(LA_sentiment_bing$sentiment)
table(Eurasia_sentiment_bing$sentiment)
table(Spokesman_sentiment_bing$sentiment)
table(Combined_sentiment_bing$sentiment)

ggplot(data = LA_sentiment_affin, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("LA Times Sentiment Range")+
  theme_minimal()

ggplot(data = Eurasia_sentiment_affin, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("Eurasia Review Sentiment Range")+
  theme_minimal()

ggplot(data = Spokesman_sentiment_affin, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("Spokesman Review Sentiment Range")+
  theme_minimal()

ggplot(data = Combined_sentiment_affin, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("West Coast Sentiment Range")+
  theme_minimal()

set.seed(42)
ggplot(LA_Times[1:50,], aes(label = word, size = n)
       ) +
  geom_text_wordcloud() +
  theme_minimal()

ggplot(Eurasia[1:50,], aes(label = word, size = n)
       ) +
  geom_text_wordcloud() +
  theme_minimal()

ggplot(Spokesman[1:50,], aes(label = word, size = n)
       ) +
  geom_text_wordcloud() +
  theme_minimal()

ggplot(Combined[1:50,], aes(label = word, size = n)
       ) +
  geom_text_wordcloud() +
  theme_minimal()
```

```{r}
LA_Times_raw <- as_tibble(readLines("LA_Times.txt"))
Eurasia_raw <- as_tibble(readLines("Eurasia_Review.txt"))
Spokesman_raw <- as_tibble(readLines("Spokesman_Review.txt"))
Combined_raw <- as_tibble(readLines("West_Coast.txt"))

View(LA_Times_raw)
View(Eurasia_raw)
View(Spokesman_raw)
View(Combined_raw)


data_prep <- function(x,y,z){
  i <- as_tibble(t(x))
  ii <- unite(i,"text",y:z,remove = TRUE,sep = "")
}

LA_Times_bag <- data_prep(LA_Times_raw, 'V1','V451')
Eurasia_bag <- data_prep(Eurasia_raw, 'V1','V3617')
Spokesman_bag <- data_prep(Spokesman_raw, 'V1','V210')
Combined_bag <- data_prep(Combined_raw, 'V1','V4278')

Sources <- c("LA","Eurasia", "Spokesman", "Combined")

tf_idf_text <- tibble(Sources,text=t(tibble(LA_Times_bag, Eurasia_bag, Spokesman_bag,Combined_bag, .name_repair = "universal")))

class(tf_idf_text)

word_count <- tf_idf_text %>%
  unnest_tokens(word, text) %>%
  count(Sources, word, sort = TRUE)

total_words <- word_count %>% 
  group_by(Sources) %>% 
  summarize(total = sum(n))

Article_words <- left_join(word_count, total_words)

View(Article_words)

Article_words <- Article_words %>%
  bind_tf_idf(word, Sources, n)

View(Article_words)
```