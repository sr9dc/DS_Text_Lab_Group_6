

Big Data Analysis Identifies New Cancer Risk Genes
Eurasia Review
July 11, 2018 Wednesday


Copyright 2018 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 604 words
Body


There are many genetic causes of cancer: while some mutations are inherited from your parents, others are acquired all throughout your life due to external factors or due to mistakes in copying DNA. Large-scale genome sequencing has revolutionised the identification of cancers driven by the latter group of mutations - somatic mutations - but it has not been as effective in the identification of the inherited genetic variants that predispose to cancer. The main source for identifying these inherited mutations is still family studies.
Now, three researchers at the Centre for Genomic Regulation (CRG) in Barcelona, led by the ICREA Research Professor Ben Lehner, have developed a new statistical method to identify cancer predisposition genes from tumour sequencing data.
"Our computational method uses an old idea that cancer genes often require 'two hits' before they cause cancer. We developed a method that allows us to systematically identify these genes from existing cancer genome datasets" explained Solip Park, first author of the study and Juan de la Cierva postdoctoral researcher at the CRG.
The method allows researchers to find risk variants without a control sample, meaning that they do not need to compare cancer patients to groups of healthy people.
"Now we have a powerful tool to detect new cancer predisposition genes and, consequently, to contribute to improving cancer diagnosis and prevention in the future," added Park.
The work, which is published in Nature Communications, presents their statistical method ALFRED and identifies 13 candidate cancer predisposition genes, of which 10 are new.
"We applied our method to the genome sequences of more than 10,000 cancer patients with 30 different tumour types and identified known and new possible cancer predisposition genes that have the potential to contribute substantially to cancer risk," said Ben Lehner, principal investigator of the study.
"Our results show that the new cancer predisposition genes may have an important role in many types of cancer. For example, they were associated with 14% of ovarian tumours, 7% of breast tumours and to about 1 in 50 of all cancers. For example, inherited variants in one of the newly-proposed risk genes - NSD1 - may be implicated in at least 3 out of 1,000 cancer patients." explained Fran Supek, CRG alumnus and currently group leader of the Genome Data Science laboratory at the Institute for Reseach in Biomedicine (IRB Barcelona).
When sharing is key to advance knowledge
The researchers worked with genome data from several cancer studies from around the world, including The Cancer Genome Atlas (TCGA) project and also from several projects having nothing to do with cancer research.
"We managed to develop and test a new method that hopefully will improve our understanding of cancer genomics and will contribute to cancer research, diagnostics and prevention just by using public data," said Solip Park.
Ben Lehner added, "Our work highlights how important it is to share genomic data. It is a success story for how being open is far more efficient and has a multiplier effect. We combined data from many different projects and by applying a new computational method were able to identify important cancer genes that were not identified by the original studies. Many patient groups lobby for better sharing of genomic data because it is only by comparing data across hospitals, countries and diseases that we can obtain a deep understanding of many rare and common diseases. Unfortunately, many researchers still do not share their data and this is something we need to actively change as a society".

Load-Date: July 11, 2018


End of Document


Computer Searches Telescope Data For Evidence Of Distant Planets
Eurasia Review
March 31, 2018 Saturday


Copyright 2018 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 783 words
Body


As part of an effort to identify distant planets hospitable to life, NASA has established a crowdsourcing project in which volunteers search telescopic images for evidence of debris disks around stars, which are good indicators of exoplanets.
Using the results of that project, researchers at MIT have now trained a machine-learning system to search for debris disks itself. The scale of the search demands automation: There are nearly 750 million possible light sources in the data accumulated through NASA's Wide-Field Infrared Survey Explorer (WISE) mission alone.
In tests, the machine-learning system agreed with human identifications of debris disks 97 percent of the time. The researchers also trained their system to rate debris disks according to their likelihood of containing detectable exoplanets. In a paper describing the new work in the journal Astronomy and Computing, the MIT researchers report that their system identified 367 previously unexamined celestial objects as particularly promising candidates for further study.
The work represents an unusual approach to machine learning, which has been championed by one of the paper's coauthors, Victor Pankratius, a principal research scientist at MIT's Haystack Observatory. Typically, a machine-learning system will comb through a wealth of training data, looking for consistent correlations between features of the data and some label applied by a human analyst - in this case, stars circled by debris disks.
But Pankratius argues that in the sciences, machine-learning systems would be more useful if they explicitly incorporated a little bit of scientific understanding, to help guide their searches for correlations or identify deviations from the norm that could be of scientific interest.
"The main vision is to go beyond what A.I. is focusing on today," Pankratius said. "Today, we're collecting data, and we're trying to find features in the data. You end up with billions and billions of features. So what are you doing with them? What you want to know as a scientist is not that the computer tells you that certain pixels are certain features. You want to know 'Oh, this is a physically relevant thing, and here are the physics parameters of the thing.'"
Classroom conception
The new paper grew out of an MIT seminar that Pankratius co-taught with Sara Seager, the Class of 1941 Professor of Earth, Atmospheric, and Planetary Sciences, who is well-known for her exoplanet research. The seminar, Astroinformatics for Exoplanets, introduced students to data science techniques that could be useful for interpreting the flood of data generated by new astronomical instruments. After mastering the techniques, the students were asked to apply them to outstanding astronomical questions.
For her final project, Tam Nguyen, a graduate student in aeronautics and astronautics, chose the problem of training a machine-learning system to identify debris disks, and the new paper is an outgrowth of that work. Nguyen is first author on the paper, and she's joined by Seager, Pankratius, and Laura Eckman, an undergraduate majoring in electrical engineering and computer science.
From the NASA crowdsourcing project, the researchers had the celestial coordinates of the light sources that human volunteers had identified as featuring debris disks. The disks are recognizable as ellipses of light with slightly brighter ellipses at their centers. The researchers also used the raw astronomical data generated by the WISE mission.
To prepare the data for the machine-learning system, Nguyen carved it up into small chunks, then used standard signal-processing techniques to filter out artifacts caused by the imaging instruments or by ambient light. Next, she identified those chunks with light sources at their centers, and used existing image-segmentation algorithms to remove any additional sources of light. These types of procedures are typical in any computer-vision machine-learning project.
Coded intuitions
But Nguyen used basic principles of physics to prune the data further. For one thing, she looked at the variation in the intensity of the light emitted by the light sources across four different frequency bands. She also used standard metrics to evaluate the position, symmetry, and scale of the light sources, establishing thresholds for inclusion in her data set.
In addition to the tagged debris disks from NASA's crowdsourcing project, the researchers also had a short list of stars that astronomers had identified as probably hosting exoplanets. From that information, their system also inferred characteristics of debris disks that were correlated with the presence of exoplanets, to select the 367 candidates for further study.

Load-Date: March 31, 2018


End of Document


Natural Product Could Lead To New Class Of Commercial Herbicide
Eurasia Review
July 16, 2018 Monday


Copyright 2018 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 633 words
Body


A garden can be a competitive environment. Plants and unseen microorganisms in the soil all need precious space to grow. And to gain that space, a microbe might produce and use chemicals that kill its plant competitors. But the microbe also needs immunity from its own poisons.
By looking for that protective shield in microorganisms, specifically the genes that can make it, a team of UCLA engineers and scientists discovered a new and potentially highly effective type of weed killer. This finding could lead to the first new class of commercial herbicides in more than 30 years, an important outcome as weeds continue to develop resistance to current herbicide regimens.
Using a technique that combines data science and genomics, the team found the new herbicide by searching the genes of thousands of fungi for one that might provide immunity against fungal poisons. This approach is known as "resistance gene-directed genome mining."
The study, which was published in Nature, also points to the potential for this genomics-driven approach to be used in medicine, with applications ranging from new antibiotics to advanced cancer-fighting drugs.
"Microorganisms are very smart at protecting themselves from the potent molecules they make to kill their enemies," said Yi Tang, the study's co-principal investigator and a UCLA professor of chemical and biomolecular engineering, and of chemistry and biochemistry. "The presence of these resistance genes provides a window into the functions of the molecules, and can allow us to discover these molecules and apply them to diverse applications in human health and agriculture."
For example, if a resistance gene that protects a microorganism from an anti-bacterial product is found, there's a possibility that the microorganism also has genes to produce that same anti-bacterial compound. That discovery could potentially lead to new antibacterial medicines.
The new herbicide acts by inhibiting the function of an enzyme that is necessary for plants' survival. The enzyme is a key catalyst in an important metabolic pathway that makes essential amino acids. When this pathway is disrupted, the plants die.
This pathway is not present in mammals, including humans, which is why it has been a common target in herbicide research and development. The new herbicide works on a different part of the pathway than current herbicides. A commercial product that uses it would require more research and regulatory approval.
"An exciting aspect of the work is that we not only discovered a new herbicide, but also its exact target in the plant, opening the possibility of modifying crops to be resistant to a commercial product based on this herbicide," said study co-principal investigator Steven Jacobsen, a professor of molecular, cell and developmental biology in the UCLA College and an investigator of the Howard Hughes Medical Institute. "We are looking to work with large agrochemical companies to develop this promising lead further."
To confirm the efficacy of the new herbicide, the UCLA team tested the fungus-produced product on a common plant used in lab studies called Arabidopsis. In experiments, the product killed the plants after they were sprayed with it. The researchers also implanted the resistance gene from the fungus into Arabidopsis genomes. The plants that had the resistance gene implanted in them were immune to the herbicide.
"The emergence of herbicide-resistance weeds is thwarting every herbicide class in use; in fact, there has not been a new type commercialized within the last 30 years," said Yan Yan, a UCLA chemical engineering graduate student who was a lead author of the paper. "We think this new, powerful herbicide - combined with crops that are immune to it - will complement urgent efforts in overcoming weed resistance."

Load-Date: July 16, 2018


End of Document


Soil Fungi May Help Determine Resilience Of Forests To Environmental Change
Eurasia Review
March 19, 2018 Monday


Copyright 2018 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 786 words
Body


Nature is rife with symbiotic relationships, some of which take place out of sight, like the rich underground exchange of nutrients that occurs between trees and soil fungi.
But what happens in the dark may have profound implications above ground, too: A major new study reveals that soil fungi could play a significant role in the ability of forests to adapt to environmental change.
Kai Zhu, assistant professor of environmental studies at UC Santa Cruz, took a unique "big data" approach to investigating the role of symbiotic fungi in tree migration in forests across the eastern United States.
"Our climate is rapidly changing, and our forests are responding, but in very slow motion-it's hardly detectable," said Zhu, who wanted to identify factors that contribute to the pace of that response.
In forests, tree growth largely depends on the nutrients available in the soil, while the transfer of carbon through roots to the soil regulates ecosystem processes. Mycorrhizal ("MY-koe-RY-zull") fungi grow on the roots of most plants and drive the nutrient-carbon exchange between plants and soil: They take up carbon resources from their hosts and provide soil nutrients that plants need. The two most common fungi associated with forest trees are ectomycorrhizal (ECM), which grow on conifers, including pines, oaks, and beeches, and arbuscular (AM), which grow on most nonconifers, such as maples.
Zhu utilized data from the U.S. Department of Agriculture's Forest Inventory and Analysis program to examine how soil carbon and nitrogen levels differ across stands of forest that are characterized by "AM dominant" trees and "ECM dominant" trees. He correlated the distribution of trees with soil fungi and content, then analyzed the distribution of trees by fungus type. In the most significant finding, Zhu was able to identify distinct soil nitrogen "signatures" that impact soils and ecosystems in ways that may determine the resilience of forests to the changing climate.
Specifically, soil carbon-to-nitrogen ratios increase with greater ECM dominance-even after accounting for climate, soil texture, and foliar nitrogen. Moreover, ECM dominance is more associated with low soil nitrogen rather than high soil carbon.
"These findings suggest that AM and ECM trees have differential success along nitrogen fertility gradients, or perhaps that AM and ECM trees promote differences in cycling rates of carbon and nitrogen because of traits associated with nitrogen acquisition," he said. "Both processes may occur simultaneously, leading to a self-reinforcing positive plant-soil feedback."
Zhu's findings suggest that the mycorrhizal guild could be an emerging "functional trait."
Functional traits are those that define species in terms of their ecological roles-how they interact with the environment and with other species. As such, they are predictable and easily measured from the ground or by satellite, which makes them particularly valuable to scientists who are monitoring environmental responses to climate change. "They tell us how the ecosystem is responding," said Zhu.
"There is no evidence yet that eastern forests are shifting their geographic ranges to higher latitudes in response to warming temperatures," said Zhu. "But understanding how mycorrhizal relationships impact ecosystems will help us predict how forests will respond to global change."
Zhu's study, published in the Journal of Ecology, is one of the first to use the USDA's large-scale data set to see how climate change is impacting the ecosystem, an approach known as "top down" rather than "bottom up."
As a quantitative environmental scientist, Zhu brings the tools of statistics and data science to the study of global ecology. Rather than measuring fungal traits in the soil and scaling up, Zhu uses existing data-including large-scale datasets generated by satellites-to look at patterns and processes playing out on continental and global scales. "Big data is becoming more and more popular and powerful," he said. "It's different from traditional research in ecology, which takes place in a lab or in the field."
Zhu, whose background is in physics and systems theory, brings tremendous urgency to his work on climate change. His research focuses on four areas: forest ecosystems, grassland, soil, and phenology, which Zhu describes as "nature's calendar."
Zhu is determined to make solid contributions to a field in which much of the evidence is incomplete and unconvincing.
"We know the environment is changing, but how it impacts the Earth and its systems is a big question," he said. "As scientists, we have the responsibility to correctly work out this problem-it's a problem that's important to scientists and the general public."

Load-Date: March 19, 2018


End of Document


Tall And Older Amazonian Forests More Resistant To Droughts
Eurasia Review
May 29, 2018 Tuesday


Copyright 2018 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 536 words
Body


Tropical rainforests play a critical role in regulating the global climate system-they represent the Earth's largest terrestrial CO2 sink. Because of its broad geographical expanse and year-long productivity, the Amazon is key to the global carbon and hydrological cycles. Climate change could threaten the fate of rainforests, but there is great uncertainty about the future ability of rainforests to store carbon. While severe droughts have occurred in recent years in the Amazon watershed, causing widespread tree mortality and affecting the forests' ability to store carbon, the drivers of tropical rainforests' sensitivity to drought are poorly understood.
A new study led by Pierre Gentine, associate professor of earth and environmental engineering at Columbia Engineering, shows that photosynthesis in tall Amazonian forests-forests above 30m-is three times less sensitive to precipitation variability than in shorter forests of less than 20m. Taller Amazonian forests were also found to be older, have more biomass and deeper rooting systems that enable them to access deeper soil moisture, which makes them more resilient to drought. The paper was published online on Nature Geoscience.
"Our findings suggest that forest height and age are an important regulator of photosynthesis in response to droughts," sayid Gentine, who is also a member of the Earth Institute and the Data Science Institute. "Although older and taller trees show less sensitivity to precipitation variations (droughts), they are more susceptible to fluctuations in atmospheric heat and aridity, which is going to rise substantially with climate change. Our study shows that the Amazon forest is not uniform in response to climate variability and drought, and illuminates the gradient of responses observable across Amazonian forests to water stress, droughts, land use/land cover changes, and climate change."
Climate change is altering the dynamics, structure, and function of the Amazon. While climate factors that control the spatial and temporal variations in forests' photosynthesis have been well studied, the influence of forest height and age (affected by deforestation for instance) on this controlling effect has rarely been considered. Gentine used remote sensing observations of solar-induced fluorescence (a proxy for photosynthesis), precipitation, vapor-pressure deficit, and canopy height, together with estimates of forest age and aboveground biomass. His group applied statistical techniques to estimate how age and height could modify forest sensitivity to droughts.
Gentine's remote sensing observations showed that tall and older forests were less sensitive to droughts but more sensitive to heat and atmospheric dryness. This finding has implications for the capacity of younger vs. older forests to withstand-or not-future droughts. For instance, deforestation could increase the fragility of the forests to droughts, as the forest becomes younger and thus more sensitive to droughts.
"Our study makes it clear that forest height and age directly impact the carbon cycle in the Amazon," Gentine said. "This is especially significant given the importance of the Amazon rainforest for the global carbon cycle and climate."

Load-Date: May 29, 2018


End of Document


UN Chief Asks Melinda Gates And Ali Baba's Jack Ma To Head Digital Experts Panel
Eurasia Review
July 16, 2018 Monday


Copyright 2018 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 842 words
Body


By J Nastranis
Noting that "the scale, spread and speed of change made possible by digital technologies is unprecedented, but the current means and levels of international cooperation are unequal to the challenge," UN Secretary-General António Guterres has announced the launch of a High-level Panel on Digital Cooperation.
As Guterres told reporters at the UN Headquarters in New York on July 12, this is the first such Panel of its kind. It is comprised of women and men at the frontiers of technology, public policy, science, and academia.
The U.S.-based Bill & Melinda Gates Foundation co-chair Melinda Gates (USA) and Alibaba Group Executive Chairman Jack Ma from China are Co-Chairs of the 20-member Panel.
"Digital technologies make a significant contribution to the realisation of the Sustainable Development Goals and they cut uniquely across international boundaries," said Guterres on the launching of the Panel.
"Therefore, cooperation across domains and across borders is critical to realizing the full social and economic potential of digital technologies as well as mitigating the risks that they pose and curtailing any unintended consequences," he added.
The UN Chief has asked the Panel to contribute to the broader public debate on the importance of cooperative and interdisciplinary approaches to ensure a safe and inclusive digital future for all taking into account relevant human rights norms. The Panel is expected to identify policy, research and information gaps, and make proposals to strengthen international cooperation in the digital space.
Explaining the purpose of the Panel, Melinda Gates noted: "Technology is neither good nor bad. It's just a tool – a very powerful tool – and what matters is how the world uses it. If all people, especially the poorest and most vulnerable, have equal access to digital technology, they will use it to improve life for themselves and their families and raise their voices in conversations about what the future holds. Enabling this widescale empowerment is what this panel is about."
The Panel will hold its first in-person meeting in late September 2018 and is expected to submit its final report to the Secretary-General within nine months.
In carrying out its work, the Panel will undertake a wide range of public consultations, including at least two public events and an open process inviting global inputs including through online engagement activities starting in September. It will be supported by a small Secretariat funded by donor resources, and based in New York and Geneva.
Jack Ma said: "Soon, every industry will be digitized, and this will have a tremendous impact on every aspect of life. In this digital era, data and technology are more broadly available, enabling entrepreneurialism, economic growth, and improved quality of life for those who have the access and training to leverage it. Global, cross-sector collaboration is critical to ensure the benefits of the digital era are possible for all."
Members of the High-level Panel on Digital Cooperation, in addition to the two co-chairs are: Mohammed Al Gergawi (UAE), Minister of Cabinet Affairs and the Future, UAE; Yuichiro Anzai (Japan), President of the Japan Society for the Promotion of Science; Nikolai Astrup (Norway), Minister of International Development, Norway; Vinton Cerf (USA), Vice President and Chief Internet Evangelist, Google; and Fadi Chehadé (USA), Partner at ABRY Partners.
Other members are: Isabel Guerrero Pulgar (Chile), Director, IMAGO Global Grassroots and Lecturer, Harvard Kennedy School; Marina Kaljurand (Estonia), Chair of the Global Commission on the Stability of Cyberspace; Bogolo Kenewendo (Botswana), Minister of Investment, Trade and Industry, Botswana; and Akaliza Keza Ntwari (Rwanda), ICT advocate and entrepreneur.
Further Panel members are: Marina Kolesnik (Russian Federation), senior executive, entrepreneur and WEF Young Global Leader; Doris Leuthard (Switzerland), Head of the Federal Department of the Environment, Transport, Energy and Communications, Switzerland; Cathy Mulligan (United Kingdom), Visiting Research Fellow Imperial College Centre for Cryptocurrency; Edson Prestes (Brazil), Professor, Institute of Informatics, Federal University of Rio Grande do Sul; and Kira Radinsky (Israel), Director of Data Science, eBay.
Still more members are: Nanjira Sambuli (Kenya), Digital Equality Advocacy Manager, World Wide Web Foundation; Sophie Soowon Eom (Republic of Korea), Founder of Adriel AI and Solidware; Dhananjayan Sriskandarajah (Australia), Secretary General, CIVICUS; and Jean Tirole (France), Chairman of the Toulouse School of Economics and the Institute for Advanced Study in Toulouse.
Amandeep Singh Gill (India), Executive Director, Secretariat of the High-level Panel on Digital Cooperation and Jovan Kurbalija (Serbia), Executive Director, Secretariat of the High-level Panel on Digital Cooperation are ex officio members.
According to the UN, all Panel members serve in their personal capacity, not as representatives of their respective institutions.

Load-Date: July 16, 2018


End of Document


What Makes A Bestseller: Fiction, Thriller And A Christmas Release
Eurasia Review
April 8, 2018 Sunday


Copyright 2018 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 485 words
Body


Books that are fiction, thrillers or mysteries, have high initial sales numbers and are released around Christmas are more likely to be bestsellers, according to a study published in the open access journal EPJ Data Science.
A team of researchers from Northeastern University, Boston, used a big data approach to investigate what makes a book successful. By evaluating data from the New York Times Bestseller Lists from 2008 to 2016, they developed a formula to predict if a book would be a bestseller. They found that general fiction and biographies were more likely to make the bestseller list more often than other genres and that those with a higher initial place on the list were more likely to stay on it for a longer amount of time.
Professor Albert-László Barabási, lead author of the study, commented: "The most surprising result was that we found a universal pattern to book sales: all hardcover bestsellers, regardless of genre, follow a sales trajectory governed by the same factors. This allowed us to create a statistical model to predict sales of a book based on its early sales numbers."
The researchers found that although fiction books sold more copies than non-fiction books, non-fiction titles were more likely to retain their bestseller status once achieved. An example of this was 'Unbroken' by Laura Hillenbrand, a non-fiction title that stayed on the bestseller list for 203 weeks, longer than any other book in the study. The fiction title that stayed on the list the longest was 'The Help', which stayed for 131 weeks; this may have been due to a popular film adaption.
The authors also found that fiction writers had more repeat success with getting on the list than non-fiction writers. Books in the romance category were more likely to be written by female authors and male authors were more likely to be authors of non-fiction books. The researchers found no gender disparity among bestselling fiction authors but most non-fiction bestsellers were written by men.
The author's evaluated sales numbers and patterns from 2,468 fiction titles and 2,025 non-fiction titles from the New York Times Bestseller Lists 2008-2016 to create their formula for predicting how well a book would sell and whether it would be a bestseller. Three key parameters were found to be important to the formula: the audience, sales numbers from the author to date and time after publication.
Professor Barabási explained: "The analysis of bestseller characteristics and the discovery of the universal nature of sales patterns with its driving forces are crucial for our understanding of the book industry, and more generally, of how we as a society interact with cultural products."
The authors caution that the formula cannot account for events like awards a book may receive movie adaptations and celebrity endorsements. Although these factors may influence book success, they are relatively rare occurrences.

Load-Date: April 8, 2018


End of Document


Why Some Older People Are Rejecting Digital Technologies
Eurasia Review
March 12, 2018 Monday


Copyright 2018 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 775 words
Body


Fear of making mistakes and wider concerns about their social responsibility are among reasons why older people are rejecting digital technologies, a new study reveals.
Despite increasing numbers of older adults accessing the Internet, and many recent retirees having used computers during their careers, the digital divide between older adults and younger people still exists. Older adults use significantly fewer digital applications and spend less time online than younger adults.
Following interviews with older adults, researchers from Lancaster University have discovered that resistance to using digital technologies is not primarily rooted in accessibility issues, as widely believed.
Researchers found that personally held values to do with the desirability of technology, wider concerns regarding its impact on society, and fears of getting things wrong when using software are also significant factors holding back technology use among older adults.
Some older people are put off using online tools because they see them as being arduous and time consuming. They feel that the trend toward online services such as comparison websites places a greater burden on themselves to become experts in all manner of things, whereas previously one could seek out trained professionals to assist with decision making.
And security concerns were omnipresent among the older adults interviewed, with many of them lacking confidence in their own knowledge of how to use online tools properly, in particular online banking.
To attend to this the researchers, Dr Bran Knowles and Professor Vicki Hanson in their paper 'The Wisdom of Older Technology (Non) Users', which has been published by the Communications of the ACM journal, recommend that the designers of online services do more to offer better safety nets that will offer actual protection and more reassurance to older users.
A strong sense of social responsibility may be a more important factor underlying many older people's rejection of digital technology. They worry that online shopping takes business from local shops, threatening the town centres in which they can socialise with their friends. They also worry that if they don't go into the bank or the post office they may close and people may lose their jobs.
Dr Knowles said: "The fact that digital technologies can and do make certain jobs obsolete is a common concern for older adults who worry about their grandchildren's job prospects. Developing solutions to attend to this wider societal problem appears to be key to fostering acceptance of digital technologies among older adults."
Social isolation is another concern. Some older people are rejecting online shopping as they welcome the social benefits of daily face to face contact when shopping in person.
Professor Vicki Hanson said: "The efficiency gained by conducting online interactions is not a powerful motivator for technology adoption by older adults who may be experiencing loneliness and isolation. In many cases, making digital technologies appealing for older adults means ensuring that digital engagements do not replace social interactions, and if possible, facilitate new social and community-building opportunities where they can meet people."
Researchers also found some older people use their age as a cover for other personally-held reasons not to engage with technology. For example, someone who doesn't like social media because they think it can enable cyberbullying may say they are 'too old' as a convenient and socially acceptable way to justify not using it, as there are fewer expectations for an older person to use social media.
Dr Knowles and Professor Hanson call this 'playing the age card' and acknowledge the utility in older adults blaming their age for their non-use of digital technologies.
Dr Knowles said: "Older adults themselves are often the worst perpetuators of the myth that old age precludes engagement with a myriad of digital technologies. Doing so allows older adults a privilege not available to most working-age adults to take personal stands against the aspects of technology they find worrying, threatening or just plain annoying."
The research was supported with funding from the UK's Research Council's Digital Economy Theme, led by the Engineering and Physical Sciences Research Council (EPSRC), and also the EU's Horizon 2020 research and innovation programme.
The paper's authors are Dr Bran Knowles and Professor Vicki Hanson, both of Lancaster University's Data Science Institute. Dr. Knowles is a Lancaster 50/50 Lecturer and Prof Hanson a Visiting Professor from the US, based at the Rochester Institute of Technology in New York.

Load-Date: March 12, 2018


End of Document


Climate Change Is Already Affecting Global Food Production (Unequally)
Eurasia Review
June 2, 2019 Sunday


Copyright 2019 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 531 words
Body


The world's top 10 crops -  barley, cassava, maize, oil palm, rapeseed, rice, sorghum, soybean, sugarcane and wheat -
supply a combined 83 percent of all calories produced on cropland. 
Yields have long been projected to decrease in future climate 
conditions. Now, new research shows climate change has already affected 
production of these key energy sources - and some regions and countries
are faring far worse than others.
Published in PLOS ONE,  the University of Minnesota-led study, conducted with researchers from  the University of Oxford and the University of Copenhagen, used weather  and reported crop data to evaluate the potential impact of observed  climate change.
The researchers found that observed climate change causes a significant yield variation in  the world's top 10 crops, ranging from a decrease of 13.4 percent for  oil palm to an increase of 3.5 percent for soybean, and resulting in an average reduction of approximately one percent (-3.5 X 10e13 kcal/year) of consumable food calories from these top 10 crops.
Also the impacts of climate change on global food production are mostly  negative in Europe, Southern Africa, and Australia, generally positive  in Latin America, and mixed in Asia and Northern and Central America.
The researchers also found that half of all food-insecure countries are experiencing decreases in crop production - and so are some affluent industrialized countries in Western Europe.
Contrastingly, recent climate change has increased the yields of certain crops in some areas of the upper Midwest United States.
"There are winners and losers, and some countries that are already food insecure fare worse," says lead author Deepak Ray of the University of Minnesota's Institute on the Environment, whose high-resolution global crop statistics databases
have also been used to help to identify how global crop production 
changes over time. These findings indicate which geographical areas and 
crops are most at risk, making them relevant to those working to achieve
the U.N. Sustainable Development Goals of ending hunger and limiting the effects of climate change. Insights like these lead to new questions and crucial next steps.
Global map of changes in wheat yield on average annually. Units 
are measured by tons per hectare per year. Figure credit: Deepak Ray.
"This is a very complex system, so a careful statistical and  data science modeling component is crucial to understand the  dependencies and cascading effects of small or large changes," says  co-author Snigdhansu Chatterjee of the University of Minnesota's School of Statistics.
The Institute's Global Landscapes Initiative, whose contributors to this study included Ray, Paul West and James Gerber,  has previously produced global scale findings that have been put to use  by international organizations such as the U.N., World Bank and  Brookings in evaluation of global food security and environmental  challenges. The scholars say this report has implications for major food  companies, commodity traders and the countries in which they operate,  as well as for citizens worldwide.
"The research documents how change is already happening, not just in some future time," says Ray.

Load-Date: June 2, 2019


End of Document


Covert Action As An Intelligence Subcomponent Of The Information Instrument – Analysis
Eurasia Review
May 17, 2019 Friday


Copyright 2019 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 5312 words
Body


By Charles Pasquale and Laura Johnson*
Covertaction (CA) has long played an important role in supporting and advancing U.S. national security and foreign policy objectives, but broad misunderstandings in both concept and application frequently lead discussants to conflate and confuse it with military operations and the militaryinstrument of power (referring to the common, yet flawed, DIME typologyof diplomatic, information, military, and economic instruments). Despite obvious areas of overlap with other instruments, CA is more appropriately understood as a tool within the intelligence subcomponent of the informationinstrument. While some might view this as a semantic distinction without a difference, CA's complexity, political and operational sensitivity, and oversight requirements increase the importance of understanding the tool in the intelligence context.
The term intelligenceitself is open to interpretation. One general description is of the activities and products associated with collecting, analyzing, producing, disseminating, and using information to ultimately support policy objectives. It may also include the various Intelligence Community (IC) organizations and a range of other functions. Intelligence regularly plays an important role in helping leaders to fill knowledge gaps and make better decisions, but there is much more toit than may be evident to a casual observer or consumer. In addition tothe associated processes and institutions, intelligence is both an instrument to wield and an underlying elementalcomponent that enables, empowers, and supports other efforts with context and perspective. It is more than just a nebulous "knowledge ether" that exists in the background as a mystical fount of knowledge that decisionmakers can dip into for insight. Intelligence—including CA—involves a deliberate process of actively prioritizing information needs, tasking, direction, and evaluation that requires a cadre of professionals who understand its structures, authorities, capabilities, and limitations.
Covert Action
U.S. statute defines covert action,in part, as one or more U.S. Government activities undertaken "to influence political, economic, or military conditions abroad, where it is intended that the [government's] role will not be apparent or acknowledged publicly."1 TheHouse Permanent Select Committee on Intelligence (HPSCI) and Senate Select Committee on Intelligence (SSCI) have sole CA congressional oversight responsibility. And unlike during the Kennedy and Reagan eras,CA is now developed, authorized, and overseen within a specific formalized process.
In 2017, then-Director of the Central Intelligence Agency (D/CIA) Mike Pompeo publicly reaffirmed this point when he statedthat, despite what we may see in the movies, "we do not pursue covert action on a whim without approval or accountability. There is a comprehensive process that starts with the President and consists of many levels of legal and policy review and reexamination. . . . When it comes to covert action, there is oversight and accountability every stepof the way."2 The current situationdid not develop automatically or organically, however; it evolved largely in response to hard learned lessons, such as those associated with the Iran-Contra scandal in the 1980s.
The legal process for initiating CA requires two key components. The first is a written "finding," which the President ofthe United States must personally authorize. It may not (with some exceptions) be retroactive and must specify the action(s) to be undertaken, which government entities are directed to participate, and whether any third parties will be used.3The second is "timely" notification to the congressional intelligence committees, although notification may be restricted to just a few congressional leaders if sensitivity is required.4 Within these components, the CA also must "support identifiable [U.S.] foreign policy objectives" and be found"important to U.S. national security." Statute further requires the President to establish a written response plan for every CA in the eventof its unauthorized public disclosure,5 and it prevents any government funds from being expended for CA without a formal finding.6The National Security Council (NSC) is the highest-ranking executive branch component involved in supporting CA, although it has no authorityto conduct such operations.7
Significantly, CA-related statute specifically excludes those actions primarily intended to collect intelligence or to conduct traditional military, diplomatic, counterintelligence, or government law enforcement activities, among other things.8 None of this is to say, however, that operators cannot collect intelligence during the course of a covert action.
Both HPSCI and SSCI consider intelligence and its related activities to include covert or clandestine activities affectingU.S. relations with a foreign government, political group, party, military force, movement, or other association.9But poor understanding of a critical distinction between "covert" and "clandestine" activity—described below—blurs the line and creates additional confusion for many.
Clandestine Operations
Much of the IC's and Department of Defense's (DOD's) work is clandestine, although only a relatively small portion fits into the category of CA. "Covert" activity hides the true affiliation or relationship of the primary person or organization behindthe action (that is, the identity of the sponsor), but the activity may be generally observable. In contrast, "clandestine" activity hides the activity itself (that is, the existence of the operation).10So "covert" conceals the actor, but "clandestine" conceals the action. There is also an unfortunate tendency to use "covert" as an adjective todescribe activities that are not specifically "overt" (done or shown openly); loosely referring to "secret" activity as "covert" only perpetuates misunderstanding of what constitutes CA.
It is worth noting that, while this article presents CA in a U.S. context, some of the general concepts may also apply to similar actions taken by foreign counterparts. Other intelligence services—particularly those with a competent external function—are also likely to have the tools, techniques, relationships, and authorities to plan and execute CA without their government's "fingerprints." However, they do not necessarily have the same statutorydefinitions, requirements, restrictions, or oversight.
Intelligence and the Military in Title 50 Covert Action: Combined but Distinct
Compounding the above, a related point ofconfusion lies with the Title 10/Title 50 distinction and discourse, which often inaccurately tries to draw clear lines between military and intelligence activity; in reality, the two overlap (this does not,however, constitute Title 60). Whereas Title 10 is exclusively related to the "Armed Forces," Title 50 "War and National Defense" includes all intelligence activities and many military operations.
Although CA was originally implemented as a CentralIntelligence Agency (CIA)-specific mission area, press reporting increasingly alludes to military special operations forces (SOF) conducting these operations. But while this idea is becoming increasingly ingrained in common perception, partly because of the aforementioned loose use of terminology, the distinction is less clear cut than it might appear. As Andru Wall notes:
[U.S.] SOF [personnel]typically work closely with CIA personnel while conducting unconventional warfare, although the relationship tends to be informal and focused more on mutual support. . . . The relationship is one of cooperation in pursuit of mutual objectives rather than a formal superior-subordinate relationship. . . . This is an important distinction that directly answers whether the unconventional warfare mission is a military operation or intelligence activity.11
This type of complex operating environment—involving both civilian IC and military operators under similar statutory authorizations—may blur the distinctions between typesof activity, who is supporting, and who is leading. This is where statutory distinctions become increasingly important.
Not only does the military have some Title 50 roles, but some Title 10 authorities may also appear outwardly similar to CA—albeit without the required Presidential finding or congressional oversight. For example, Title 10 currently allows the Secretary of Defense to expend up to $100 million in any fiscal year to support "foreign forces, irregular forces, groups, or individuals who are supporting or facilitating ongoing [U.S. SOF] operations to combat terrorism."12 (Prior to the 2005 National Defense Authorization Act, SOF reportedly relied on CIA funding for these operations.13) But these are not covert actions, which the legislation specifically excludes among the provided authorities;14rather, they are more consistent with traditional SOF unconventional warfare, although some of them probably would share many common characteristics were they conducted under CA intelligence authorities.
Similarly, Title 10 gives the U.S. Special Operations Command (USSOCOM) commander the responsibility and authority to conduct all affairs relating to special operations activities, which include "such other activities as may be specified by the President or the Secretary of Defense."15 However, it explicitly does notconstitute authority for DOD to conduct any action that "if conducted as an intelligence activity, would require a notice to [SSCI and HPSCI]."16 Although CA would clearlyfall within that requirement, some critics worry the criteria are actually designed to expand DOD activities while avoiding the additionaloversight.17
The CIA has historically been—and available public reporting suggests that it remains—the leading entity for CA operations,even when they include U.S. military SOF personnel who may be temporarily placed under CIA authorities, guidance, and direction. Although statute technically allows the President to designate any agency to conduct CA,18doing so is not necessarily feasible, and the same intelligence oversight requirements and restrictions would apply in any event. CA is acore mission area for the CIA, which arguably has unique institutional processes, structures, and experience to carry it out. Secrecy is difficult enough to maintain in ordinary operational conditions; CA sensitivities exponentially magnify this challenge and therefore requireextraordinary structural elements to be in place and functioning.
The widely publicized raid that captured and killedOsama bin Laden presents a useful example of military resources being used in an operation officially under the direction and control of a civilian intelligence agency and under CA authorities and congressional intelligence committee oversight. President George W. Bush in 2001 reportedly had issued a finding specifically to target and kill bin Laden,19 and President Barack Obama,shortly after taking office in 2009, reportedly directed then–CIA Director Leon Panetta "to make the killing or capture of bin Laden the top priority of our war against al-Qaeda."20Panetta has since publicly stated that he officially commanded the overall May 2011 bin Laden raid from the CIA as a Title 50 covert operation, even while then–USSOCOM Commander Admiral Bill McRaven executed operational-level control of the mission from Afghanistan.21 Nick Rasmussen, director of the National Counterterrorism Center, recounted 5 years after the operation:
During the raid itself, I clearly recall the role that Admiral McRaven played from Jalalabad, Afghanistan. In addition to carrying out his command and control function with his team, he was piped in via secure video conference to provide updates to the CIA and the assembled officials at the White House Situation Room, including thePresident. As the Department of Defense operators would move down theirchecklist, we heard McRaven's voice as each operational or geographicalmark or milestone was hit.22
It is unclear whether the U.S. Government originally intended to acknowledge the bin Laden operation after the fact. Had one of the "stealth" helicopters not crashed in the Abbottabadcompound, leaving clear traces of U.S. involvement, it is plausible that the operators could have gotten in and out without leaving America's "fingerprints," thereby maintaining plausible deniability. This is a clear example of using Title 50 CA authorities under the CIA'sdirection and control, while using military forces as the action arm.23(As a side note, readers should not conflate Panetta's overall "direction and control" of the covert action with Admiral McRaven's "command and control" of the military forces on the ground and in the air.)
The following additional examples help to  illustrate how CA had been used in the mid-1990s and early 2000s with  varying degrees of success (or failure) against Saddam Hussein's regime  in Iraq to create psychological conditions for regime change, insert  teams to conduct counterterrorist and counter-regime operations, and  develop indigenous opposition militia groups. But because CA details  largely remain shrouded in secrecy, this article's authors take no  position on the veracity of these examples—they are primarily to show  how covert operations may play out in practice and to highlight some potential challenges associated with CA as a strategic tool.
Selected Examples from Iraq Covert Action Cases
Although the U.S.-led coalition soundly defeated Iraq in the 1990–1991 Gulf War and devastated its infrastructure and army, Saddam remained a meddling dictator with an apparent penchant for weapons of mass destruction. President George H.W.Bush was unwilling to take down the Iraqi regime in the Gulf War because—as he wrote in his memoir several years later—the human and political costs of removing Saddam would have been incalculable. Had he gone to Baghdad following the dislodgment of Iraq from Kuwait:
the coalition would have instantly collapsed. . . . [It]would have destroyed the precedent of international response to aggression we hoped to establish. Had we gone the invasion route, the United States could conceivably still be an occupying power in a bitterly hostile land. It would have been a dramatically different — andperhaps barren — outcome.24
Instead of pushing to Baghdad, the President reportedly issued a finding that authorized the CIA to spend up to $100 million to covertly "create the conditions" that would lead to Iraqi regime change from within using two main lines of effort: overseeing a propaganda campaign and creating an opposition movement in Iraq.
Third-Party Propaganda. TheCIA reportedly contracted the Rendon Group (TRG)—a private strategic communications and public affairs company—to set up a propaganda office in London.25 TRG's work included planting false stories in the foreign press about Saddam to highlight his atrocities and undermine his legitimacy; this supposedly was easy todo, as he was a frequent perpetrator of real atrocities, and the best lies tend to have a modicum of truth. But TRG supposedly supplied misinformation to unwitting British journalists, who then published it in London press stories that occasionally filtered back into the U.S. media. Because CA statutes prohibit actions "intended to influence United States political processes, public opinion, policies, or media,"26 the CIA reportedly criticized this unintended "blowback" aspect and took additional steps to prevent domestic U.S. circulation.27
Seeking a contractor to expand its propaganda operations inside Iraq to bring down the regime,28 TRG engaged Dr. Ahmed Chalabi, a London-based Iraqi exile who came from an elite Shiite family that fled Iraq in 1958;29 he also held a doctorate in mathematics from the University of Chicago30 and had developed significant Washington, DC, political connections.31TRG funded Chalabi to create the conditions in Iraq that would bring down Saddam, but, according to author James Bamford, "Chalabi [himself] was a creature of American propaganda to a large degree. TRG basically created his organization, the Iraqi National Congress , and put Chalabi in charge."32
The INC reportedly set up a print shop in the Kurdish governorate of Salahuddin and ran a disinformation campaign, creating fake versions of Iraqi newspapers filled with stories of regimeabuses. Robert Baer—a former CIA officer who reportedly worked with Chalabi—compared this to "something like a spy novel . . . people were scanning Iraqi intelligence documents into computers, and doing disinformation . . . [and] forgeries . . . to bring down Saddam."33 Butwithout publicly available assessments of the propaganda's effectiveness, it is unclear to what degree the results met the desired policy outcomes.
Third-Party Support to Opposition Forces. A second part of Chalabi's mission included building an indigenous opposition force to bring down the Iraqi regime.34Although he had no military training or service, Chalabi and the INC created a 1,000-man militia to fight the Iraqi military, which he incorrectly claimed was extremely weak, stating it was like "a leaking warehouse of gas, and all we had to do was light a match," according to Baer.35 In addition to the militia, Chalabi attempted to increase the size of his own opposition alliance bybribing non-Kurdish, Mosul-based tribal leaders who agreed to support the INC's rebellion. Press reporting indicates that he may also have partnered with Iranian intelligence officers to conduct a separate coordinated operation in southern Iraq.36
In March 1995, Chalabi launched the attack, reportedly against Baer's advance warnings and recommendation to abort the operation because the plot had leaked and the United States would not provide backup if he went ahead. Iraqi forces killed many of Chalabi's men, and most of the rest deserted as the bribed tribal leaders sat out the operation and Iran withheld support.37The failure ultimately ended what remained of Chalabi's relationship with the CIA, but the Iraq Liberation Act of 1998 called for Iraqi regime change as an overt U.S. strategic objective, meaning the INC's actions no longer had to be covert; DOD and the Department of State werethen free to openly support Chalabi and others in the INC.38
First-Party Counterterrorism Operations. Incontrast to working wholly through intermediaries, other purported CA missions have directly involved CIA paramilitary officers in the planning, preparing, and conducting operations.39According to press, President George W. Bush in the days following the 9/11 attacks signed a counterterrorism (CT) CA finding that empowered the CIA to create and deploy paramilitary teams to hunt and kill designated terrorists anywhere in the world as part of the war on terror.40 This was especially applicable to Iraq in the summer of 2002, as the administration presumably had been considering war with that country for its alleged complicity with al Qaeda. The below examples are partly based on one self-described CIA CT operator's published description of his deploymentto Northern Iraq in advance of the 2003 Iraq War.41
In July 2002, a CIA CT team reportedly entered Northern Iraq and linked up with supportive Kurdish Peshmerga fighters to find and kill terrorists.42 They soon found roughly 1,000 members of Ansar al-Islam and al Qaeda encampedin the ungoverned northern Kurdistan part of Iraq along the border withIran, where hundreds of al Qaeda had sought safe haven after the coalition offensive in Afghanistan.43Although the CIA team was eager to capture or kill the terrorists, CIA headquarters reportedly did not provide the necessary support to proceed. Unable to conduct the CA offensive, the team instead collected and reported on the groups, interrogated the Peshmerga's captives, destroyed key infrastructure in preparation for war, and built a broad human intelligence network throughout Kurdish-controlled Iraq. This resulted in a trove of raw intelligence that the team sent back to Washington for analysis, and reflected the tangential intelligence collection that falls outside primary statutory authorities for CA.44
First-Party CA Support to Indigenous Paramilitary Forces. The same finding noted above45also reportedly authorized the CIA team to support an Iraqi Arab paramilitary insurgency group—"the Scorpions"—that was trained to conduct psychological and other operations throughout Iraq. This reportedly was part of the U.S. policy of regime change.46Former D/CIA George Tenet wrote in his memoir that the group was to conduct sabotage and raids to destabilize Iraq prior to the 2003 war.47The CIA team reportedly vetted the men for suitability before moving them through Turkey to the United States for CIA training, but accounts differ about the Scorpions' operational capability and effectiveness—onepress report indicated that the war's quick conclusion minimized the initial mission,48 but others refer to the group's inherent lack of skill and capability49—leaving an open question as to how planners in Washington perceived the value of such a specialized indigenous team.50
Not the Same by Any Other Name
Some may argue that CA is incompatible with the information instrument, which tends to reflect the soft-power side of national statecraft, or that "covert" simply describes a way of doing things, applied to whatever instrument is being used that way. Butit is not that simple, and the above statutory and operational examplesshow that the essence of CA is not whether it involves pamphlets or paramilitary forces, but to what extent information is withheld or obfuscated about the sponsor. Each of the DIME instruments can be applied overtly, clandestinely, or covertly, but their individual characteristics are secondary to information when applied in a CA context.
National governments overtlyuse public diplomacy and public affairs to directly engage foreign and domestic populations, convey diplomatic messages and intentions, and shape their opinions. But these are different from the covertinformational activities described in the Iraq examples because TRG andthe INC manipulated foreign and Iraqi perceptions of Saddam's regime through false information to achieve an objective on behalf of the U.S. Government while concealing its role.Similarly, the Scorpions, as a symbol of Iraqi resistance, may have hada powerful psychological effect on those Iraqis who saw hope for an indigenous uprising, even though the group was a U.S.-manufactured proxyinstead of a function of the Iraqi people's will.
Returning to the DIME typology, it is important to remember that in CA, the nature of the tool used does not supersede the information aspect.Some CAs described above (for example, propaganda efforts) clearly align with the "i." But while other examples included applying negotiation skills to engage the Peshmerga (aligning with the "d"), using force to kill terrorists and blow up infrastructure (aligning withthe "m"), and wielding large sums of cash to achieve desired influence effects (aligning with the "e"), none of these existed in a vacuum. Thisis a question of fit as well as function; each of these examples existed within the framework of one or more CA findings, and because CA exists under congressional intelligence oversight and regularly relies on intelligence assets, it remains first and foremost tied to the "i" instrument of national power. CA can therefore never be solelyany of the others. Rather, it may be useful to think of DIME instruments in CA operations as creating a hybrid, such as "information-military" is similar to the "political-military" and "political-economic" hyphenated compound terms that are commonly used insecurity discourse.
The realities noted above have significant implications for applying the DIME construct to intelligenceand information, whether overt, clandestine, or covert. Strategists, operators, and educators must be vigilant not only in remembering that CA is rooted in intelligence as a subcomponent of the information instrument, but also in comprehending what constitutes CA, why it is a useful instrument in the strategist's toolkit, and how to weigh the associated costs and risks.
Because covert action's functional mechanism is to  deliberately manipulate information and knowledge about the actors  involved in an activity, it falls squarely within the information  instrument. Moreover, because its oversight function falls to  congressional intelligence committees, it is more specifically within  the intelligence subcomponent of information. This remains a distinction with a difference.
*About the authors: Dr. Charles Pasquale was the Central Intelligence Agency Director Representative to the National War College from 2016–2019 and is an Adjunct Professorial Lecturer at American University. Dr. Laura Johnson is an Adjunct Professor with the Catholic University of America and Director of Analysis at FOG Data Science.
Source: This article was published in the Joint Force Quarterly 93, which is published by the National Defense University.
Notes:
1 Title 50, U.S. Code, War and National Defense, § 3093(e).
2 CentralIntelligence Agency (CIA), "Director Pompeo Delivers Remarks at CSIS," April 13, 2017, available at <www.cia.gov /news-information/speeches-testimony/2017-speeches-testimony/pompeo-delivers-remarks-at-csis.html>.
3 Title 50, U.S. Code, War and National Defense, § 3093(a).
4 Ibid. See Title 50, U.S. Code, War and National Defense, § 3093(a)(1); see also Title 50, U.S. Code, § 3093(c)(2).
5 Title 50, U.S. Code, War and National Defense, § 3093(h).
6 Ibid., § 3094(c).
7 ExecutiveOrder (EO) 12333, "United States Intelligence Activities," December 4, 1981, as amended by EOs 13284 (2003), 13355 (2004), and 13470 (2008). United States Intelligence Agencies (2008); see especially, Section 1.2,"The National Security Council."
8 Title 50, U.S. Code, War and National Defense, § ????(e).
9 "Permanent Select Committee on Intelligence," Rule X, 11[j][1], in Rules of the House of Representatives, 114th Cong. 1st sess., (2015), 16, available at http://clerk.house.gov /legislative/house-rules.pdf.
10 Joint Counterterrorism Assessment Team, "Terminology (Covert)," in Intelligence Guide for First Responders(Washington, DC: Office of the Director of National Intelligence, 2014), 46, available at <www.dni.gov /files/NCTC/documents/features_documents/Intelligence_Guide_for_First_Responders.pdf>.
11 Andru E. Wall, "Demystifying the Title 10–Title 50 Debate," Harvard National Security Journal 3 (2011), 115, available at http://harvardnsj.org /wp-content/uploads/2012/01/Vol-3-Wall.pdf.
12 Title 10, U.S. Code, Armed Forces, § 127e(a). This expenditure requires concurrence of the relevant chief of mission, by statute.
13 Richard A. Best, Jr., and Andrew Feickert, Special Operations Forces (SOF) and CIA Paramilitary Operations: Issues for Congress,RS22017 (Washington, DC: Congressional Research Service, December 6, 2006), 4, available at http://fas.org /sgp/crs/intel/RS22017.pdf.
14 Title 10, U.S. Code, Armed Forces, § 127e(f); see also Ronald W. Reagan National Defense Authorization Act for Fiscal Year 2005, Pub. L. No. 108-375, 118 Stat. 1811 (2004), available at <www.congress.gov /108/plaws/publ375/PLAW-108publ375.pdf>.
15 Title 10, U.S. Code, Armed Forces, § 167(e).
16 Title 10, U.S. Code, Armed Forces, § 127e(a).
17 Jennifer D. Kibbe, "Conducting Shadow Wars," Journal of National Security Law & Policy 5, no. 2 (January 24, 2012), 373–392, available at http://jnslp.com /2012/01/24/conducting-shadow-wars/.
18 EO 12333, "United States Intelligence Activities," § ?.?(a)(?).
19 Bob Woodward, "CIA Told to Do 'Whatever Necessary' to Kill bin Laden," Washington Post, October 21, 2001, available at <www.washingtonpost.com /wp-dyn/content/article/2007/11/18/AR2007111800655.html>.
20 "PresidentObama on Death of Osama bin Laden," video, 9.29, The White House, available at https://obamawhitehouse.ar chives.gov/blog/2011/05/02/osama-bin-laden-dead.
21 "CIAChief Panetta: Obama Made 'Gutsy' Decision on Bin Laden Raid," video, 11:28, PBS News Hour, May 3, 2011, available at <www.pbs.org /newshour/bb/terrorism-jan-june11-panetta_05-03/>.
22 NickRasmussen, "The Weight of One Mission: Recounting the Death of Usama Bin Laden, Five Years Later," The White House, May 2, 2016, available athttps://obamawhitehouse.ar chives.gov/blog/2016/05/02/weight-one-mission-recounting-death-usama-bin-laden-five-years-later.
23 For an excellent discussion of the increased role of the military in covert action, see Kibbe, "Conducting Shadow Wars."
24 George H.W. Bush and Brent Scowcroft, A World Transformed (New York: Knopf, 1998), 489.
25 Jane Mayer, "The Manipulator," The New Yorker,June 7, 2004, available at <www.newyorker.com /magazine/2004/06/07/the-manipulator> see Seymour M. Hersh, "The Debate Within," The New Yorker,March 11, 2002, available at <           www.newyorker.com /magazine/2002/03/11/the-debate-within> see also Peter Rost, "How a Public Relations Firm Helped Start the War," Huffington Post, December 6, 2017, available at <           www.huffingtonpost.com /dr-peter-rost/how-a-public-relations-fi_b_22912.html>.
26 Title 50, U.S. Code, War and National Defense, § 3093(f).
27 Mayer, "The Manipulator."
28 "EventsLeading Up to the 2003 Invasion of Iraq," History Commons; see especially "June 1992," available at <www.historycommons.org /timeline.jsp?timeline=complete_timeline_of_the_2003_invasion_of_iraq&complete_timeline_of_the_2003_invasion_of_iraq_iraq_in_the_1980s_and_1990s=&startpos=100>.
29 Mayer, "The Manipulator."
30 Dan Murphy, "Discredited Iraqi Ally Regroups," Christian Science Monitor, June 15, 2004, available at <www.csmonitor.com /2004/0615/p01s04-woiq.html>.
31 Robert Dreyfuss, "Tinker, Banker, NeoCon, Spy," The American Prospect, October 23, 2002, available at http://prospect.org /article/tinker-banker-neocon-spy.
32 "Buying the War," Bill Moyers Journal, PBS, April 25, 2007, available at <www.pbs.org /moyers/journal/btw/transcript1.html>.
33 See Mayer, "The Manipulator."
34 "Events Leading Up to the 2003 Invasion of Iraq."
35 Mayer, "The Manipulator."
36 AndrewCockburn, "What He Did as a Catspaw for Teheran: How He Nearly Bankrupted Jordan; the Billions He Stands to Make Out of the New Iraq," Counterpunch.org,May 20, 2004, available at <www.counterpunch.org /2004/05/20/what-he-did-as-a-catspaw-for-teheran-how-he-nearly-bankrupted-jordan-the-billions-he-stands-to-make-out-of-the-new-iraq/>.
37 Ibid.
38 Dreyfuss, "Tinker, Banker, NeoCon, Spy."
39 The book used for this example—Mike Tucker and Charles S. Faddis, Operation Hotel California: The Clandestine War Inside Iraq (Guilford, CT: The Lyons Press, 2009)—containsinformation that the CIA reviewed and cleared as not disclosing classified information, but the authors' claims are unconfirmed. They are included here for illustrative purposes only.
40 Dana Priest, "Covert CIA Program Withstands New Furor," Washington Post,December 30, 2005, available at <www.washingtonpost.com /wp-dyn/content/article/2005/12/29/AR2005122901585.html>.Priest sourced the information about the CIA paramilitary teams to "a dozen current and former intelligence officials and congressional and executive branch sources."
41 Tucker and Faddis, Operation Hotel California, 4–9, 24–27, 35–48.
42 Ibid., xiv, xxii.
43 Ibid., 1–2, 10.
44 Ibid., 3–24.
45 Priest, "Covert CIA Program Withstands New Furor."
46 Dana Priest and Josh White, "Before the War, CIA Reportedly Trained a Team of Iraqis to Aid U.S.," Washington Post, August 3, 2005, available at <www.washingtonpost.com /wp-dyn/content/article/2005/08/02/AR2005080201579.html>.
47 George Tenet and Bill Harlow, At the Center of the Storm: My Years at the CIA (New York: HarperCollins, 2007), 388–389.
48 Tucker and Faddis, Operation Hotel California, 44.
49 Ibid., 36–48.
50 Ibid., 33–48.

Load-Date: August 12, 2019


End of Document


Keeping Children Safe In The 'Internet Of Things' Age
Eurasia Review
June 24, 2019 Monday


Copyright 2019 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 465 words
Body


Children need protection when using programmable Internet computing 
devices - and Lancaster University scientists have drawn up new 
guidelines to help designers build in safeguards.
Young people are growing up in a digital world where everyday 
objects contain sensors and stream data to and from the Internet - a 
trend known collectively as the Internet of Things (IoT).
Children are also getting hands-on - using small-scale 
easy-to-program devices such as the BBC micro:bit to experiment and get 
creative with digital technologies.
These kinds of devices are very useful educational tools that 
children are using to build their knowledge and digital skills - and the
developers of the BBC micro:bit took a very considered ethical approach
to developing their device. However, unless properly considered, 
Internet-connected devices can present risks to children and others 
around them.
These risks can include peer-to-peer abuse or bullying, dangers 
of abuse by adults, as well as risks related to the use, exploitation, 
commercialisation, or insecure management of any data the children 
generate by using the devices.
Dr Bran Knowles, Lecturer in Data Science at Lancaster 
University's School of Computing and Communications, said: "Children who
are learning to programme  IoT devices still have critical gaps in 
their understanding of privacy and security. In addition, their parents 
may also lack technical understanding of IoT, which makes it difficult 
for them to help ensure their children are managing their privacy and 
keeping safe.
"Formal training is available for online safety issues such as 
social media bullying and sexting, but, as yet, there is no IoT 
component to this curriculum.
"It is essential therefore that the designers of these IoT 
devices anticipate the full spectrum of contexts in which children may 
use these devices and adopt strategies that will ensure they have 
properly considered, and mitigated, the potential safety and privacy 
risks to children and their families.
"Our research provides a framework to help designers approach these 
critical risks with their own devices, while still enabling these 
devices to have enough functions activated so that they still provide a 
fun learning experience." she said.
The Lancaster University team's methodology includes working with
supervised groups of school children to explore a wide range of ways 
that young people may want to use Internet-connected computing devices.
The findings from these sessions, alongside findings from 
workshops with child safety experts, help designers to create 
fictionalised 'use scenarios' that provide a detailed picture of how 
children will use the devices. Key questions can emerge from these 
scenarios that form the basis for developing risk mitigation checklists 
when designing digital tools.

Load-Date: June 24, 2019


End of Document


Querying Big Data Just Got Universal
Eurasia Review
June 26, 2019 Wednesday


Copyright 2019 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 489 words
Body


To solve one of the key obstacles in big-data science, KAUST 
researchers have created a framework for searching very large datasets 
that runs easily on different computing architectures. Their achievement
allows researchers to concentrate on advancing the search engine, or 
query engine, itself rather than on painstakingly coding for specific 
computing platforms.
Big data is one of the most promising yet challenging aspects of 
today's information-heavy world. While the huge and ever-expanding sets 
of information, such as online-collected data or genetic information, 
could hold powerful insights for science and humanity, processing and 
interrogating all this data require highly sophisticated techniques.
Many different approaches to querying big data have been explored. 
But one of the most powerful and computationally effective is based on 
analyzing data with a subject-predicate-object triplestore structure of 
the form (e.g., apple, is a, fruit). This structure lends itself to 
being treated like a graph with edges and vertices, and this 
characteristic has been used to code query engines for specific 
computing architectures for maximum efficiency. However, such 
architecture-specific approaches cannot be readily ported to different 
platforms, limiting the opportunities for innovation and advancement in 
analytics.
"Modern computing systems provide diverse platforms and 
accelerators, and programming them can be intimidating and time 
consuming," say Fuad Jamour and Yanzhao Chen, Ph.D. candidates in Panos 
Kalnis's group in KAUST's Extreme Computing Research Center. "Our 
research group focuses on building systems and algorithms for processing
and analyzing very large datasets. This research addresses the desire 
to write a program once and then use it across different platforms."
Rather than the previously used graph-traversal or exhaustive 
relational-indexing approaches, the group queried triplestore data by 
using an applied mathematical approach called sparse-matrix algebra.
"Our paper describes the first research graph-query engine with 
matrix algebra at its core to address the issue of portability," says 
Jamour. "Most existing graph-query engines are designed for single 
computers or small distributed-memory systems. And porting existing 
engines to large distributed-memory systems, like supercomputers, 
involves significant engineering effort. Our sparse-matrix algebra 
scheme can be used to build scalable, portable and efficient graph-query
engines."
The team's experiments on large-scale real and synthetic datasets 
achieved performance comparable with, or better than, existing 
specialized approaches for complex queries. Their scheme also has the 
capacity to scale up to very large computing infrastructures handling 
datasets of up to 512 billion triples.
"These ideas can facilitate building analytics components in graph 
databases with cutting-edge performance, which is currently in high 
demand," says Chen.

Load-Date: June 26, 2019


End of Document


Too Many Businesses Failing To Properly Embrace AI Into Processes, Not Reaping Benefits
Eurasia Review
June 12, 2019 Wednesday


Copyright 2019 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 503 words
Body


Businesses actively embracing artificial intelligence and striving to
bring technological advancements into their operations are reaping 
dividends not seen by companies who fail to properly adapt and adopt.
While most business and technology leaders are optimistic about the 
value-creating potential of AI in their enterprise - Enterprise 
Cognitive Computing (ECC) - the actual rate of adoption is low, and 
benefits have proved elusive for a majority of organisations.
A study involving Lancaster University Management School's Centre for Technological Futures and MIT Sloan School's Center for Information Systems Research, published in MIT Sloan Management Review,  examined adoption of ECC in 150 organisations from various industries  across Europe, North America, Asia and Australia, to understand why.
Companies who are able to generate value from ECC do so having built
a number of organisational capabilities. They develop skills for data 
science and algorithmic expertise, shape their business and the roles of
staff to accommodate and integrate ECC initiatives, and account for the
need to include human judgement and digital inquisitiveness in order to
see benefits. Such businesses have strong domain expertise and a good 
operating IT infrastructure.
They apply these capabilities to a number of practices across the 
organisation, including co-creation involving people from across the 
business through the lifecycle of ECC applications, and developing use 
cases around pressing and meaningful business problems. They have 
strategies for managing and training AI algorithms within the ECC 
applications, and - importantly - they both create a positive buzz about
ECC and at the same time have realistic and clear-eyed expectations of 
the benefits they can expect.
Professor Monideepa Tarafdar,  Professor of Information Systems and Co-Director of the Centre for  Technological Futures at Lancaster University, who co-authored the  study, said: "Bringing AI successfully into a business has many positive  effects. It can free employees to perform tasks that require  adaptability and creativity found in human input, enhance operations,  and augment employees' skills.
"But one of our studies showed half of companies have no ECC in 
place, and only half of those who have believe it to have produced 
measurable value. This suggests that generating value from such AI is 
not easy if organizations do not develop the needed capabilities and 
practices.
"Companies that are serious about AI applications spend the money to
hire the right staff and develop the business practices that ensure ECC
can improve their business operations, rather than spending money and 
harnessing massive amounts of data with no obvious benefits."
She added: "Having the proper capabilities in place enables 
employees to execute the new practices, and the practices in turn 
strengthen the capabilities of the ECC programmes. Such a virtuous cycle
can lead to dramatic improvements in operational and financial 
performance, and customer satisfaction."

Load-Date: June 12, 2019


End of Document


Artificial Intelligence Can Predict Premature Death
Eurasia Review
March 28, 2019 Thursday


Copyright 2019 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 643 words
Body


Computers which are capable of teaching themselves to predict 
premature death could greatly improve preventative healthcare in the 
future, suggests a new study by experts at the University of Nottingham.
The team of healthcare data scientists and doctors have developed 
and tested a system of computer-based 'machine learning' algorithms to 
predict the risk of early death due to chronic disease in a large 
middle-aged population.
They found this AI system was very accurate in its predictions and 
performed better than the current standard approach to prediction 
developed by human experts. The study is published by PLOS ONE in a special collections edition of "Machine Learning in Health and Biomedicine".
The team used health data from just over half a million people aged 
between 40 and 69 recruited to the UK Biobank between 2006 and 2010 and 
followed up until 2016.
Leading the work, Assistant Professor of Epidemiology and Data 
Science, Dr Stephen Weng, said: "Preventative healthcare is a growing 
priority in the fight against serious diseases so we have been working 
for a number of years to improve the accuracy of computerised health 
risk assessment in the general population. Most applications focus on a 
single disease area but predicting death due to several different 
disease outcomes is highly complex, especially given environmental and 
individual factors that may affect them.
"We have taken a major step forward in this field by developing a 
unique and holistic approach to predicting a person's risk of premature 
death by machine-learning. This uses computers to build new risk 
prediction models that take into account a wide range of demographic, 
biometric, clinical and lifestyle factors for each individual assessed, 
even their dietary consumption of fruit, vegetables and meat per day.
"We mapped the resulting predictions to mortality data from the 
cohort, using Office of National Statistics death records, the UK cancer
registry and 'hospital episodes' statistics. We found machine learned 
algorithms were significantly more accurate in predicting death than the
standard prediction models developed by a human expert."
The AI machine learning models used in the new study are known as 
'random forest' and 'deep learning'. These were pitched against the 
traditionally-used 'Cox regression' prediction model based on age and 
gender - found to be the least accurate at predicting mortality - and 
also a multivariate Cox model which worked better but tended to 
over-predict risk.
Professor Joe Kai, one of the clinical academics working on the 
project, said: "There is currently intense interest in the potential to 
use 'AI' or 'machine-learning' to better predict health outcomes. In 
some situations we may find it helps, in others it may not. In this 
particular case, we have shown that with careful tuning, these 
algorithms can usefully improve prediction.
"These techniques can be new to many in health research, and 
difficult to follow. We believe that by clearly reporting these methods 
in a transparent way, this could help with scientific verification and 
future development of this exciting field for health care."
This new study builds on previous work by the Nottingham team which 
showed that four different AI algorithms, 'random forest', 'logistic 
regression', 'gradient boosting' and 'neural networks', were 
significantly better at predicting cardiovascular disease than an 
established algorithm used in current cardiology guidelines. This 
earlier study is available here.
The Nottingham researchers predict that AI will play a vital part in
the development of future tools capable of delivering personalised 
medicine, tailoring risk management to individual patients. Further 
research requires verifying and validating these AI algorithms in other 
population groups and exploring ways to implement these systems into 
routine healthcare.

Load-Date: March 28, 2019


End of Document


New Satellite Keeps Close Watch On Antarctic Ice Loss
Eurasia Review
March 7, 2019 Thursday


Copyright 2019 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 588 words
Body


A recently-launched satellite mission has captured precision data on 
the elevation of the Antarctic ice sheet proving a valuable addition to 
monitoring efforts in the region, according to work published this week 
in The Cryosphere.
From its orbit 815 km above the Earth, the Sentinel-3 satellite 
was able to detect the height of the ice surface to within tens of 
centimetres , tests carried out at the remote Lake Vostok validation 
site in East Antarctica showed.
The study, led by researchers from the new joint Lancaster 
University-CEH Centre of Excellence in Environmental Data Science 
(CEEDS), alongside European Space Agency (ESA) and industry partners, 
shows the potential of Sentinel-3 - one of the EU Copernicus satellite 
missions - to contribute towards long-term ice sheet monitoring 
programmes.
The scientists also found that Sentinel-3 could detect areas 
where the ice surface was rapidly lowering, thereby establishing the 
satellite's credentials as a new platform which can help to monitor 
Antarctica's contribution to sea level rise.
Determining how well Sentinel-3 functions over ice sheets is 
particularly important given that CryoSat-2, ESA's dedicated polar 
mission, is already operating well beyond its planned lifetime.
CryoSat-2 was designed to fly in a unique orbit, to maximise 
coverage of coastal areas of the ice sheet, and to map the regions close
to the North and South Poles that were beyond the reach of previous 
satellites.
Although Sentinel-3 - which has to balance many applications - 
cannot match this coverage, it still holds potential as a valuable 
long-term monitoring platform for decades to come.
Dr Mal McMillan, lead author and co-Director of CEEDS, said: 
"Although the Sentinel-3 altimeter was primarily designed to monitor the
oceans, we wanted to test how well it works over ice, and to see 
whether it is able to detect signs of glaciological change. Through the 
support offered by ESA's Scientific Exploitation of Operational Missions
element, we have been able to study the performance of the Sentinel-3 
mission for several years now, and we are pleased to be able to publish 
these results."
He added: "From what we can see here, with just two years' worth 
of data, Sentinel-3 is going to be a very useful tool for surveying the 
Antarctic ice sheet."
Sentinel-3 uses a radar technique called Delay-Doppler altimetry 
to make high resolution measurements of the height of the ice sheet.
Where the ice is relatively flat, Sentinel-3 could map its height
to within 10 cm of measurements taken by aircraft, as part of NASA's 
Operation Icebridge campaign.
Dr McMillan explained: "This level of accuracy means that we can 
also use Sentinel-3 to track important features on the ice surface, like
the imprint of active subglacial lakes draining and refilling beneath 
several kilometres of ice."
Using radar satellites like Sentinel-3 over ice nonetheless has 
its challenges.  For example, measurements over Antarctica's steeper, 
craggy coastal areas were less accurate because of how the rough 
landscape affects the radar signal.
Future research into Sentinel-3's performance, as well as further
improvements to data processing, will help take these effects into 
account. In the meantime, Sentinel-3 has already shown its value as a 
new tool for detecting ice sheet change.
Co-author Jérôme Benveniste of the European Space Agency 
summarised: "We are delighted with the early promise shown by Sentinel-3
for ice sheet monitoring, and are increasingly confident that it will 
be a long-term asset to climate science."

Load-Date: March 7, 2019


End of Document


Sri Lanka: Rapidly Ageing Population, Opportunity For Business Expansion
Eurasia Review
January 25, 2019 Friday


Copyright 2019 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 484 words
Body


Sri Lanka's rapidly ageing population will offer so many  opportunities for businesses and create a positive impact on the overall  economy if Sri Lanka puts right policies in place, said Lakshman Dissanayake, Senior Professor in Demography and Vice Chancellor of University of  Colombo.
Sri Lanka needs to put right policies in place to better  garner benefits from its first demographic dividend, and Sri Lanka's  rapidly ageing population will offer so many opportunities for  businesses and create a positive impact on the overall economy.
"They will certainly request and demand for certain things and it  will have a positive impact on local businesses in the future. We can  make our ageing population as an opportunity, if we can make them  productive and healthy by improving the quality of our current labour  force. If the government is able to provide decent employment  opportunities to the current labour force and improve their savings  capacity; by the time they reach their elderly age of 60-65 or at the  time of their retirement, they will accumulate some wealth in order to  take care of themselves during their elderly age," Prof. Dissanayake  told the 15th South Asian Economic Students' Meet, jointly organized by  the Faculty of Economics, University of Colombo and the World Bank in  Colombo.
Noting that the majority of planners in Sri Lanka don't pay  much attention to the changing ageing structure of the population,  especially the age sex structure of the population, which determines the  needs and requirements of the society, Prof. Dissanayake emphasised  that the needs and requirements of the society are primarily determined  by each individual's age and gender.
"It is very important for us to  look at several aspects of the economy; it is not just looking at the  core economic ideas. Also, we have to bring or add some  multi-disciplinary flavours into the overall economic activities," he  said.
As per calculations of Prof. Dissanayake, the first demographic  dividend of Sri Lanka will continue until 2027. According to him  country's first demographic dividend was created due to historical  declining of fertility and improved survival chances.
Second  demographic dividend starts with the termination of first demographic  dividend. In Sri Lanka according to his calculation, the terminal year  for the first demographic dividend is in 2037.Sri Lanka is ageing and  its ageing process is faster than most of countries in the region.
In  2012, persons above the age of 60 category accounted for 12.4% of the  total population. By 2031, as per my medium-term projections, that will  come to about 22%. And a lot of people are alarmed by looking at this  ageing process." he said.
He further added that artificial intelligence, machine learning, data
science and data analytics could be utilized to cater to the diverse 
requirements of ageing population in the near future.

Load-Date: January 25, 2019


End of Document


How Healthy Will We Be In 2040?
Eurasia Review
October 18, 2018 Thursday


Copyright 2018 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 1276 words
Body


A new scientific study of forecasts and alternative scenarios for life expectancy and major causes of death in 2040 shows all countries are likely to experience at least a slight increase in lifespans. In contrast, one scenario finds nearly half of all nations could face lower life expectancies.
The rankings of nations' life expectancies offer new insights into their health status.
For example, China, with an average life expectancy of 76.3 years in 2016, ranked 68th among 195 nations. However, if recent health trends continue it could rise to a rank of 39th in 2040 with an average life expectancy of 81.9 years, an increase of 5.6 years.
In contrast, the United States in 2016 ranked 43rd with an average lifespan of 78.7 years. In 2040, life expectancy is forecast to increase only 1.1 years to 79.8, but dropping in rank to 64th. By comparison, the United Kingdom had a lifespan of 80.8 years in 2016 and is expected to increase to 83.3, raising its rank from 26th to 23rd in 2040.
In addition, the study, published today in the international medical journal The Lancet, projects a significant increase in deaths from non-communicable diseases (NCDs), including diabetes, chronic obstructive pulmonary disease (COPD), chronic kidney disease, and lung cancer, as well as worsening health outcomes linked to obesity.
However, there is "great potential to alter the downward trajectory of health" by addressing key risk factors, levels of education, and per capita income, authors say.
"The future of the world's health is not pre-ordained, and there is a wide range of plausible trajectories," said Dr. Kyle Foreman, Director of Data Science at the Institute for Health Metrics and Evaluation (IHME) at the University of Washington, and lead author on the study. "But whether we see significant progress or stagnation depends on how well or poorly health systems address key health drivers."
The top five health drivers that explain most of the future trajectory for premature mortality are high blood pressure, high body mass index, high blood sugar, tobacco use, and alcohol use, Foreman said. Air pollution ranked sixth.
The study is available at http://www.healthdata.org .
In addition to China, several other nations are expected in 2040 to increase substantially in their rankings in terms of life expectancy, including:
Syria is expected to rise most in rank globally - from 137th in 2016 to 80th in 2040 -likely, according to the authors, due to a conservative model for conflict;
Nigeria from 157th to 123rd; and
Indonesia from 117th to 100th
In contrast, Palestine is expected to drop the most in its life expectancy ranking - from 114th in 2016 to 152nd in 2040. Moreover, several high-income nations are forecast to drop substantially in their rankings, including:
United States, dropping the most for high-income countries, from 43rd in 2016 to 64th in 2040;
Canada from 17th to 27th ;
Norway from 12th to 20th ;
Taiwan (Province of China) from 35th to 42nd ;
Belgium from 21st to 28th ;
Netherlands from 15th to 21st ;
The rankings also find that Spain is expected to place first in the world in 2040 (average lifespan of 85.8 years), a rise from fourth in 2016 (average lifespan of 82.9 years). Japan, ranked first in 2016 (average lifespan 83.7 years), will drop to second place in 2040 (average lifespan 85.7 years).
Rounding out the top 10 for 2040 are:
Singapore (average lifespan 85.4 years) ranked third, as compared to 83.3 years in 2016 and ranking also of third
Switzerland (average lifespan 85.2 years), as compared to 83.3 years in 2016 and ranking of second
Portugal (average lifespan 84.5 years), as compared to 81.0 years in 2016 and ranking of 23rd
Italy (average lifespan 84.5 years), as compared to 82.3 years in 2016 and ranking of seventh
Israel (average lifespan 84.4 years), as compared to 82.1 years in 2016 and ranking of 13th
France (average lifespan 84.3 years), as compared to 82.3 years in 2016 and ranking also of eighth
Luxembourg (average lifespan 84.1 years) as compared to 82.2 years in 2016 and ranking of 10th
Australia (average lifespan 84.1 years), as compared to 82.5 years in 2016 and ranking of fifth.
Among those top 10 nations, even their 'worse' scenarios in 2040 remain above 80 years. In stark contrast, the bottom-ranked nations, which include Lesotho, Swaziland, Central African Republic, and South Africa, the "better" and "worse scenarios" in 2040 range from a high of 75.3 years in South Africa ("better" scenario) to a low of 45.3 years in Lesotho ("worse scenario"), a 30-year difference.
"Inequalities will continue to be large," said IHME Director Dr. Christopher Murray. "The gap between the 'better' and 'worse' scenarios will narrow but will still be significant. In a substantial number of countries, too many people will continue earning relatively low incomes, remain poorly educated, and die prematurely. But nations could make faster progress by helping people tackle the major risks, especially smoking and poor diet."
In a "worse" scenario, life expectancy decreases in nearly half of all countries over the next generation. Specifically, 87 countries will experience a decline, and 57 will see an increase of one year or more. In contrast, in the "better" scenario, 158 countries will see life expectancy gains of at least five years, while 46 nations may see gains of 10 years or more.
The future shift toward increased premature mortality from NCDs and injuries and away from communicable diseases is apparent by the changing proportions of the top 10 causes of premature death.
In 2016, four of the top 10 causes of premature mortality were NCDs or injuries; in contrast, in 2040, that number increases to eight. The eight NCD or injury causes in the top ten in 2040 are expected to be ischemic heart disease, stroke, COPD, chronic kidney disease, Alzheimer's disease, diabetes, road injuries, and lung cancer.
The study is unprecedented in scope, Foreman said, and provides more robust statistical modeling and more comprehensive and detailed estimates of risk factors and diseases than previous forecasts from the United Nations and other population studies institutes.
IHME researchers leveraged data from the Global Burden of Disease (GBD) study to produce forecasts and alternative "better" and "worse" scenarios for life expectancy and mortality due to 250 causes of death for 195 countries and territories.
Researchers produced forecasts of independent drivers of health, including sociodemographic measurements of fertility, per capita income, and years of education, along with 79 independent drivers of health such as smoking, high body mass index, and lack of clean water and sanitation. They then used information on how each of these independent drivers affects specific causes of death to develop forecasts of mortality.
"The range of 'better' and 'worse' scenarios enables stakeholders to examine potential changes to improve health systems - locally, nationally, and globally," Murray said. "These scenarios offer new insights and help to frame health planning, especially regarding long lag periods between initial investments and their impacts, such as in the research and development of drugs."
In addition to calling attention to the growing importance of non-communicable diseases, the analysis exposes a substantial risk of HIV/AIDS mortality rebounding, which could undo recent life expectancy gains in several nations in sub-Saharan Africa.
Furthermore, while NCDs are projected to rise in many low-income countries, communicable, maternal, neonatal, and nutritional diseases are likely to remain among the leading causes of early death, thereby creating a "double burden" of disease.

Load-Date: October 18, 2018


End of Document


An AI For Deciphering What Animals Do All Day
Eurasia Review
May 1, 2018 Tuesday


Copyright 2018 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 624 words
Body


Much of what biologists have learned about animal behavior over the years has come from careful observation and painstaking notes. There could soon be an easier way.
In a new study in the journal eLife, researchers at Columbia University show how an algorithm for filtering spam can learn to pick out, from hours of video footage, the full behavioral repertoire of tiny, pond-dwelling Hydra. A close relative of coral, jellies and sea anemones, Hydra is so primitive that it lacks a backbone or brain. But when it moves, feeds and evades predators, it behaves in predictable ways that a computer can recognize.
By comparing Hydra's behaviors to the firing of its neurons, the researchers hope to eventually understand how its nervous system, and that of more complex animals, works. "People have used machine learning algorithms to partly analyze how a fruit fly flies, and how a worm crawls, but this is the first systematic description of an animal's behavior," said the study's senior author, Rafael Yuste, a neuroscientist at Columbia University and a member of Columbia's Data Science Institute. "Now that we can measure the entirety of Hydra's behavior in real-time, we can see if it can learn, and if so, how its neurons respond."
Hydra's ancestors appeared on Earth some 700 million years ago, before the Cambrian explosion that gave rise to most modern species. Instead of a brain, hundreds of neurons run along its narrow, translucent body coordinating behaviors that range from basic - curling into a ball to avoid predators - to sophisticated - somersaulting to get around.
In an earlier study in Current Biology, Yuste and his colleagues recorded all of its neurons firing in real-time and discovered four sets of neural circuits that control four distinct elongation and bending behaviors, paving the way to understand how Hydra's nervous system regulates its behavior.
In the current study, the team went a step further by attempting to catalog Hydra's complete set of behaviors. To do so, they applied the popular "bag of words" classification algorithm to hours of footage tracking Hydra's every move. Just as the algorithm analyzes how often words appear in a body of text to pick out topics (and flag, for example, patterns resembling spam), it cycled through the Hydra video and identified repetitive movements.
Their algorithm recognized 10 previously described behaviors, and measured how six of those behaviors responded to varying environmental conditions. To the researchers' surprise, Hydra's behavior barely changed. "Whether you fed it or not, turned the light on or off, it did the same thing over and over again like an Energizer bunny," said Yuste.
The researchers think Hydra may have evolved a way of adjusting to its environment as if on auto-pilot. They are now experimenting with other stimuli to see if Hydra will respond and learn. Eventually, they hope to crack its neural code with a model that shows how its networks of neurons create behavior.
Lessons learned from Hydra may also be useful to a branch of engineering concerned with maintaining stability and precise control in machines, from ships to planes, navigating in highly variable conditions.
The nervous systems of even simple animals like Hydra have evolved to maintain constancy in their behaviors, said Yuste. If engineers could unlock their secret, technology could be infused with biologically-inspired controls that have evolved over hundreds of millions of years.
"Reverse engineering Hydra has the potential to teach us so many things," said the study's lead author, Shuting Han, a graduate student at Columbia.
Formerly in Yuste's lab, the study's other authors, Ekaterina Taralova and Christophe Dupre, are now at the startup Zoox Inc. and Harvard.

Load-Date: May 1, 2018


End of Document


Helping Discover The Diversity In Soil
Eurasia Review
February 9, 2020 Sunday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 807 words
Body


Microbiological communities, which include bacteria, single-celled organisms and nematodes, reveal a great deal of information about the state of soils. All around the world, a lot of research is being performed on this biodiversity at a genetic level but third parties are not always able to put these research results to the best possible use. The reason for this: The information recorded in databases varies in terms of quality. UFZ researchers have now built up a new metadata-database for terrestrial metagenomes with over 15,000 datasets, which is intended to make work easier for scientists. This was published in the scientific journalNucleic Acids Research.
More than 202,000 metagenomes, i.e. the entire genetic information contained in a given soil sample, can be found in the two most important databases in which microbiologists can archive research data: the MG-Rast and Sequence Read Archive (SRA) repositories. Here, international researchers have recorded where they performed investigations into microbiological communities or genome sequencing on the seabed, in forests, in grassland or on rocks, and their findings. By doing so, they enable other researchers to use this data in their own research activities and compare it to their own findings. And it saves them from having to repeat time-consuming work on questions that may have already been answered.
The researchers do, however, come across obstacles to their work time and time again: the datasets are often incomplete and not uniformly marked. "This makes it more difficult for interested users to further process the data," says Dr Ulisses Nunes da Rocha, microbiological ecologist at the UFZ and one of the study's lead authors. This starts with minor details, such as the temperature. Temperature can be recorded in different ways using Fahrenheit, Kelvin or Celsius; the way in which the units are abbreviated varies in addition. But there is also uncertainty with regard to what may seem to be basic issues; for example, some scientists around the world have different understandings of the exact definition of a biome (the scientific term for a large-scale habitat). All this, says Dr da Rocha, makes it more difficult to use the data efficiently.
Dr Ulisses Nunes da Rocha and his team have now filtered the metagenome data out of the MG Rast and SRA datasets collected by researchers in the terrestrial environment around the world. In contrast, they screened out data collected from the seas and oceans. Exactly 15,022 metagenome datasets from forests or grasslands or from the subsoil originating in 84 countries were brought together in the new metadata-database. They did not develop any new scientific standards for the exact description of this metadata, such as the geographical coordinates, the pH value or the temperatures involved but used an existing method of standardisation. "The metadata-database helps researchers whose work centres on the terrestrial environment and who want to incorporate data of this kind into their own work," says the UFZ researcher. Instead of performing complex laboratory experiments for the purpose of CO2 fixation or establishing the effect of pesticides on microbiological communities, to name two examples, researchers can consult the database to see if researchers somewhere around the world have already performed similar experiments on this topic and have made their data available.
The UFZ's freely accessible "TerrestrialMetagenomeDB" metadata-database went online at the beginning of November. Users can initially use six filters, such as the origin of the biome, sample type or the data source to search the database and, if necessary, track down more specific data by means of a further 33 filters. Secondly, another approach provides an interactive map of the world that users can use to look for datasets according to geographical features. Three video tutorials offer additional user support on how to best conduct research and download the data. The metadata-database is automatically updated twice a year - in January and July. As part of this process, new or corrected datasets are automatically retrieved from the MG-Rast and SRA repositories, assuming that groups of scientists have adapted the attributes of their own data to the standards of the new database. There is great potential: there are another 100,000 or so datasets on hold containing data on terrestrial metagenomes that could not be standardised to date because the data had not been entered precisely enough. For Dr Ulisses Nunes da Rocha and his UFZ "Microbiological Systems Data Science" working group, this is only the first step in a process of facilitating big data analyses of microbiological communities in terrestrial systems on a global scale.
The post Helping Discover The Diversity In Soil appeared first on Eurasia Review.

Load-Date: February 9, 2020


End of Document


Arab Spring Success Story: Tunisians Vote – Analysis
Eurasia Review
September 13, 2019 Friday


Copyright 2019 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 1518 words
Body


Tunisia's  election could surprise as voters coalesce around two camps, generally  conservative versus progressive, to choose among 26 candidates.
By Dhafer Malouche*
As Tunisia prepares for an election, its fourth since a popular 
uprising overthrew dictator Zine El Abidine Ben Ali, enthusiasm has 
subsided, and candidates have coalesced around two broad camps, offering
clarity to voters. Many in the region will closely watch the electoral 
outcome in this country of 11.5 million that witnessed the birth of Arab
Spring.
The wave of Arab Spring protests – starting in Tunisia before moving 
on to Egypt, Libya and Syria – jolted the region in 2011. In Egypt, a 
military coup stopped the democratization process. Libya and Syria now 
endure bloody civil wars: two governments fight in Libya, and a dictator
clings to power in Syria after the Islamic State extremists occupied 
large sections of the nation and conducted terrorist attacks in Europe, 
Tunisia and beyond. Only Tunisia secured a successful, sustained 
peaceful transition of power from dictatorship to democracy, though 
Algerians have recently begun seeking real democratic transition for 
their country.
The September 15 election is Tunisia's fourth since December 2010 and  the second for selecting a president. Tunisia, located along the  southern edge of the Mediterranean Sea, is less than 300 kilometers from  Europe, and failure in the democratization process could threaten  European security. The politics and procedures of democratic elections  are complex, and as noted by the World Bank, the revolution  for an improved economy and jobs is not yet finished. Tunisia has  struggled in recent years, with GDP reaching a peak of $47 million in  2014 and declining since. The unemployment rate exceeds 15 percent, even  higher among youths, and more than 10 percent of the population lives  abroad.
According to the new Tunisian constitution approved in 2014, the 
presidential elections take place in two rounds, two weeks apart. This 
year, 26 candidates compete in the first round to determine two 
finalists for the second round. The field is crowded, and the country is
polarized between progressives and conservatives, much as it was for 
the 2014 presidential election. The first round then had 27 candidates 
with more than 3.3 million Tunisians voting. The top five candidates 
combined collected almost 3 million votes, or nearly 90 percent. The 
first-round finalists received more than 70 percent of the vote: Beji 
Caid Essebsi, who represented the liberal-progressive camp including 
socialists and former communists, along with well-known figures from the
Ben Ali era, secured 39 percent; Moncef Marzouki representing a range 
of conservative and pro-revolution stances that opposed Ben Ali, 
received 33.4 percent. For the second round, the liberal camp delivered 
1.8 million votes, or 54 percent, to Caid Essebsi who became modern 
Tunisia's first democratically elected president. The 
revolutionary-conservative camp mustered 46 percent of the vote for 
Marzouki.
For this election, political opinion polls, media punditry and reactions of Tunisians on social networks reveal two trends:
? First, the public expresses considerable disgust towards the 
political class, which could reduce voter turnout, expected to be lower 
than in 2014, when 63 percent of the eligible population voted, but not 
as low as the 36 percent turnout for the May 2018 municipal elections. 
Estimates suggest candidates will be fortunate if the turnout reaches 55
percent, with participation of about 3 million voters, about 10 percent
less than in 2014. About 70 percent of the country's 11 million people 
are over age 18 and eligible to vote.
? Second, voters migrate between the liberal-secular and the  revolutionary-conservative camps, and distribution is fluid. Estimates  suggest that the liberal-progressive camp is composed of 1.5 million  voters and the revolutionary-conservative camp includes 1.1 million.  Voters could be described as less polarized today than in 2014, when a  new political class emerged for the Tunisian public. For 23 years, all  political life and power had been concentrated in one man with any  opposition short-lived and as useful as a mausoleum. The election was  held three years after Ben Ali stepped down and fled.
Crowded field: Tunisia's 26 presidential candidates can be generally divided into two camps, pro- and anti-revolution, with progressives and liberals on either side
The 2014 debates focused on two personalities: Caid Essebsi and 
Marzouki. The first already had a tremendous political career since the 
Bourguiba era, summoned from retirement in 2011 for the prime minister 
post after the revolution and managing organization of the 2012 
elections. Such prominence boosted him in the 2014 polls, and he served 
as president for five years, until his sudden death in July. Amid spontaneous national mourning, Tunisians advanced organization of the next elections.
Since 2014, the last five years of power in Tunisia have been 
characterized by a coalition between two longtime antagonists: the 
Islamic conservative group Nahdha and Nidaa Tounes, a political party 
created by Caid Essebsi in 2013 and winner of the 2014 parliamentary 
elections. Marzouki was not involved. In 2014, the country had 27 
candidates, with Caid Essebsi the most unique representative of the 
liberal-progressive camp. Nahdha activists and the other part of the 
conservative-revolutionary camp supported Marzouki, though he didn't 
receive open support from Nahdha leaders. The leaders also declined open
support for Caid Essebsi. The other candidates represented various 
political movements with no significant impact on Tunisians.
The period since 2014 has seen many episodes of bickering among these  political actors. The Nidaa Tounes party broke up into many small  parties, and the conservative side had divisions, too. For example,  Hamdi Jabali, head of the Nahdha government in 2012, resigned from the  party and recently presented himself as an independent, therefore one of  the multiple representatives of the revolutionary-conservative camp.  The CPR political party, founded by Marzouki and companions like Mohamed  Abbou, has almost disappeared. Several leaders went on to create their  own political parties.
Candidates
fall into two categories: The progressives as a group reclaim the 
Habiba Bourguiba legacy inspired by the reforms made by Tunisia's first 
president. These reforms focused on modernization of the education 
system and civil code, including a ban on polygamy, making Tunisia the 
only country in the Arab world that has prohibited the practice. This 
group contains former leftist and communist sympathizers of the 1960s 
and 1970s who especially oppose any form of non-secularization of the 
state and dominance of religious power. The group of 
conservatives/revolutionaries is mainly composed of Tunisians who seek a
more significant presence of religion in the public space, mostly 
composed of supporters of the Nahdha party that was severely repressed 
during the Ben Ali era. This group also includes part of the population 
that inspires significant change in the current political system. They 
may have leftist and progressive tendencies, but can find themselves 
aligned with the Islamists for radical reform of the political system.
Still, new leaders have emerged and are better known by Tunisians, 
and low turnout means the most passionate supporters could win the 
presidency. Nabil Karoui is a favorite at the polls and often compared 
to Silvio Berlusconi, the media tycoon and populist in Italy who served 
as prime minister and is now a member of the European Parliament. 
Abdelkerim Zbidi, a former defense minister, finds support among much of
the Tunisian elite class. Youssef Chahed is the current president. Abir
Moussi, a lawyer and fervent activist, has never hidden her hostilities
for the conservative Nahdha party. Another personality, Kaïs Saïed, an 
assistant at a law university and popular in the polls, remains a 
favorite. Mohamed Abbou, representing some the left movement in the 
conservative-revolutionary camp, also counts among the favorites in this
race.
Thus, in the next election, it can be safely predicted that the 39 
percent obtained by Caid Essebsi in the 2014 elections may be divided 
among eight liberal-progressive candidates and the same could happen 
with the revolutionary-conservative camp. None of the five candidates 
mentioned is expected to capture more than 500,000 votes, or more than 
20 percent.
For Tunisian voters, top issues include corruption, terrorism, 
unemployment and security. The second round of voting for Tunisia, 
October 6, should offer a clear choice, promising a sizable turnout. 
Otherwise, the winner's legitimacy to govern Tunisia for the next five 
years could be in question.
*Dhafer  Malouche is currently an associate research scholar at Yale University  in the Department of Statistics and Data Science and for the Whitney and  Betty MacMillan Center for International and Area Studies. He's also a  professor of statistics on leave from the University of Carthage in Tunisia.

Load-Date: September 13, 2019


End of Document


Artificial Intelligence In Military Operations: Where Does India Stand? – Analysis
Eurasia Review
August 5, 2019 Monday


Copyright 2019 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 1620 words
Body


Policymakers must have a sound understanding of the objectives that AI seeks to achieve in the strategic context of India to disseminate artificial intelligence in defence. What kind of AI do we want?
By Ambuj Sahu*
Artificial Intelligence (AI), also dubbed as the Industrial 
Revolution 4.0, has been making giant strides in scientific and 
technological innovation across varying fields. It is capable of 
bringing significant transformations in the way civilian activities and 
military operations are conducted. Till now, the idea of attaining 
military superiority was conceivable only to a few countries like the 
US, China and Russia, who maintain large armed forces. AI, being a 
dual-use technology, may have interesting implications on the distribution of military power
in the future. The possibility of AI-ushered advancements has opened 
the scope of an arms race where the conventional military capabilities 
will matter much less as time progresses. As a result, middle powers
leading in civilian AI-tech also has the field wide open to compete for
hard power. In this light, India is hard-pressed to enter the AI race 
in defence sooner rather than later. In January 2019, Army Chief Gen. 
Bipin Rawat was noted to have said , that India will be too late if the 
armed forces do not embrace AI soon enough.
Although there is a 
broad consensus on what AI is, i.e., carrying out tasks that can be 
performed by humans through a computer or digitally-controlled robots, 
there are diverse opinions about how AI can be achieved. At a time when 
data science is the new norm in the tech industry, it is perceived — in 
popular understanding— that it is inseparable from artificial 
intelligence. On the contrary, machine learning is only one of the tools
that has contributed to the creation of AI technologies in addition to 
natural language processing (NLP), robotics, autonomous locomotives and 
other technology mediums. Hence, it is vital to take up a holistic 
understanding of the AI landscape in India and not limit it to the lack 
of data sciences infrastructure in the country.
This article discusses 
the developments in artificial intelligence in the defence sector and 
analyses the prospects and challenges that might be faced by India in 
the near future. It surveys the institutions and initiatives around 
which the AI policy of India can be expected to revolve around. 
Subsequently, it lists the barriers against the proliferation of AI in 
India's defence and highlights the fundamental questions that 
policymakers should address before embarking on an AI programme.
What has India done so far?
The Ministry of Defence (MoD) constituted a multistakeholder task force for Strategic Implementation of Artificial Intelligence and Defence in February 2018 that submitted its report
in June. The MoD implemented the recommendations by providing an 
institutional framework for policy implementation, issuing guidelines to
the defence organisations, and laying out a vision for 
capacity-building.
In February 2019,
the ministry established a high-level Defence AI Council (DAIC) under 
the chairmanship of Minister of Defence assigned with the task of 
providing strategic direction towards the adoption of AI in defence. The
DAIC will guide the partnership between the government and industry and
also review the recommendations concerning the acquisition of 
technology and startups. It also envisions the formation of a Defence AI
Project Agency (DAIPA) as the central executive body.
The 
ministry ordered to focus on the capacity building within defence 
machinery. The tasks range from the knowledge production in the form of 
data collection, patents etc to acclimatising the personnel on-duty 
through internships, training programmes and sabbaticals. Each Service 
Headquarter (SHQ) will be provided with a window of Rs 100 crores for AI
specific application developments from the ministerial budgetary 
allocation. The task force recognised AI as a 'force multiplier' and 
emphasised that all the defence organisations lay down their strategies 
of AI appropriation.
As discussed earlier, 
robotics is also one of the ways to achieve AI. The Centre of Artificial
Intelligence and Robotics (CAIR) in the Defence Research and 
Development Organisation (DRDO) has also developed autonomous 
technology-based products.
It has focused on the net-centric communication systems for tactical 
command control. For surveillance and reconnaissance purposes, CAIR has 
developed intriguing probes like snake robots, hexa-bots, and sentries. 
It has a comprehensive library for AI-based algorithms and data mining 
toolboxes that can potentially be used for image/video recognition, NLP,
swarming. However, in a data-based approach to artificial intelligence,
efficient learner algorithms can only serve a limited purpose without 
the hardware that can collect and process a large amount of data.
Few questions to be addressed..
The key challenges for the adoption of artificial intelligence, in general, are elucidated in the Niti Aayog's 2018 document
titled 'National Strategy for Artificial Intelligence #AIforAll.' In 
addition to this, there are many challenges that AI brings up in the 
military sector.
Firstly, policymakers must have a sound 
understanding of the objectives that AI seeks to achieve in the 
strategic context of India to disseminate artificial intelligence in 
defence. What kind of AI do we want? Do we require fully autonomous 
drones to engage with the adversary aircraft in a dogfight or deploy 
autonomous patrolling vehicles at the borders for getting the job done? 
How much autonomy should be given to the machines on the battlefield? A 
clear vision of the AI programme is necessary for a middle-income 
country like India that cannot afford to invest heavily in this sector. 
It is a matter of guns vs. bread and butter — there is a trade-off 
between spending in national security and public welfare. Unlike 
technological programmes in developed countries which can afford to fail
first and fail faster, India does not have this luxury due to paucity 
of resources.
Secondly, the lack of critical infrastructure is one of the biggest impediments
in the prospects of AI in India for both civilian and military uses. As
AI runs complex algorithms on loads of data, it is essential to have 
robust hardware and enabling data banks within the country. If a 
critical AI-based military technology harnesses the data on a remote 
server located beyond the borders, it can potentially hinder the goal of
Indian foreign policy from preserving its strategic autonomy in a way 
that it might be compromised.
Thirdly, the role of the private 
sector will be pivotal in making the AI accessible and efficient. As AI 
demands high-skills and capital, innovations need an ecosystem 
supporting the free flow of both money and skill. However, as per the existing FDI policy
in defence, only 49% of the foreign investment is permitted within the 
automatic route above which it needs government approval. India has 
traditionally been conservative in handing over the reins of the defence
industry in the hands of the private sector.
Way forward..
Leading powers like the US, China and the EU (and France)
have their vision documents for research and development programmes in 
artificial intelligence. To begin with, India should envisage a clear 
strategic vision regarding the AI on similar lines. Despite resource 
limitations, India is home to world class academicians in computer 
science and engineering spread across the IITs, IISc, NITs and IISERs. 
An academia-industry-policy synergy is of utmost importance to realise 
the strategic, societal and cultural implications of AI in defence. It 
will help us to find answers to questions raised in the previous 
section.
The government should create a supportive ecosystem in 
which the AI industry in India can thrive. There is a dire need to 
invest in critical infrastructure so that the data servers lie within 
the territory. Apart from ensuring strategic independence, it will also 
address data privacy concerns.
The AI-market for civilian purposes in the country is on the rise. For instance, India ranks third in G20 countries in AI-based startups. Last month, PM Modi unveiled
his "5-I" vision to maximise the societal benefit at the G20 summit on 
digital economy and artificial intelligence. India should realise the 
dual-use nature of artificial intelligence and thus also open its market
to AI investments in defence. The 49% cap on FDI policy in defence 
should be revisited to account for this fact. Policymakers should 
brainstorm on the prospects of marrying the flagships initiatives of the
current government— Make in India in Defence and Digital India — to 
bring a technological revolution in the defence industry.
New 
Delhi, whose major security priorities lie in the subcontinent, cannot 
ignore the progress made by China in the field of artificial 
intelligence. The State Council AI plan, released in June 2017, outlines China's ambitious AI policy which aims to create an AI-industry of worth 150 billion RMB
that is ten-fold its 2017 figures. At the same time, India should avoid
setting their benchmarks taking Chinese investments in mind. Since 
India is a late entrant in the field, it could capitalise on the 
late-movers advantage, i.e., mimicking the existing narrow-AI 
technologies, to fulfil its basic security needs (like border patrols 
and intel-gathering) rather than seeking innovation. The 'AI in defence'
vision should be centered on achieving military modernisation over 
balance of power considerations. Hence, this is the time for New Delhi 
to step up in the arena of what hardliner security experts refer to as 
an AI arms race.
*Ambuj Sahu is research intern in the Strategic Studies Programme at ORF New Delhi.

Load-Date: August 5, 2019


End of Document


Use Of Increasingly Advanced Analytics May Present Crucial Way For P&C Insurers To Unlock Tangible Value
Eurasia Review
August 21, 2019 Wednesday


Copyright 2019 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 821 words
Body


The latestsigmareport from the Swiss Re Institute, "Advanced analytics: unlocking new frontiers in P&C insurance", suggests that technology advances are enabling P&C insurers to unlock new frontiers in risk assessment and mitigation thanks to advanced analytics.
Past successes that focused on improving expense ratios have catalysed new investment with pilots by insurers showing meaningful improvement in loss ratios, as insurers gain better visibility into underlying loss drivers. True potential will only be realised through co-ordinated efforts between developers and users, although expectation of success in all projects could limit adoption and constrain a virtuous circle of trial and improvement.
"Most insurers aim for a success rate of one-third in operationalising pilots. Too high a success rate may mean that the use cases are not challenging enough", says Daniel Knüsli, Swiss Re's Head P&C Analytics, P&C Solutions.
Exploring the opportunities for advanced analytics
Advanced analytics will undoubtedly make an impact along the insurance value chain. Challenges to success remain in the form of legacy systems, traditional mind sets and scarce talent at the intersection of data science, risk knowledge and technology.
Despite this, the Swiss Re Institute expects spending on data and analytics to rise within static IT budgets, as more insurers complete core systems updates over the coming years and seek out differentiating capabilities.
Advanced analytics should be considered from the perspective of business capabilities rather than technologies.
These include how to enable growth by using analytics to achieve an in-depth understanding of new market opportunities and new risk pools; how to better comprehend and influence customers; how to gain insights on risk accumulation and portfolio steering through linking existing portfolios with orthogonal external datasets; and how to improve efficiency by automating manual and repetitive tasks that take up valuable time for underwriters and claims managers.
Advanced analytics pilots across several lines of business do indicate healthy loss ratio improvements (see Figure 1) but for various reasons, results in real-time trading conditions may vary. All told, most insurers seem to be targeting around 2-5% improvement in loss ratios under real trading conditions.
Figure 1
"The time taken to implement P&C Solutions projects depends on the Lines of Business and project objective, but several weeks is the minimum time for rapid deployments. Wider business integration and extracting larger scale efficiencies can take longer, P&C Analytics, part of the P&C Solutions suite, can also integrate other solutions into its projects for added client benefits", says Eric Schuh, Global Head of P&C Solutions at Swiss Re.
A holistic view of advanced analytics activities across P&C
Thesigmastudy brings together insights and experience across key commercial P&C lines.
"The ability to gain useful predictive insights from ever-increasing amounts of data is challenging. There needs to be more investment of time and resources on data curation. Many new data sources are not created for insurance, and owners of the data may neither understand insurance nor what needs to be done to make the data usable for insurers", says Daniel Ryan, Head of Insurance Risk Research at Swiss Re Institute.
In commercial property, insurers are using data to auto-fill underwriting criteria for new business and renewals and moving towards virtual inspection platforms. Data about location and occupancy can be modelled to produce risk scores that enable underwriters to base risk selection and price on market-wide experience (see figure 2).
Figure 2
In marine, insurers can now use detailed behavioural and situational data on over 100,000 vessels to identify risky behaviour and monitor risk concentration, opening the path towards "pushing" the insured to improved preventative measures (See Figure 3).
Figure 3
Making the case for investment in advanced analytics
Successful implementations of analytics projects start with insurers asking the right questions on value propositions and data sources e.g., how to identify areas where analytics generate tangible value; how to build a holistic data strategy; what are the success criteria (e.g., ROI and time horizon)? One useful framework to determine the value of projects is to evaluate them across desirability, feasibility and viability (See Figure 4). Insurers should focus initially on areas where there is high potential on all three fronts.
Figure 4
Daniel Knüsli emphasises how the increasing interest in using advanced analytical tools are influencing SwissRe's business development: "We continue to see demand for P&C Analytics, which is part of Swiss Re's P&C Solutions suite, to provide tangible data-driven business insights that help our clients grow their business, increase their profitability and enhance their efficiency."

Load-Date: August 21, 2019


End of Document


Why Should You Care About AI Used For Hiring?
Eurasia Review
September 8, 2019 Sunday


Copyright 2019 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 396 words
Body


Artificial intelligence has become much more prominent in business 
processes recently, and was voted the number one trend in SIOP's Top 10 
Workplace Trends for 2019. SIOP is currently celebrating Smarter 
Workplace Awareness Month to highlight trends like AI in an effort to 
help organizations grow and thrive in ways that may not be possible 
without the help of I-O psychology.
As AI continues to gain traction, it will be critical for 
organizations to include I-O psychologists on their data science teams 
to leverage expertise in psychological theory and methods in ensuring 
optimal outcomes for organizations.
"Artificial Intelligence in Talent Assessment and Selection" is 
the latest in the Society for Industrial and Organizational Psychology 
(SIOP) white paper series. Written by Neil Morelli, PhD, VP of Product 
and Assessment Science at Berke, this paper provides an overview of 
artificial intelligence in the workplace, provides practical to-dos for 
organizations considering AI tools for their hiring process, and 
explains how I-O psychologists can help along the way.
Media channels often feature stories on the "future of work," 
"the skills gap," "income inequality," and "globalization." What these 
stories have in common is a focus on the work people will do in the 
future and how they will be placed in those jobs.  In other words, how 
people are hired and managed are interests among everyday people. AI is a
driving force behind the workforce changes occurring and is a tool that
can help hire people into jobs.
However, HR is late to the AI game. Anyone interested in the 
future of work, HR, or hiring should read this whitepaper to get up to 
speed on this evolving topic. Artificial intelligence is changing the 
way businesses operate and has the potential to revolutionize the way we
select and retain talent. For businesses to take advantage of new 
technology, they must first understand it. AI is complex topic, but 
Morelli breaks it down in a relatable manner so that HR practitioners 
can take action.
In addition to providing an overview and background on the topic,
Morelli discusses three next steps for consumers to consider before 
using an AI hiring tool. He explains the options of getting I-O 
psychologists involved, pairing AI-based tools with human decision 
makers, and applying a healthy amount of skepticism to marketing 
materials provided by vendors.

Load-Date: September 8, 2019


End of Document


Why Urban Planners Should Pay Attention To Restaurant-Review Sites
Eurasia Review
July 16, 2019 Tuesday


Copyright 2019 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 936 words
Body


Apartment seekers in big cities often use the presence of restaurants
to determine if a neighborhood would be a good place to live. It turns 
out there is a lot to this rule of thumb: MIT urban studies scholars 
have now found that in China, restaurant data can be used to predict key
socioeconomic attributes of neighborhoods.
Indeed, using online restaurant data, the researchers say, they can 
effectively predict a neighborhood's daytime population, nighttime 
population, the number of businesses located in it, and the amount of 
overall spending in the neighborhood.
"The restaurant industry is one of the most decentralized and 
deregulated local consumption industries," says Siqi Zheng, an urban 
studies professor at MIT and co-author of a new paper outlining the 
findings. "It is highly correlated with local socioeconomic attributes, 
like population, wealth, and consumption."
Using restaurant data as a proxy for other economic indicators can 
have a practical purpose for urban planners and policymakers, the 
researchers say. In China, as in many places, a census is only taken 
once a decade, and it may be difficult to analyze the dynamics of a 
city's ever-changing areas on a faster-paced basis. Thus new methods of 
quantifying residential levels and economic activity could help guide 
city officials.
"Even without census data, we can predict a variety of a 
neighborhood's attributes, which is very valuable," adds Zheng, who is 
the Samuel Tak Lee Associate Professor of Real Estate Development and 
Entrepreneurship, and faculty director of the MIT China Future City Lab.
"Today there is a big data divide," says Carlo Ratti, director of 
MIT's Senseable City Lab, and a co-author of the paper. "Data is crucial
to better understanding cities, but in many places we don't have much 
data. At the same time, we have more and more data generated  by apps and websites. If we use this method we [can] understand  socioeconomic data in cities where they don't collect data."
The paper, "Predicting neighborhoods' socioeconomic attributes using restaurant data," appears in the Proceedings of the National Academy of Sciences.
The authors are Zheng, who is the corresponding author; Ratti; and Lei 
Dong, a postdoc co-hosted by the MIT China Future City Lab and the 
Senseable City Lab.
The study takes a close neighborhood-level look at nine cities in 
China: Baoding, Beijing, Chengdu, Hengyang, Kunming, Shenyang, Shenzen, 
Yueyang, and Zhengzhou. To conduct the study, the researchers extracted 
restaurant data from the website Dianping, which they describe as the 
Chinese equivalent of Yelp, the English-language business-review site.
By matching the Dianping data to reliable, existing data for those 
cities - including anonymized and aggregated mobile phone location data
from 56.3 million people, bank card records, company registration 
records, and some census data - the researchers found they could 
predict 95 percent of the variation in daytime population among 
neighborhoods. They also predicted 95 percent of the variation in 
nighttime population, 93 percent of the variation in the number of 
businesses, and 90 percent of the variation in levels of consumer 
consumption.
"We have used new publicly available data and developed new data 
augmentation methods to address these urban issues," says Dong, who adds
that the study's model is a "new contribution to [the use of] both data
science for social good, and big data for urban economics communities."
The researchers note that this is a more accurate proxy for 
estimating neighborhood-level demographic and economic activity than 
other methods previously used. For instance, other researchers have used
satellite imaging to calculate the amount of nightime light in cities, 
and in turn used the quantity of light to estimate neighborhood-level 
activity. While that method fares well for population estimates, the 
restaurant-data method is better overall, and much better at estimating 
business activity and consumer spending.
Zheng says she feels "confident" that the researchers' model could 
be applied to other Chinese cities because it already shows good 
predictive power across cities. But the researchers also believe the 
method they employed - which uses machine learning techniques to zero 
in on significant correlations - could potentially be applied to cities
around the globe.
"These results indicate the restaurant data can capture common 
indicators of socioeconomic outcomes, and these commonalities can be 
transferred ... with reasonable accuracy in cities where survey outcomes
are unobserved," the researchers state in the paper.
As the scholars acknowledge, their study observed correlations 
between restaurant data and neighborhood characteristics, rather than 
specifying the exact causal mechanisms at work. Ratti notes that the 
causal link between restaurants and neighborhood characteristics can run
both ways: Sometimes restaurants can fill demand in already-thriving 
area, while at other times their presence is a harbinger of future 
development.
"There is always [both] a push and a pull" between restaurants and 
neighborhood development, Ratti says. "But we show the socioeconomic 
data is very well-reflected in the restaurant landscape, in the cities 
we look at. The interesting finding is that this seems to be so good as a
proxy."
Zheng says she hopes additional scholars will pick up on the method,
which in principle could be applied to many urban studies topics.
"The restaurant data itself, as well as the variety of neighborhood 
attributes it predicts, can help other researchers study all kinds of 
urban issues, which is very valuable," Zheng says.

Load-Date: July 16, 2019


End of Document


Artificial Intelligence On The Battlefield: Implications For Deterrence And Surprise – Analysis
Eurasia Review
January 3, 2020 Friday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 10552 words
Body


By Zachary Davis*
Artificial intelligence (AI) has burst upon the national security scene with a speed and an intensity surprising even the most veteran observers of the national policy discourse. Factors that have driven this spike of interest include the perception of AI as a revolutionary technology, on par with the discovery of fire, electricity, or nuclear weapons; the rapid absorption of nascent AI-based technologies into diverse sectors of the U.S. economy, often with transformative effects (as, for example, in the sciences and in social media); and the ambitions of potential U.S. adversaries.1 Echoing the 19th-century naval strategist Alfred Thayer Mahan ("Whoever rules the waves rules the world"), Russian president Vladimir Putin has argued that the nation that rules in AI "will be the ruler of the world."2
People's Republic of China President Xi Jinping is less outspoken on this matter, but he has committed China to become the dominant AI power by 2030.3 There are mounting fears of a "Sputnik moment," which might reveal the United States to be woefully underprepared to manage the new AI challenges. If there is an AI arms race, what are the implications for U.S. security?4 Could AI disrupt the strategic balance, as blue-water navies and nuclear weapons did in previous eras? Might it do so in a manner so severe that deterrence fails and leads to war? If war involving AI-guided weapons occurs, can we win?
This article will calibrate the potential risks and rewards of military applications of AI technologies and will explore:
What military applications are likely in the near term?What are the potential consequences of these applications for strategic stability?How could AI alter the fundamental calculus of deterrence?How could AI-assisted military systems affect regional stability? Relatedly, what is the connection between regional stability and strategic deterrence?What are the risks of unintended consequences and strategic surprise from AI?
AI, Big Data, and Machine Learning in Science and Business
Before answering the questions posed above, it is useful to recall the state of the art for AI in scientific and business applications. Much of the near-hysteria over AI stems from the fuzziness of our view of the technologies that combine to make AI. So far, at least, the national security community lacks a common language for discussing AI and a detailed appreciation of the different technologies and the timelines by which they might mature into militarily significant capabilities.
The term "artificial intelligence" is used to describe a range of loosely related phenomena that are generally associated with using computers to glean insight from "big data." Much as the generic term "cyber" is used in reference to everything from networks to hardware, software, automation, industrial controls, hacking, bullying, warfare, and all things social media, AI is used as a generic term that washes over meaningful distinctions between its different manifestations. This breeds confusion, especially regarding claims about its revolutionary effects.
For the vast majority of current applications, AI consists of algorithms that form the basis of pattern recognition software. When combined with high-performance computing power, data scientists are able to probe and find meaning in massive data collections. Neural networks supercharge the ability of the algorithms to identify and organize patterns in the data by "training" them to associate specific patterns with desired outcomes. Multiple layers of neural networks, known as deep learning neural networks, are what make current approaches to "machine learning," "supervised learning," and "reinforcement learning" possible.5However, the neural network approach portrays only a fraction of the advancements in AI methods. For example, AI also includes language processing, knowledge representation, and inferential reasoning, which are all increasingly possible due to advancements in software, hardware, data collection, and data storage. AI represents a quantum leap in the ability to find needles in data haystacks—as long as you know what you are looking for.
It is useful to distinguish between narrow and general applications of AI. Narrow AI encompasses discrete problemsolving tools designed to perform specific narrow tasks. General AI encompasses technologies designed to mimic and recreate functions of the human brain. The gap between the two is significant. Most experts appear to agree that the accomplishments of narrow AI, though quite significant, are a long way from the requirements of replicating human-like reasoning as envisioned by proponents of general AI. Although IBM's Watson, Google's DeepMind, and other such experiments have made breakthroughs in replicating human-like reasoning, they are far from being able to reliably replicate the performance of the human brain in its multiple dimensions. It is not surprising, however, that the human imagination has been captured by the prospect of what futurists have called "The Singularity"—a point in time when "we will multiply our effective intelligence a billion fold by merging with the intelligence we have created."6 The quest for "superintelligence" notwithstanding, recent progress in brain enhancement for now mostly replenishes impaired functions7 and has a long way to go before it is possible to equip citizens, soldiers, or robots with superhuman powers.8
.
Although general AI stimulates intriguing science fiction about cyborgs, space wars, and robot armies, narrow AI is already here—and has been for some time. In both business and science, AI has wide applications, primarily in data-rich research fields, including fundamental research (for example, in physics, chemistry, and biology) and applied sciences (medicine, aeronautics, and environmental studies). Data science is facilitating rapid advancements in every aspect of scientific discovery, even changing long-held methodological standards and practices.9Figure 1 highlights some of the scientific areas where AI-fueled deep learning is having its greatest effect.
The crossover of AI into business applications has supercharged predictive analytics for market research, consumer behavior, logistics, quality control, and many other data-rich areas. The proliferation of cameras and sensors creates even more opportunities for data analysis. When combined with robotics, AI is ushering in a new industrial age, with far-reaching societal implications for labor and management.10For these types of applications, however, AI is more of a well-established, sustaining, and enabling technology than a revolutionary new disruptive technology in its own right. Data analytics is not new, but it is getting better.
For these scientific and business applications, AI is an enabling technology, a cross-cutting force multiplier when coupled with existing data-centric systems, such as the internet, health care, social media, industrial processes, transportation, and just about every aspect of the global economy, where recognizing patterns is the key to insight and profit. Growing interconnectivity, illustrated by the Internet of Things (IOT), is producing more data and providing more opportunity for AI algorithms to reveal hidden insights.
What Military Applications are Likely in the Near Term? Tactical and Strategic Effects
Should we expect similarly important AI applications in the military field? Like so many technologies, AI is loaded with latent military potential.11Many see algorithmic warfare as the prime mover of a new revolution in military affairs.12AI was central to the so-called Third Offset Strategy pursued by the Department of Defense (DOD) in the second Obama Administration and thus was a principal focus of multiple government initiatives to accelerate the development of advanced technologies.13In June 2018, DOD established its Joint Artificial Intelligence Center and issued its Artificial Intelligence Strategy in February 2019.14The White House established its Select Committee on AI in May 2018 and released its Executive Order on Maintaining American Leadership in Artificial Intelligence in parallel with the DOD Strategy, also in February 2019.15DOD and Intelligence Community spending on AI has increased substantially.16For military applications with direct analogs in the civilian world, like logistics, planning, and transportation, AI-supported data analytics is already in use throughout the defense and intelligence communities.17These applications are separate and distinct from applications to warfighting, which tend to fall into one of two categories: ones having impact primarily at the tactical/operational level of war, and those that also have impact at the strategic level of war. Tactical or operational effects stem from the way wars are fought—including specific weapons and organizational concepts. We define "strategic" as "extraordinarily consequential actions capable of causing a shift in the balance of power."18The strategic level refers primarily to major conflict between great powers. It is possible, however, for actions at the operational level to spill over and have effects at the strategic level.
AI Applications at the Tactical/Operational Level of War
The process of managing and making sense of the staggering amount of intelligence, surveillance, and reconnaissance (ISR) data involved in modern warfare is a natural fit for AI and is the objective of DOD's Project Maven, also known as the Algorithmic Warfare Cross Functional Team.19According to Lieutenant General Jack Shanahan, former Director of Defense Intelligence for Warfighter Support, Project Maven was conceived as "the spark that kindles the flame front for artificial intelligence across the rest of the department."20While Maven's initial mission was to help locate Islamic State fighters, its implications are vast. Multidomain warfare involves colossal amounts of heterogenous data streams that can only be exploited with the help of AI. Mirroring the proliferation of sensors in the civilian world, the multidomain, hybrid warfare battlefield has become a military version of the IoT, teeming with vital information for assessing tactical and strategic threats and opportunities. While the ability to manage this data colossus in real time portends tremendous advantages, failure to draw meaning from that information could spell disaster.
Being able to rapidly process the flood of information from varied platforms operating in multiple domains translates into two fundamental military advantages—speed and range. Moving faster than your adversary enhances offensive mobility and makes you harder to hit. Striking from farther away similarly benefits the element of surprise and minimizes exposure to enemy fire. These were central tenets of the previous revolution in military affairs that had its debut in the Gulf War. AI makes it possible to analyze dynamic battlefield conditions in real time and strike quickly and optimally while minimizing risks to one's own forces.
Omnipresent and Omniscient Autonomous Vehicles
The new generation of autonomous vehicles is a high priority for military applications of AI, with much of the focus on navigation for a variety of unmanned land, sea, and air systems.21Space and undersea platforms will also benefit from AI-informed guidance systems. AI is at the heart of the so-called drone swarms that have been the subject of much attention in recent years.22AI-informed navigation software supported by ubiquitous sensors not only enables unmanned vehicles to find their way through hostile terrain, but also may eventually make it possible for complex formations of various types of drones operating in multiple domains, with complementary armaments to conduct sophisticated battle tactics, instantly adjusting to enemy maneuvers to exploit battlefield opportunities and report changing conditions. Autonomous vehicles and robotics are poised to revolutionize warfare.
As a recent Defense Science Board Study demonstrated, integrated battle management, command, control, communications, and intelligence (BMC3I) capabilities are well suited to finding and targeting deployed missile batteries, and thus could be the key to countering critical elements of the anti-access/area denial (A2AD) strategies of Russia and China.23These systems were designed to exploit vulnerabilities of U.S. land and sea assets in Europe and Asia. In addition to geolocating targets, AI-enabled BMC3I could help guide and coordinate kinetic effects involving multiple platforms, possibly providing a counter to current adversary A2AD. From this perspective, the cumulative effects of tactical-level AI could become a strategic-level game changer.
Big Data–Driven Modeling, Simulation, and Wargaming
AI has steadily been increasing the power of simulations and gaming tools used to study nuclear and conventional weapons. From Samuel Glasstone's early calculations of nuclear effects to the extensive library of RAND studies on nuclear issues, quantitative methods have been integral to the development of nuclear weapons systems.
AI is enabling scientists to model nuclear effects to confirm the reliability of the nuclear stockpile without nuclear testing. Simulation and modeling is already a key part of the design process for nearly all major weapons systems, from jets and ships to spacecraft and precision-guided munitions.24Massive modeling and simulation will be necessary to design the all-encompassing multidomain system of systems envisioned for battle management and complex missions such as designing, planning, and managing systems for space situational awareness. On the production side, AI already informs quality control for novel production methods, such as additive manufacturing.25
AI is also enriching battlefield simulations and wargames involving multi-actor interactions. AI enables wargamers to add and modify game variables to explore how dynamic conditions (weapons, effects, allies, intervention, and so forth) could affect outcomes and decisionmaking. AI is used to analyze the results of such games.26These are examples of evolutionary learning that are unlikely to cause strategic surprise or undermine stability unless the results negatively influence decisionmaking.
Focused Intelligence Collection and Analysis
With so many incoming streams of intelligence (human, signals, open-source, measurement and signatures, geospatial, electronic) being collected, all requiring analysis to be useful for policymakers, the Intelligence Community faces the challenge of information overload.27This is a data-centric problem for which AI and machine learning are well suited.28For example, a project at Lawrence Livermore National Laboratory uses neural networks to probe multimodal data sets (images, text, and video) in search of key indicators of proliferation activity. Machine learning also makes it possible to combine open-source trade and financial data with multiple forms of intelligence to glean insights about illicit technology transfers, proliferation networks, and the efforts of proliferators to evade detection.29These insights enable analysts to inform policymakers and support counterproliferation policy and actions.
Machine learning will be an important tool for all-source analysts who are increasingly required to take into account information from many sources, locations, and disciplines to understand today's global security environment. To the extent that better information leads to informed decisions, applying AI to these collection and analysis problems would benefit strategic stability.
AI Applications with Implications for the Strategic Level of War
Some military applications of AI appear to have broader implications beyond the battlefield. AI that makes it possible to locate and target strategic assets could alter the logic of strategic deterrence.
A System of Systems Enabling Exquisite ISR
For the military, object identification is a natural starting point for AI, as it requires culling images and information collected from satellites and drones to find things of military importance such as missiles, troops, and intelligence information. Accordingly, the National Geospatial-Intelligence Agency has led the charge in applying AI to military and intelligence needs.30But object identification is just the beginning. Intelligence, surveillance, and reconnaissance (ISR) is the key to multidomain situational awareness. This awareness is increasingly critical as the battlefield extends to all domains—sea, land, air, space, and cyber on a global scale.
Precision Targeting of Strategic Assets
AI-empowered ISR that makes it possible to locate, track, and target a variety of enemy weapons systems raises the possibility of striking strategic assets, such as aircraft carriers, mobile missiles, or nuclear weapons. This capability, and perceptions of its existence, could disrupt long-held assumptions about deterrence stability, especially if it appeared possible to conduct a disarming counterforce strike against an adversary's retaliatory forces.31 The combination of offensive weapons that can "find, fix, and finish" a significant portion of an adversary's strategic assets, with defensive systems that can shoot down remaining retaliatory capabilities, could challenge fundamental precepts of deterrence based on mutual vulnerability.32
Effective Missile Defense
Advancements in AI-enhanced targeting and navigation also improve prospects for a wide range of tactical and strategic defense systems, especially ballistic missile defenses, by empowering target acquisition, tracking, and discrimination.33The convergence of powerful new offensive and defensive capabilities has, however, rekindled fears of a surprise attack that could rattle strategic stability.
AI-Guided Cyber
As an inherently digital domain, the cyber realm naturally lends itself to AI applications, as illustrated by the centrality of AI algorithms for social media titans such as Google and Facebook. The availability of enormous amounts of data in electronic formats is well suited to AI strengths. AI-guided probing, mapping, and hacking of computer networks can provide useful data for machine learning, including discovery of network vulnerabilities, identities, profiles, relationships, and other information that could be valuable for offensive and defense purposes.34Chinese applications of AI for surveillance purposes illustrate broad concerns about its implications for privacy and democracy.
On the offensive side, AI could help locate and target particular nodes or individual accounts for collection, disruption, or disinformation. Cyber attacks on national command infrastructure and networks, for example, could be catastrophic.35On the defensive side of the equation, AI can help detect such intrusions and search for debilitating anomalies in civilian and military operating systems.36AI will equally empower offensive and defensive measures, both of which could have positive and negative strategic effects.
Potential Consequences of these Applications for Strategic Stability
AI has multiple potential applications in the military domain at both the operational and strategic levels of war. But at the strategic level, some of the implications may not be altogether positive, as already foreshadowed. Indeed, the disruptive effects of new technologies cannot be limited to the adversary. Some of those effects are potentially quite significant for strategic stability. How might this be so?
The Enemy Has AI Too
No one country can gain all of the benefits of AI while denying them to potential adversaries. Competition to gain advantage will bring uncertainty about the future balance. Russia, China, and other nations' advancements in these same AI-enabled technologies have the potential to shift the strategic calculus as well, especially in regional contexts. For example, while Russian and Chinese A2AD systems designed to defeat U.S. regional forces may reduce U.S. allies' confidence in American security guarantees to protect them, the ability of the United States to defeat those A2AD systems with AI-accelerated ISR, BMC3I, defensive systems, and autonomous vehicles would demonstrate resolve and provide opportunities for joint U.S.-allied defense cooperation, thereby enhancing stability and deterrence. Reinforcing regional conventional deterrence is also an essential part of strategic stability.37However, even theperceptionof an imbalance that favors striking first can lead to misperception, miscalculation, and arms racing. Whatever advantages can be attained with AI are likely to evoke countermeasures that mitigate temporary unilateral advantages. Russian and Chinese interest in hypersonic vehicles and counterspace operations may fall into this category.
Data Is Fragile...
AI systems are vulnerable to flawed data inputs, which can cause unintended consequences. In her bookWeapons of Math Destruction, data scientist Cathy O'Neil demonstrates how AI logarithms distort reality and lead to incorrect, misleading, and unjust decisions.38Perhaps the biggest obstacle to increasing reliance on AI is the age-old problem of data reliability. AI can magnify the "garbage in, garbage out" problem.39Data comes from many places and is not always carefully collected or curated. Compounding the problems with the data itself leading to skewed results, AI often reflects human bias.40Computer vision—the AI-informed object and pattern recognition software behind Project Maven and many other applications—is easily fooled by misleading data.41Differentiating between similar objects is difficult and more challenging with denial and deception campaigns, such as the use of camouflage and decoys.42Even when data seems accurate, AI sometimes "hallucinates" things that do not exist.43Transferring these inherent problems of data reliability and interpretation onto the battlefield raises critical questions about the safety and reliability that come with the desirable qualities of speed and lethality. Accidentally hitting the wrong targets, for example, could have strategic consequences.
...And Easily Manipulated
Countering many AI applications can be simple and straightforward. Adversarial manipulation of data provides many opportunities for mischief and mistakes.44The fact that AI is easily deceived invites efforts to counter the sought-after military benefits.45By corrupting data in calculated ways, it may be possible to cause catastrophic equipment failures, miscommunication, confusion, logistical nightmares, and devastating mistakes in AI-reliant systems. The "black box" problem of not understanding how and why AI makes decisions also means that it would be hard to recognize if data had been compromised to produce inaccurate outcomes, such as hitting the wrong targets or misdirecting U.S. and allied forces. The vulnerability of data could be the Achilles' heel of AI.
Faster is Not Always Better
Speedy decisionmaking and operational execution may not serve well the goals of effective crisis management. On October 19, 1962, only three days into the Cuban Missile Crisis, General Curtis LeMay counseled President John F. Kennedy, "I just don't see any other solution except direct military action right now."46Ten days later, the crisis was resolved diplomatically. If one of the advantages of AI is the speed it adds to decisionmaking, that same speed could be a disadvantage if it accelerates the escalation of conflict from crisis to war and even potential nuclear confrontation.47The battlefield advantages of AI-driven ISR and autonomous systems could shrink the time available for diplomacy to avoid or manage crises. As currently conceived, AI-driven battlefield systems would not include real-time reporting and analysis of national and international diplomatic efforts to avoid, control, contain, or end a conflict—violating Clausewitz's principle of war as "the continuation of politics by other means." In many cases, logic might dictate striking first, as General LeMay advised. Accelerated decisionmaking might have pushed the Cuban Missile Crisis toward different outcomes. In practice, slowing things down can be the key to victory, especially when the stakes involve nuclear weapons.
Many of the positive regional deterrence effects that could eventually result from an integrated ISR, defense, and battle management complex might not be attainable, at least not in the near term. The overarching architecture and strategy for complex new AI-guided ISR/battle management systems do not yet exist. In fact, a proliferation of AI systems may actually complicate one of the main problems confronting U.S. military forces—effective joint operations.
Systems of Systems of Systems
AI-supported weapons, platforms, and operating systems operate according to custom-built software and hardware that is specifically designed for each separate system and purpose. There is currently no overarching mechanism to integrate scores of AI-powered systems operating on multiple platforms.48To achieve the desired effects of multidomain ISR, it is necessary to integrate across scores of sensors, radars, weapons, and communications systems operating in multiple geophysical domains. If this were not challenging enough, those systems would be built and operated by different agencies, commands, and contractors, with different authorities, access, and procedures. Adding allies with their own AI systems to this landscape brings further complexity and risk. Without seamless integration, the hoped-for benefits of speed and lethality could be fleeting, and the credibility of such an unproven system of systems could be called into question. Massively complex and unproven capabilities could invite challenges that could be destabilizing.
Strategic Warning Requires More than Data
Big data and machine learning might not solve the challenge of strategic warning. Designing a multiplex of AI-informed platforms that have the ability to communicate in real time requires a new generation of data fusion, integrative software, and command architectures. Pulling all these pieces together to develop a holistic threat assessment that provides policymakers with strategic warning will not happen naturally. Instead, this task will require Herculean efforts to collect and analyze information "owned" by diverse stakeholders with different classification systems, analytic roles, and customer loyalties. Integrating and analyzing sensitive information from diverse sources is already a challenge, especially if it needs to be done quickly. Moreover, while machine learning, computer vision, and other techniques will help sort and prioritize the flood of intelligence information, analysts will still have to make judgments based on incomplete and sometimes unreliable information. Developing a fully integrated system capable of providing strategic warning will take many years.
AI Unpredictability
The close operation and integration of multiple AI systems, as required on the battlefield, can be expected to have unanticipated results, some of which could have strategic consequences. The flip side of stovepiped systems not talking to each other is the issue of unexpected convergences. It is uncertain how separate AI-infused platforms might interact with one another, as various AI-guided systems operate in shared battlespace. Unknown outcomes resulting from friendly interactions are likely to be compounded by interactions with foreign AI systems. With so much uncertainty about the internal "black box" mechanisms that produce AI outcomes, AI-to-AI interactions are likely to produce unanticipated and unexplainable results— for example, choosing the wrong targets.49Lastly, we cannot anticipate how AI will converge with other technologies, such as quantum computing, electromagnetic pulses, Internet of Things, 5G, or blockchain/distributed ledgers. Potential convergences could produce strategic surprises that confuse and confound friends and foes alike, making the fog of war even more impenetrable and increasing the risks of escalation.
Who Is In the AI Loop?
Whether or not there are humans in every part of the decisionmaking loop, that loop is getting crowded. The interface between humans and machines—where the proverbial "person in the loop" is supposed to exert human control— also raises critical questions about decisionmaking authority and organizational hierarchies.50Within the military, questions of rank, service branch, and responsibility for lethal actions can be contentious in the best of times, as illustrated by the debates over authority for U.S. drone strikes.51Deconflicting military and intelligence missions will not be made easier. With scores of AI-informed battlefield systems operating at breakneck speed, each connected to its own chain of command, coordination among the humans who are in the loop of fast-moving battlefield operations spanning multiple adversaries, domains, agencies, clearance levels, contractors, allies, and organizational cultures will be challenging, especially if the goal is to maintain offensive advantage via speedy decisionmaking. Budgets, reorganizations, accesses, personalities, and leadership changes may have as much influence over AI capabilities as the technology itself. There will be lots of men and women in the loop in lots of places, each influencing how AI contributes to separate and shared objectives. Achieving strategic effects will require extraordinary cooperation and communication.
Fake Nuclear News
Public perception is a giant wildcard. AI algorithms are a central component of cyber influence operations aimed at shaping public perceptions. By now, it should be understood that the use and misuse of electronic media to manipulate public perceptions, including the use of fake news, cyber bots, and deep fakes, can affect strategic stability.52How the public views particular international conflicts can shape leadership decisionmaking and can build or undermine support for issues of war and peace, especially in democratic states. Decisions to escalate conflict could be influenced by public attitudes. AI-powered tools such as cyber bots and deep fake technology could enrage or pacify public opinion or mislead decisionmakers. Now that cyber conflict has become an ingrained feature of the international landscape, we should expect manipulation of public perceptions to affect crisis management, escalation, deterrence stability, and possibly nuclear decisionmaking.
Close Is Not Good Enough
Decisions of war and peace cannot be left to predictive analytics. There are fundamental differences in the ways that data is used for scientific, economic, and logistic purposes and for predicting human behavior. Machine learning cannot reliably predict the outcomes of sports contests, elections, or international conflict, at least within acceptable margins of error for making big decisions involving questions of war and peace. Despite longstanding interest in predictive analytics that can tell decisionmakers what to expect before it happens, faith in the ability to predict incidents or outcomes of war and conflict based on big data machine learning is fraught with misplaced optimism.53Much like self-driving cars, where AI can correctly assess most—but not all—situations, a 90 percent success rate could mislead decisionmakers and put soldiers' and citizens' lives at stake. All of the potential dangers stemming from unreliable (outdated, biased, compromised) data, machine learning bias, and interpretation errors are magnified when human emotions, nonrational behavior, and inherent unpredictability cloud the dataandthe decisionmaking. The result is wider margins of error, which may be acceptable for research purposes but do not satisfy the practical and ethical demands of national security decisionmaking. Close is not good enough when it comes to war, especially where nuclear risks are involved.
Crowdsourcing Armageddon?
Lastly, public–private partnerships shape the future of AI—but war remains the preserve of the state. As a quintessentially dual-use technology, AI is freely available to everyone. It is being developed and applied beyond the reach of governmental controls. Like many other dual-use technologies, governments rely on the private sector for the underlying research and development, software, hardware, and expertise required for AI to be used for military purposes. DOD and the Intelligence Community have deep ties to Silicon Valley and have doubled down on efforts to expedite the acquisitions process, especially for cyber and AI.54Competition among nations to secure AI talent could have strategic implications, especially with respect to counterintelligence, intellectual property, and respect for international norms of behavior.
America's Got Talent
What this means in practice is that many countries will use the same experts, companies, and global supply chains to support their military AI aspirations, creating potential competitive conflicts of interest and security vulnerabilities related to sharing intellectual property. This dynamic is already evident in cyber markets, where Google and other companies have found it advantageous to accommodate Chinese government practices on censorship and surveillance while simultaneously expressing political opposition to supporting U.S. military AI projects such as Project Maven.55Global technology companies will have to weigh the costs and benefits of serving some national customers while keeping others at arm's length. The U.S. Government, however, has little choice but to remain heavily dependent on the private sector to develop and implement AI strategies.56Such dependence could have strategic implications if it interferes with our ability to compete for top talent and cutting-edge capabilities.
How Could AI Alter the Fundamental Calculus of Deterrence?
In the classic Cold War movieWarGames, a young hacker breaks into a DOD supercomputer designed to use AI to plan and execute nuclear war plans. He engages the computer to play "Global Thermonuclear War" and accidentally triggers a simulated scenario of nuclear Armageddon, which is mistaken for the real thing. The computer ultimately learns that for nuclear deterrence, "the only way to win is not to play." If AI disrupts the central logic of nuclear deterrence as understood by the nuclear powers or fundamentally changes the underlying precepts that support it, the strategic consequences could be far-reaching, and the prospects that computers will learn "not to play" uncertain.
With these potential strategic impacts in mind, how could AI alter the fundamental calculus of deterrence? How might the convergence of the tactically and strategically relevant factors discussed above affect the strategic balance?
AI Is Changing Perceptions About the Threat of Surprise Attack
At the top of the list of AI applications that could have true strategic significance for deterrence strategy is the threat of surprise attack. The combination of effective defenses with exquisite ISR that makes it possible to locate mobile targets and strike them with speed and precision raises long-held fears of an AI-guided "bolt from the blue" first strike. While the fundamental logic of deterrence is unchanged, perceptions that an adversary has sufficient intent and capability to conduct such a preemptive attack on vital assets can be expected to motivate a variety of countermeasures.
Evaluating the incentive to strike first evokes memories of Pearl Harbor, in which the United States underestimated Japan's risk calculus while fully recognizing Tokyo's military capacity to launch a cross-Pacific raid. AI contributions to military and intelligence capabilities do not override political considerations—with an important caveat added for the possibility of AI-fueled manipulation of public attitudes that could distort political judgment. Avoiding and deterring conflict remain a paramount responsibility for national leaders. Slightly improved odds of eliminating all but a few of an adversary's strategic weapons and shooting down any surviving retaliation with missile defenses still involves catastrophic risks and does not even begin to answer questions about the aftermath of such a conflict.
Nevertheless, possessing the theoretical capability to conduct a disarming first strike inevitably triggers a classic security dilemma, which is guaranteed to provoke countermeasures from those threatened by enhanced striking power. Further advances in defenses against counterforce strikes would be a predictable response, as well as hardening and camouflage to evade and confuse exquisite ISR. To the extent that AI influences perceptions of intent and capability and alters the calculus of risk and reward, it will inspire new thinking about possible offensive and defensive maneuvers in the evolution of nuclear strategy.57
Farewell to Mutual Vulnerability?
Some may see AI as eroding mutual strategic vulnerability and thereby as increasing the risk of war. The combination of exquisite ISR with an effective defensive shield could make it tempting to conduct a disarming, decapitating, or blinding first strike at strategic targets, including nuclear command, control, and communications (NC3), early warning radars, or dual-capable missiles and aircraft.58Such a revision of deterrence logic could be highly destabilizing. Shared vulnerability and assured retaliation are central concepts of mutually assured destruction (MAD) deterrence theory. Switching the theoretical incentive from MAD to improve the odds of successfully conducting a disarming first strike could change the risk calculus that has formed the basis of strategic stability for decades.59Preventing such a revision of nuclear deterrence logic was the essence of Russia President Vladimir Putin's claim in March 2018 that his new weapons are "invincible against all existing and prospective missile defense and counter-air defense systems."60By evadingperceivedU.S. global strike and missile defense capabilities, Putin's claims about new AI-guided retaliatory forces were justified as efforts to preserve MAD.
AI Is Poised to Alter Regional Stability in Asia and Europe
How could AI-assisted weapons systems affect regional stability, including U.S. allies? Widespread deployment of AI-supported ISR platforms is likely to affect regional stability in the five- to ten-year time frame. While the United States remains the leader in translating AI to currently deployed platforms, China and Russia are not far behind.61Many U.S. allies are rapidly advancing their own AI capabilities. Initially, the speed and lethality gained from AI-informed situational awareness and battle management systems are likely to provide the United States and its allies with options for countering Russian and Chinese A2AD.
The coming architecture of ISR, BMC3I, and defense systems appears well positioned to give net advantages to U.S. and allied regional security alliances. In addition to tactical military benefits, co-development of multidomain ISR provides opportunities for collaboration that directly addresses threats to allied security, especially with respect to extended deterrence relationships with key allies in Asia and Europe. Strengthening regional conventional deterrence and regional extended nuclear deterrence reduces incentives for risk taking and supports broader interests in strategic deterrence. AI applications that support these objectives will have beneficial effects for strategic stability.
AI Competition Could Also Benefit Strategic Stability and Bolster Deterrence
Global competition in military AI is already heating up. An AI arms race is under way. Whatever advantages are possible in the near term, however, may be short-lived as U.S. allies, major adversaries, and a multitude of rising powers incorporate AI into their political and military strategies. In light of the rising tide that is advancing AI prospects around the world, temporary advantages are unlikely to yield lasting military predominance. For example, China and Russia will eventually possess their own versions of multidomain ISR coupled with precision strike and layered defenses. How will these capabilities influence Beijing's thinking about the U.S. role in the South China Sea, or Russian assessments of the North Atlantic Treaty Organization's defense of the Baltics?
These are not primarily technical issues. AI is enhancing the performance of many tactical and strategic systems but is not giving definitive unilateral advantage to any one. The nature of warfare is changing, and AI is fueling many of those changes, but the fundamental calculus of deterrence remains steady. Competition for military capabilities that retains a balance of power can be stabilizing.
Risks of Unintended Consequences and Strategic Surprise
Predicting the future of technology is a risky business. We know with certainty that AI is being incorporated into an array of military missions with the intent of improving our knowledge of the operational environment, adversary capabilities, and the speed and precision of offensive and defensive weapons. We can usefully speculate about how these developments are poised to change the face of modern warfare and how those changes might affect regional and strategic deterrence stability, based on our understanding of established political and military realities. More elusive, however, is a clear picture of how AI might converge with other technologies to produce unexpected outcomes, or "unknown unknowns." Nevertheless, here are a few possibilities that could have major strategic consequences and alter the underlying realities on which regional and strategic stability are founded:
Distorted data could lead AI systems to take unintended actions, such as incorrectly identifying and striking the wrong targets. Data can be polluted intentionally via counter-AI methods or can occur naturally for many reasons. Such actions could hasten escalation and interfere with conflict management efforts.Compounding the problems of distorted data, AI makes mistakes with a frequency that could be untenable for decisions affecting strategic stability. Misinterpretations of data that lead to unintended actions could spark highly undesirable reactions, including escalation and retaliation.The convergence of AI and cyber presents several possibilities for unintended consequences and strategic surprise. AI-informed cyber attacks on NC3 could present the target of such an attack with a "use it or lose it" situation, prompting early resort to nuclear weapons.AI-supported cyber/information warfare, including use of fake news, deep fakes, and other methods could distort public and leadership perceptions of international events, inflaming passions and prompting escalation.Accelerated battle rhythm made possible by multidomain ISR could preclude diplomatic efforts to avoid or deescalate conflict. Even if AI works perfectly to increase the speed and lethality of warfare, moving at the speed of AI might not be optimal for all cases.Unpredictable AI interactions with foreign and friendly platforms could produce unwanted AI calculations that misrepresent human intentions. The "black box" underlying AI decisions is not well understood and could produce destabilizing results, such as striking the wrong targets.Unexpected convergences with other technologies, such as quantum computing and electromagnetic pulse, could confuse/distort offensive or defensive instructions and lead to undesirable results, such as striking the wrong targets.If it were eventually possible through a variety of AI-supported information gathering methods, emerging technologies, and analytic tools to track strategic assets such as submarines, the sanctity of assured retaliation could come into question. Such a strategic surprise could prompt a variety of destabilizing actions, including possible movement toward launch on warning postures.
AI Is Part of a Bigger Challenge for Deterrence, Stability, and Strategy
Evolutionary changes in the logic of regional and strategic deterrence are not new, nor are they necessarily harmful to U.S. national security. Efforts to integrate AI-based technologies into U.S. defense and intelligence strategies illustrate the continued innovation and competitive advantages sought in support of U.S. national security policy. Applications of AI that support U.S. nuclear forces and infrastructure, such as command and control, logistics, and stockpile stewardship, serve to reinforce strategic deterrence by bolstering the survivability and credibility of our retaliatory forces.
AI that bolsters tactical/battlefield applications can also support strategic deterrence, especially in a regional context. The connection between regional and strategic deterrence has always been important and appears to be even more tightly coupled as increased speed, precision, and lethality at the tactical level hold the potential to produce military outcomes that could escalate to the strategic level of conflict. Specifically, failure to deter Chinese or Russian aggression against U.S. regional allies that results in armed conflict may be hard to contain, especially if early victories on the battlefield leave one side facing a humiliating defeat. The United States and its allies still maintain conventional superiority, and AI is likely to extend those advantages in the near term to defeat Russian and Chinese A2AD. Rather than accept defeat, Russia or China might choose an "escalate to de-escalate" strategy that includes use of nuclear or other unconventional weapons to mitigate the technological advantages held by the United States and its allies, including AI-supported ISR, battle management, and defenses. For the military applications of AI to advance U.S. national security objectives, they must be integrated into a broader strategy that reinforces deterrence at the regional and strategic levels.
The rapid expansion of AI's military applications throughout the world merits a high level of focused attention to ensure maximum advantage for the United States and its allies, to minimize its negative impacts on strategic stability, and to prevent strategic surprise.
*About the author: Dr. Zachary Davis is a Senior Fellow at the Center for Global Security Research at Lawrence Livermore National Laboratory and Research Professor at the Naval Postgraduate School. This article draws on a report published by the Technology for Global Security and the Center for Global Security Research on February 13, 2019 on "AI and the Military: Forever Altering Strategic Stability." The report was a collaboration of the Center for Global Security Research and the Lawrence Livermore National Laboratory.
Source: This article was published in PRISM Vol. 8, No. 3, which is published by the National Defense University.
Notes
1Anthony Cuthbertson, "What's Bigger than Fire and Electricity? Artificial Intelligence, Says Google Boss,"Newsweek, January 22, 2018, <https://www.newsweek.com/artificial-intelligence-more-profound-electricity-or-fire-says-google-boss-786531>; "Elon Musk: Mark My Words—AI Is Far More Dangerous than Nukes," CNBC, March 13, 2018, <           https://www.cnbc.com/2018/03/13/elon-musk-at-sxsw-a-i-is-more-dangerous-than-nuclear-weapons.html>; Peter Holley, "Stephen Hawking Just Got an Artificial Intelligence Upgrade, but Still Thinks AI Could Bring an End to Mankind,"Washington Post, December 2, 2014.
2"Whoever Leads in AI Will Rule the World: Putin to Russian Children on Knowledge Day," RT News, September 1, 2017, <https://www.rt.com/news/401731-ai-rule-world-putin/.>
3Paul Mozur, "Beijing Wants A.I. to Be Made in China by 2030,"New York Times, July 20, 2017, <https://www.nytimes.com/2017/07/20/business/china-artificial-intelligence.html>.
4Patrick Tucker et al.,The Race for AI: The Return of Great Power Competition Is Spurring the Quest to Develop Artificial Intelligence for Military Purposes(Defense One e-book, March 2018), <https://www.defenseone.com/assets/race-ai/portal/>.
5Jürgen Schmidhuber, "Deep Learning in Neural Networks: An Overview,"Neural Networks61 (January 2015): 85–117, <https://doi.org/10.1016/j.neunet.2014.09.003>.
6Dom Galeon and Christianna Reedy, "Kurzweil Claims that the Singularity Will Happen by 2045,"Futurism, October 5, 2017, <https://futurism.com/kurzweil-claims-that-the-singularity-will-happen-by-2045/>; "Artificial Intelligence and Life in 2030, One Hundred Year Study on Artificial Intelligence," report of the 2015 Study Panel, Stanford University, September 2016, <           https://ai100.stanford.edu/sites/default/files/ai_100_report_0901fnlc_single.pdf>.
7Sara Reardon, "AI-controlled Brain Implants for Mood Disorders Tested in People,"Nature, November 22, 2017; Antonio Regalado, "Reversing Paralysis: Scientists Are Making Remarkable Progress at Using Brain Implants to Restore the Freedom of Movement that Spinal Cord Injuries Take Away,"MIT Technology Review, <https://www.technologyreview.com/s/603492/10-breakthrough-technologies-2017-reversing-paralysis/>.
8Sara Reardon, "The Pentagon's Gamble on brain Implants, Bionic Limbs, and Combat Exoskeletons,"Nature, June 10, 2015; Annie Jacobsen, "Engineering Humans for War,"The Atlantic, September 23, 2015, <https://www.theatlantic.com/international/archive/2015/09/military-technology-pentagon-robots/406786/>; Michael Joseph Gross, "The Pentagon's Push to Program Soldiers' Brains,"The Atlantic, November 2018, <           https://www.theatlantic.com/magazine/archive/2018/11/the-pentagon-wants-to-weaponize-the-brain-what-could-go-wrong/570841/>.
9David Weinberger, "Our Machines Now Have Knowledge We'll Never Understand,"Wired, April 18, 2017, <https://backchannel.com/our-machines-now-have-knowledge-well-never-understand-857a479dcc0e>.
10Darrell West,The Future of Work: Robots, AI, and Automation(Washington, DC: Brookings Institution Press, 2018); Molly Kinder, "Learning to Work with Robots: AI Will Change Everything. Workers Must Adapt—or Else,"Foreign Policy, July 11, 2018, <https://foreignpolicy.com/2018/07/11/learning-to-work-with-robots-automation-ai-labor/>.
11Zachary Davis and Michael Nacht, eds.,Strategic Latency: Red, White and Blue, Managing the National and International Security Consequences of Disruptive Technologies(Berkeley, CA: Lawrence Livermore National Laboratory, 2018).
12Frank G. Hoffman, "Will War's Nature Change in the Seventh Military Revolution? Exploring War's Character and Nature,"Parameters47, no. 4 (Winter 2017–18).
13"Deputy Secretary: Third Offset Strategy Bolsters America's Military Deterrence," Department of Defense, October 31, 2016, <https://dod.defense.gov/News/Article/Article/991434/deputy-secretary-third-offset-strategy-bolsters-americas-military-deterrence/>.
14"Summary of the 2018 Department of Defense Artificial Intelligence Strategy," February 2018, <https://media.defense.gov/2019/Feb/12/2002088963/-1/-1/1/SUMMARY-OF-DOD-AI-STRATEGY.PDF>; Memorandum from the Deputy Secretary of Defense, "Establishment of the Joint Artificial Intelligence Center," June 27, 2018.
15The White House, "Summary of the 2018 White House Summit on Artificial Intelligence for American Industry," May 10, 2018; The White House, Executive Order on Maintaining American Leadership in Artificial Intelligence, February 11, 2019, <https://www.whitehouse.gov/presidential-actions/executive-order-maintaining-american-leadership-artificial-intelligence/>.
16DARPA, "DARPA Announces $2 Billion Campaign to Develop Next Wave of AI Technologies," September 7, 2018, <https://www.darpa.mil/news-events/2018-09-07>.
17Daniel Hoadley and Nathan Lucas,Artificial Intelligence and National Security(Washington, DC: Congressional Research Service, April 26, 2018); Marcus Weisgerber, "The Pentagon's New Artificial Intelligence Is Already Hunting Terrorists,"Defense One, December 21, 2017, <https://www.defenseone.com/technology/2017/12/pentagons-new-artificial-intelligence-already-hunting-terrorists/144742/>; Matt Leonard, "Army Leverages Machine Learning to Predict Component Failure,"Defense Systems, July 2, 2018, <           https://defensesystems.com/articles/2018/07/03/army-vehicle-predictive-maintenance.aspx>.
18"Strategic Latency and Warning: Private Sector Perspectives on Current Intelligence Challenges in Science and Technology," Report of the Expert Advisory Panel Workshop, Lawrence Livermore National Laboratory, January 8, 2016.Strategic warningdescribes the goal of alerting decisionmakers of impending threats of a strategic nature.Strategic surprisedescribes the failure to provide adequate warning of such threats.
19Kelsey Atherton, "Targeting the Future of the DOD's Controversial Project Maven Initiative,"C4ISRNET, July 27, 2018, <https://www.c4isrnet.com/it-networks/2018/07/27/targeting-the-future-of-the-dods-controversial-project-maven-initiative/>.
20Jack Corrigan, "Project Maven Uses Machine Learning to Go Through Drone Video Feeds, but That's Just the Beginning, Air Force Lt. Gen Shanahan Said,"Nextgov, November 2, 2017, <https://www.nextgov.com/cio-briefing/2017/11/three-star-general-wants-artificial-intelligence-every-new-weapon-system/142225/>.
21National Academies of Science,Autonomy in Land and Sea and in the Air and Space, Proceedings of a Forum, 2018, <http://nap.edu/25168>.
22National Academy of Sciences,Counter-Unmanned Aircraft System (CUAS) Capability for Battalion and Below Operations, Abbreviated Version of a Restricted Report, 2018, <www.nap.edu/read/24747/chapter/1>.
23Defense Science Board,Study on Countering Anti-access Systems with Longer Range and Standoff Capabilities: Assault Breaker II, 2017 Summer Study on Long Range Effects, Office of the Under Secretary of Defense for Acquisition, Technology, and Logistics, June 2018.
24Lisa Owens Davis, "Moving at the Speed of S&T: Calibrating the Role of National Laboratories to Support National Security," in Davis and Nacht,Strategic Latency: Red, White and Blue.
25"Machine Learning to Prevent Defects in Metal 3D Printed Parts in Real Time," Lawrence Livermore National Laboratory, Newsline, September 13, 2018, <https://webcenter.llnl.gov/myllnl/faces/oracle/webcenter/portalapp/pages/top-story-wrapper.jspx?articleId=52535&_afrLoop=77869951013468&_afrWindowMode=0&_afrWindowId=blank#%40%3F_afrWindowId%3Dblank%26_afrLoop%3D77869951013468%26articleId%3D52535%26_afrWindowMode%3D0%26_adf.ctrl-state%3Dt66qlfya5_65>.
26Andrew Reddie, Bethany Goldblum, Kiran Lakkaraju, Jason Reinhardt, Michael Nacht, Laura Epifanovskaya, "Next Generation Wargames"Science, December 21, 2018; Magy Seif El-Nasr, Anders Drachen, Alessandro Canossa, editors,Game Analytics: Maximizing the Value of Player Data, (London: Springer, 2013)
27Marc Pomerleau, "Can the Intel and Defense Community Conquer Data Overload?"C4ISRNET, September 5, 2018, <https://www.c4isrnet.com/intel-geoint/2018/09/05/can-the-intel-and-defense-community-conquer-data-overload/?utm_source=Sailthru&utm_medium=email&utm_campaign=daily%20brief%209/5/18&utm_term=Editorial%20-%20Daily%20Brief>.
28Marc Pomerleau, "Here's How Intelligence Agencies Will Take Advantage of Machine Learning and AI,"C4ISRNET, May 1, 2018, <https://www.c4isrnet.com/intel-geoint/2018/05/01/heres-how-intelligence-will-take-advantage-of-machine-learning-and-ai/>.
29"Deep Learning to Advance Nuclear Nonproliferation,"LLNL Newsline, August 21, 2018.
30Ben Conklin, "How Artificial Intelligence Is Transforming GEOINT," GCN, April 18, 2018, https://gcn.com/articles/2018/04/18/ai-transform-geoint.aspx; Sandra Erwin, "NGA official: Artificial Intelligence Is Changing Everything, We Need a Different Mentality,"Spacenews, May 13, 2018, <           https://spacenews.com/nga-official-artificial-intelligence-is-changing-everything-we-need-a-different-mentality/>.
31Edward Geist and Andrew Lohn,How Might Artificial Intelligence Affect the Risk of Nuclear War?(Santa Monica, CA: RAND, 2018); Paul Bracken, "The Intersection of Cyber and Nuclear War,"The Strategy Bridge, January 17, 2017, <https://thestrategybridge.org/the-bridge/2017/1/17/the-intersection-of-cyber-and-nuclear-war>.
32Jeremy Hsu, "AI Can Help Hunt Down Missile Sites in China,"Wired, November 21, 2017, <https://www.wired.com/story/ai-can-help-hunt-down-missile-sites-in-china/>.
33Jen Judson, "Hyten: To Address Russian and Chinese Missile Threats, It's All About the Sensors,"Defense News, August 7, 2018, <https://www.defensenews.com/digital-show-dailies/smd/2018/08/07/hyten-to-address-russian-and-chinese-missile-threats-its-all-about-the-sensors/>.
34Jack Corrigan, "DARPA Wants to Find Botnets Before They Attack,"Defense One, September 12, 2018, <https://www.defenseone.com/technology/2018/09/darpa-wants-find-botnets-they-attack/151199/?oref=defenseone_today_nl>.
35"Nuclear Weapons in the New CyberAge: A Report of the Cyber-Nuclear Weapons Study Group," Nuclear Threat Initiative, September 2018, <https://www.nti.org/media/documents/Cyber_report_finalsmall.pdf>.
36Michael Sulmeyer and Kathryn Dura, "Beyond Killer Robots: How Artificial Intelligence Can Improve Resilience in Cyber Space,"War on the Rocks, September 6, 2018, <https://warontherocks.com/2018/09/beyond-killer-robots-how-artificial-intelligence-can-improve-resilience-in-cyber-space/>.
37Dave Johnson,Russia's Conventional Precision Strike Capabilities, Regional Crises, and Nuclear Thresholds, Livermore Papers on Global Security no. 3, February 2018; John Warden,Limited Nuclear War: The 21st Century Challenge for the United States, Livermore Papers on Global Security no. 4, July 2018.
38Cathy O'Neil,Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy(New York: Broadway Books, 2017).
39Hillary Sanders and Joshua Saxe, "Garbage In, Garbage Out: How Purportedly Great Machine Language Models Can Be Screwed Up by Bad Data," Proceedings of Blackhat 2017, July 26–27, 2017, Las Vegas, NV.
40Jesse Emspak, "How a Machine Learns Prejudice,"Scientific American, December 29, 2016, <https://www.scientificamerican.com/article/how-a-machine-learns-prejudice/>; ProPublica, "Machine Bias," May 23, 2016, <           https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing>; Will Knight, "Forget Killer Robots, Bias Is the Real AI Danger,"Technology Review, October 3, 2017, <           https://www.technologyreview.com/s/608986/forget-killer-robotsbias-is-the-real-ai-danger/>.
41Louise Matsakis, "Researchers Fooled a Google AI into Thinking a Rifle Was a Helicopter,"Wired, December 20, 2017, <https://www.wired.com/story/researcher-fooled-a-google-ai-into-thinking-a-rifle-was-a-helicopter/>.
42Daniel Cebul, "Differentiating a Port from a Shipyard Is a New Kind of Problem for AI,"C4ISRNET, September 18, 2018, <https://www.c4isrnet.com/intel-geoint/2018/09/18/differentiating-a-port-from-a-shipyard-is-a-new-kind-of-problem-for-ai/?utm_source=Sailthru&utm_medium=email&utm_campaign=Daily%209/19&utm_term=Editorial%20-%20Daily%20Brief>.
43Anna Rohrbach et al., "Object Hallucination in Image Captioning," Cornell University Library, <https://arxiv.org/abs/1809.02156>.
44Sandia National Laboratory,Counter Adversarial Data Analytics, SAND2015-3711, May 8, 2015.
45Defense Science Board, "Memorandum for Chairman, Terms of Reference, Defense Science Board Task Force on Counter Autonomy," June 18, 2018, <https://www.acq.osd.mil/dsb/TORs/2018_TOR_CounterAutonomy_18Jun2018.pdf>.
46Tim Weiner, "Word for Word, The Cuban Missile Crisis: When Kennedy Faced Armageddon, and His Own Scornful General,"New York Times, October 5, 1997, <https://www.nytimes.com/1997/10/05/weekinreview/word-for-word-cuban-missile-crisis-when-kennedy-faced-armageddon-his-own.html>.
47Paul Scharre, "A Million Mistakes a Second,"Foreign Policy, September 12, 2018, <https://foreignpolicy.com/2018/09/12/a-million-mistakes-a-second-future-of-war/>.
48Lawrence Livermore National Laboratory, "Building a Network of Collaborative Autonomous Machines," Science and Technology Review (June 2018); Mark Pomerleau, "To Win Future Conflicts, Combatant Commands Must Be Integrated,"C4ISRNET, August 15, 2018, <https://www.c4isrnet.com/show-reporter/dodiis/2018/08/14/to-win-future-conflicts-combatant-commands-must-be-integrated/?utm_source=Sailthru&utm_medium=email&utm_campaign=Daily%208/15&utm_term=Editorial%20-%20Daily%20Brief>.
49Will Knight, "The Dark Secret at the Heart of AI,"Technology Review, April 11, 2017, <https://www.technologyreview.com/s/604087/the-dark-secret-at-the-heart-of-ai/>.
50Michael Piellusch and Tom Galvin, "Is the Chain of Command Still Meaningful?"War Room, U.S. Army War College, September 6, 2018, <https://warroom.ar mywarcollege.edu/articles/chain-of-command/>.
51Stimson Center,An Action Plan on U.S. Drone Policy: Recommendations for the Trump Administration, 2018, <https://www.stimson.org/sites/default/files/file-attachments/Stimson%20Action%20Plan%20on%20US%20Drone%20Policy.pdf>.
52Herb Lin, "Developing Responses to Cyber-Enabled Information Warfare and Influence Operations,"Lawfare, September 6, 2018, <https://www.lawfareblog.com/developing-responses-cyber-enabled-information-warfare-and-influence-operations>.
53Kori Schake, "Why We Get It Wrong: Reflecting on the Future of War," book review of Lawrence Freedman,The Future of War: A History, War on the Rocks, August 10, 2018, <https://warontherocks.com/2018/08/why-we-get-it-wrong-reflections-on-predicting-the-future-of-war/>; Richard Danzig,Driving in the Dark: Ten Propositions About Prediction and National Security, Center for a New American Security, October 2011.
54Frank Gac, Timothy Grayson, and Joseph Keogh, "What Works? Public-Private Partnerships for Development of National Security Technology," in Davis and Nacht,Strategic Latency.
55Suzanne Nossel, "Google Is Handing the Future of the Internet to China,"Foreign Policy, September 10, 2018, <https://foreignpolicy.com/2018/09/10/google-is-handing-the-future-of-the-internet-to-china/.>
56Laura Seligman, "Why the Military Must Learn to Love Silicon Valley,"Foreign Policy, September 12, 2018, <https://foreignpolicy.com/2018/09/12/why-the-military-must-learn-to-love-silicon-valley-pentagon-google-amazon/.>
57Lawrence Freedman,The Evolution of Nuclear Strategy(New York: St. Martin's Press, 1981).
58James Acton, "Escalation through Entanglement: How the Vulnerability of Command and Control Systems Raises the Risks of an Inadvertent Nuclear War,"International Security43, no. 1 (Summer 2018).
59Kier Lieber and Daryl Press, "The New Era of Counterforce: Technological Change and the Future of Nuclear Deterrence,"International Security41, no. 4 (Spring 2017).
60August Cole and Amir Husain, "Putin Says Russia's New Weapons Can't Be Beat. With AI and Robotics, They Can,"Defense One, March 13, 2018, <https://www.defenseone.com/ideas/2018/03/putin-says-russias-new-weapons-cant-be-beat-ai-and-robotics-they-can/146631/>.
61Elsa Kania and John Costello,Quantum Hegemony: China's Ambitions and the Challenge to U.S. Innovation Leadership(Washington, DC: Center for a New American Security, September 12, 2018), <https://www.cnas.org/publications/reports/quantum-hegemony>.
The post Artificial Intelligence On The Battlefield: Implications For Deterrence And Surprise - Analysis appeared first on Eurasia Review.

Load-Date: January 3, 2020


End of Document


New Map Of Milky Way Reveals Giant Wave Of Stellar Nurseries
Eurasia Review
January 8, 2020 Wednesday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 1081 words
Body


Astronomers at Harvard University have discovered a monolithic, wave-shaped gaseous structure - the largest ever seen in our galaxy - made up of interconnected stellar nurseries. Dubbed the "Radcliffe wave" in honor of the collaboration's home base, the Radcliffe Institute for Advanced Study, the discovery transforms a 150-year-old vision of nearby stellar nurseries as an expanding ring into one featuring an undulating, star-forming filament that reaches trillions of miles above and below the galactic disk.
The work, published in Nature, was enabled by a new analysis of data from the European Space Agency's Gaia spacecraft, launched in 2013 with the mission of precisely measuring the position, distance, and motion of the stars. The research team combined the super-accurate data from Gaia with other measurements to construct a detailed, 3D map of interstellar matter in the Milky Way, and noticed an unexpected pattern in the spiral arm closest to the Earth.
The researchers discovered a long, thin structure, about 9,000 light years long and 400 light years wide, with a wave-like shape, cresting 500 light years above and below the mid-plane of our Galaxy's disk. The Wave includes many of the stellar nurseries that were previously thought to form part of "Gould's Belt", a band of star-forming regions believed to be oriented around the Sun in a ring.
"No astronomer expected that we live next to a giant, wave-like collection of gas - or that it forms the Local Arm of the Milky Way," said Alyssa Goodman, the Robert Wheeler Willson Professor of Applied Astronomy at Harvard University, research associate at the Smithsonian Institution, and co-director of the Science Program at the Radcliffe Institute of Advanced Study. "We were completely shocked when we first realized how long and straight the Radcliffe Wave is, looking down on it from above in 3D - but how sinusoidal it is when viewed from Earth. The Wave's very existence is forcing us to rethink our understanding of the Milky Way's 3D structure."
"Gould and Herschel both observed bright stars forming in an arc projected on the sky, so for a long time, people have been trying to figure out if these molecular clouds actually form a ring in 3D," said João Alves, professor of stellar astrophysics at the University of Vienna and Radcliffe Fellow (2018-2019). "Instead, what we've observed is the largest coherent gas structure we know of in the galaxy, organized not in a ring but in a massive, undulating filament. The Sun lies only 500 light years from the Wave at its closest point. It's been right in front of our eyes all the time, but we couldn't see it until now."
The new, 3D map shows our galactic neighborhood in a new light, giving researchers a revised view of the Milky Way and opening the door to other major discoveries.
"We don't know what causes this shape but it could be like a ripple in a pond, as if something extraordinarily massive landed in our galaxy," said Alves. "What we do know is that our Sun interacts with this structure. It passed by a festival of supernovae as it crossed Orion 13 million years ago, and in another 13 million years it will cross the structure again, sort of like we are 'surfing the wave'."
An insider's view of the galaxy
Disentangling structures in the "dusty" galactic neighborhood within which we sit is a long-standing challenge in astronomy. In earlier studies, the research group of Douglas Finkbeiner, professor of astronomy and physics at Harvard, pioneered advanced statistical techniques to map the 3D distribution of dust using vast surveys of stars' colors. Armed with new data from Gaia, Harvard graduate students Catherine Zucker and Joshua Speagle recently augmented these techniques, dramatically improving the ability of astronomers to measure distances to star-forming regions. That work, led by Zucker, is published in the Astrophysical Journal.
"We suspected there might be larger structures that we just couldn't put in context. So, to create an accurate map of our solar neighborhood, we combined observations from space telescopes like Gaia with astrostatistics, data visualization, and numerical simulations." explained Zucker, who is an NSF Graduate Fellow and Ph.D. candidate at Harvard's Graduate School of Arts and Sciences based in Harvard's department of Astronomy.
Zucker played a key role in compiling the largest-ever catalog of accurate distances to local stellar nurseries - the basis for the 3D map used in the study. She has set herself the goal of painting a new picture of the Milky Way Galaxy, near and far. "We pulled this team together so we could go beyond processing and tabulating the data to actively visualizing it - not just for ourselves but for everyone. Now, we can literally see the Milky Way with new eyes," she said.
"Studying stellar births is complicated by imperfect data. We risk getting the details wrong, because if you're confused about distance, you're confused about size." said Finkbeiner.
Goodman agreed, "All of the stars in the universe, including our Sun, are formed in dynamic, collapsing, clouds of gas and dust. But determining how much mass the clouds have, how large they are - has been difficult, because these properties depend on how far away the cloud is."
A universe of data
According to Goodman, scientists have been studying dense clouds of gas and dust between the stars for over a hundred years, zooming in on these regions with ever-higher resolution. Before Gaia, there were no significant datasets expansive enough to reveal the galaxy's structure on large scales. Since its launch in 2013, the space observatory has enabled measurements of the distances to one billion stars in the Milky Way galaxy.
The flood of data from Gaia served as the perfect testbed for innovative, new statistical methods that reveal the shape of local stellar nurseries and their connection to the Milky Way's galactic structure. In this data-science-oriented collaboration, the Finkbeiner, Alves, and Goodman groups collaborated closely. The Finkbeiner group developed the statistical framework needed to infer the 3D distribution of the dust clouds; the Alves group contributed deep expertise on stars, star formation, and Gaia; and the Goodman group developed the 3D visualizations and analytic framework, called "glue", that allowed the Radcliffe Wave to be seen, explored, and quantitatively described.
The post New Map Of Milky Way Reveals Giant Wave Of Stellar Nurseries appeared first on Eurasia Review.

Load-Date: January 7, 2020


End of Document


Revealed The Experts And Public's Attitude Towards Gene-Edited Crops
Eurasia Review
January 26, 2020 Sunday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 1114 words
Body


Experts' interest in utilizing gene editing for the breeding crops has seen revolutionary growth. Meanwhile, people's awareness for food safety has also been increasing. To understand the attitudinal difference among experts and public towards gene-edited crops, a team of Japanese researchers, led by Dr. Naoko Kato-Nitta, a research scientist at the Joint Support-Center for Data Science Research and The Institute of Statistical Mathematics, Tokyo, Japan, conducted a survey of perceptions of the Japanese experts and public to gene editing versus other emerging or conventional breeding techniques in Japan, where the production of genetically modified crops is strictly regulated and not readily accepted.
Their findings were published in Palgrave Communications.
The authors conducted experimental web-based surveys to obtain clearer understanding of both experts and public perception of the benefits, risks and value of utilizing gene editing technology for the breeding of crops compared to other technologies. Participants for surveys consisted of 3,197 volunteers of the lay public and scientists with and without expertise in molecular biology.
According to the study, participants who had expert knowledge of molecular biology perceived emerging technologies to offer the lowest risk and highest benefits or value for food application, while lay public showed the highest risk and lowest benefit. Experts from other disciplines had similar perceptions to the lay public in terms of the risk, but similar perceptions to the molecular biology experts in terms of value. The lay public tended to perceive gene-edited crops as being both more beneficial and valuable than other genetically modified crops, while also posing less risk. Even though the differences in perception between gene editing and genetic modification was very small compared to the differences in perception between genetic modification and conventional plant breeding techniques, obtaining such results from the participants living in Japan, may hold great potential for this emerging technology.
Additionally, "the results enabled us to elucidate the deficit model's boundary conditions in science communication by proposing two new hypotheses," said Kato-Nitta. The model's assumption is that as scientific knowledge increases, so too does public acceptance of new technologies. "Firstly, this assumption was valid only for conventional science, knowledge of which can be acquired through classroom education, but not valid for emerging science, such as gene editing, knowledge on which may be acquired mainly through informal learning," Kato-Nitta said. "Secondly, the model's assumption on emerging science is valid only for increasing benefit perceptions but not for reducing risk perceptions."
Food scarcity is becoming a worldwide problem and the famine is frequently found in many regions on the earth. One of the major reasons for this is the rapid increase of the global population which has reached 7.7 billion, and is still growing, whereas area of farmable land has been continuingly decreasing because of various reasons such as extensive industrial or urban development, extended droughts and other extreme weather conditions. To compensate with the increasing needs for crops, enhancement of production through breeding has also been a powerful tool for farmers to produce more crop products from their limited resources in addition to the extensive usage of fertilizers and pesticides. Recent development of recombinant DNA technology, which is commonly called genetic engineering technology, and its adaptation to crops are believed to speed up the time-consuming breeding processes and to widen the range of genetic features to the original plants such as enhancement of nutritional value, resistance to drought, frost, or pests, for example.
The gene editing technology has been very tantalizing for molecular biologists, and in theory offers significant potential to improve the global food security; however, there are many people who are not convinced and remain somewhat skeptical, preferring to take a more cautious approach to how we produce the food that ultimately goes into our bodies and which significantly contributes to our overall health and well-being.
There are two viewpoints concerning gene editing. The first, known as product-based policy, views gene editing as technology through which no foreign genetic material is added, that is more similar to conventional plant breeding procedure than genetic modification. The second, known as process-based policy views gene editing similar to that of genetic modification, as both requires genetic manipulation to achieve the desired results, but gene editing can just do so quicker. In their study, experts in molecular biology tended to adopt product-based policy, while non-specialists tended to take the process-based policy.
For many, the potential benefits associated with utilizing these unconventional plant breeding methods are not worth the potential risks. But are these attitudes and beliefs influenced by lack of sufficient scientific knowledge of the subject, and can they be changed if the information is passed on from experts in an effort to improve public scientific literacy? This concept, which is known as the deficit model of science communication, attributes public skepticism of science and technology to a lack of understanding, arising from lack of scientific literacy or knowledge on the subject. Their research statistically elucidates where such explanation types are valid and where they are not.
According to Kato-Nitta, their findings suggest that people perceive gene editing to have greater potential than genetic modification, especially in terms of the benefit aspects of utilizing this technology. "We still have to be cautious in terms of people's attitude toward the risk and value aspects associated with this technology," she noted. "In the survey, the experts in other field perceived even more risk in gene editing than genetic modification in terms of "Possibility of misusing this technology."
"I hope my research will help to narrow the gap between scientific experts and the public." said Kato-Nitta. "The scientific experts need to understand the diverse range of people outside their domain-specific community. I am currently working on developing a new model on public communication of science and technology that can explain the key factors that affect various facets of people's attitudes toward emerging science more comprehensively than the previous studies have done."
The post Revealed The Experts And Public's Attitude Towards Gene-Edited Crops appeared first on Eurasia Review.

Load-Date: January 25, 2020


End of Document


Who's Liable? The AV Or The Human Driver?
Eurasia Review
January 15, 2020 Wednesday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 1061 words
Body


A recent decision by the National Transpiration Safety Board (NTSB) on the March 2018 Uber crash that killed a pedestrian in Arizona split the blame among Uber, the company's autonomous vehicle (AV), the safety driver in the vehicle, the victim, and the state of Arizona. With the advent of self-driving cars, the NTSB findings raise a number of questions about the uncertainty in today's legal liability system. In an accident involving an AV and a human driver, who is liable? If both are liable, how should the accident loss be apportioned between them?
AVs remove people from the hands-on task of driving and thus pose a complex challenge to today's accident tort law, which primarily punishes humans. Legal experts anticipate that, by programming driving algorithms, self-driving car manufacturers, including car designers, sensor vendors, software developers, car producers, and related parties who contribute to the design, manufacturing, and testing, will have a direct influence on traffic. While these algorithms make manufacturers indispensable actors, with their product's liability potentially playing a critical role, policy makers have not yet devised a quantitative method to assign the loss between the self-driving car and the human driver.
To tackle this problem, researchers at Columbia Engineering and Columbia Law School have developed a joint fault-based liability rule that can be used to regulate both self-driving car manufacturers and human drivers. They propose a game-theoretic model that describes the strategic interactions among the law maker, the self-driving car manufacturer, the self-driving car, and human drivers, and examine how, as the market penetration of AVs increases, the liability rule should evolve.
Their findings are outlined in a new study to be presented on January 14 by Sharon Di, assistant professor of civil engineering and engineering mechanics, and Eric Talley, Isidor and Seville Sulzbacher Professor of Law, at the Transportation Research Board's 99th Annual Meeting in Washington, D.C
While most current studies have focused on designing AVs' driving algorithms in various scenarios to ensure traffic efficiency and safety, they have not explored human drivers' behavioral adaptation to AVs. Di and Talley wondered about the "moral hazard" effect on humans, whether with exposure to more and more traffic encounters with AVs, people might be less inclined to exercise "due care" when faced with AVs on the road and drive in a more risky fashion.
"Human drivers perceive AVs as intelligent agents with the ability to adapt to more aggressive and potentially dangerous human driving behavior," says Di, who is a member of Columbia's Data Science Institute. "We found that human drivers may take advantage of this technology by driving carelessly and taking more risks, because they know that self-driving cars would be designed to drive more conservatively."
The researchers used game theory to model a world with interacting players who try to select their own actions to optimize their own goals. The players-law makers, AV manufacturers, AVs, and human drivers-have different goals in the transportation ecosystem. Law makers want to regulate traffic with improved efficiency and safety, self-driving car manufacturers are profit-driven, and both self-driving cars and human drivers interact on public roads and seek to select the best driving strategies. To capture the complex interaction among all the players, the researchers applied game theory methods to see which strategy each player settles on, so that others will not take advantage of his or her decisions.
The hierarchical game helped the team to understand the human drivers' moral hazard (how much risk drivers might decide to take on), the AV manufacturer's impact on traffic safety, and the law maker's adaptation to the new transportation ecosystem. They tested the game and its algorithm on a set of numerical examples, offering insights into behavioral evolution of AVs and HVs as the AV penetration rate increases and as cost or environment parameters vary.
The team found that an optimally designed liability policy is critical to help prevent human drivers from developing moral hazard and to assist the AV manufacturer with a tradeoff between traffic safety and production costs. Government subsidies to AV manufacturers for the reduction of production costs would greatly encourage manufacturers to produce AVs that outperform human drivers substantially and improve overall traffic safety and efficiency. Moreover, if AV manufacturers are not regulated in terms of AV technology specifications or are not properly subsidized, AV manufacturers tend to be purely profit-oriented and destructive to the overall traffic system.
"The tragic fatality in Arizona involving a self-driving automobile elicited tremendous attention from the public and policy makers about how to draw the lines of legal liability when AVs interact with human drivers, cyclists, and pedestrians," Talley adds. "The emergence of AVs introduces a particularly thorny type of uncertainty into the status quo, and one that feeds back onto AV manufacturing and design. Legal liability for accidents between automobiles and pedestrians typically involves a complex calculus of comparative fault assessments for each of the aforementioned groups. The introduction of an autonomous vehicle can complicate matters further by adding other parties to the mix, such as the manufacturers of hardware and programmers of software. And insurance coverage distorts matters further by including third party stakeholders. We hope our analytical tools will assist AV policy-makers with their regulatory decisions, and in doing so, will help mitigate uncertainty in the existing regulatory environment around AV technologies."
Di and Talley are now looking at multiple AV manufacturers that target different global markets with different technological specifications, making the development of legal rules even more complex.
"We know that human drivers will take more risks and develop moral hazard if they think their road environment has become safer," Di notes. "It's clear that an optimal liability rule design is crucial to improve social welfare and road safety with advanced transportation technologies."
The post Who's Liable? The AV Or The Human Driver? appeared first on Eurasia Review.

Load-Date: January 15, 2020


End of Document


Facebook, Cambridge Analytica And Surveillance Capitalism - OpEd
Eurasia Review
March 23, 2018 Friday


Copyright 2018 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 1429 words
Body


Whether it creeps into politics, marketing, or simple profiling, the nature of surveillance as totality has been affirmed by certain events this decade.  The Edward Snowden disclosures of 2013 demonstrated the complicity and collusion between Silicon Valley and the technological stewards of the national security state.
It took the election of Donald J. Trump in 2016 to move the issue of social media profiling, sharing and targeting of information, to another level.  Not only could companies such as Facebook monetise their user base; those details could, in turn, be plundered, mined and exploited for political purpose.
As a social phenomenon, Facebook could not help but become a juggernaut inimical to the private sphere it has so comprehensively colonised.  "Facebook in particular," claimed WikiLeaks' Julian Assange in May 2011, "is the most appalling spy machine that has ever been invented." It furnished "the world's most comprehensive database about people, their relationships, their names, their addresses, their locations, their communications with each other, and their relatives, all sitting within the United States, all accessible to US intelligence."
Now, the unsurprising role played by Cambridge Analytica with its Facebook accessory to politicise and monetise data reveals the tenuous ground notions of privacy rest upon.  Outrage and uproar has been registered, much of it to do with a simple fact: data was used to manipulate, massage and deliver a result to Trump – or so goes the presumption.  An instructive lesson here would be to run the counter-factual: had Hillary Clinton won, would this seething discontent be quite so enthusiastic?
Be that as it may, the spoliations of Cambridge Analytica are embedded in a broader undertaking: the evisceration of privacy, and the generation of user profiles gathered through modern humanity's most remarkable surveillance machine.  The clincher here is the link with Facebook, though the company insists that it "received data from a contractor, which we deleted after Facebook told us the contractor had breached their terms of service."
Both Facebook and Cambridge Analytica have attempted to isolate and distance that particular contractor, a certain Aleksandr Kogan, the Cambridge University researcher whose personality quiz app "thisisyourdigitallife" farmed the personal data of some 50 million users who were then micro-targeted for reasons of political advertising.
The sinister genius behind this was the ballooning from the initial downloads – some 270,000 people – who exchanged personal data on their friends including their "likes" for personality predictions.  A broader data set of profiles were thereby created and quarried.
Kogan claims to have been approached by Cambridge Analytica, rather than the other way around, regarding "terms of usage of Facebook data".  He was also reassured that the scheme was legal, being "commercial" in nature and typical of the way "tens of thousands of apps" were using social media data.  But it took Cambridge Analytica's whistleblower, Christopher Wylie, to reveal that data obtained via Kogan's app was, in fact, used for micro-targeting the US electorate in breach of privacy protocols.
Mark Zuckerberg's response has entailed vigorous hand washing.  In 2015, he claims that Facebook had learned that Cambridge Analytica shared data from Kogan's app.  "It is against our policies for developers to share data without other people's consent, so we immediately banned Kogan's app from our platform". Certifications were duly provided that such data had been deleted, though the crew at Facebook evidently took these at unverified face value.  Not so, as matters transpired, leading to the claim that trust had not only been breached between Facebook, Kogan and Cambridge Analytica, but with the users themselves.
Facebook, for its part, has been modestly contrite.  "We have a responsibility to protect your data," went Zuckerberg in a statement, "and if we can't then we don't deserve to serve you."  His posted statement attempts to water down the fuss.  Data protections – most of them, at least – were already being put in place. He described the limitations placed on the accessing of user information by data apps connected to Facebook friends.
The networked sphere, as it is termed in with jargon-heavy fondness by some academics, has seen the accumulation of data all set and readied for the "information civilisation".  Google's chief economist Hal Varian has been singled out for special interest, keen on what he terms, in truly benign fashion, "computer-mediated transactions".  These entail "data extraction and analysis," various "new contractual forms" arising from "better monitoring", "personalisation and customisation" and "continuous experiments".
Such are the vagaries of the information age. As a user of such freely provided services, users are before a naked confessional, conceding and surrendering identities to third parties with Faustian ease.  This surrender has its invidious by products, supplying intelligence and security services accessible data.
Cambridge Analytica, for its part, sets itself up as an apotheosis of the information civilisation, a benevolent, professionally driven information hitman. "Data drives all we do," it boldly states to potential clients.  "Cambridge Analytica uses data to change audience behaviour."
This sounds rather different to the company's stance on Saturday, when it claimed that, "Advertising is not coercive; people are smarter than that."  With cold show insistence, it insisted that, "This isn't a spy movie."
Two services are provided suggesting that people are not, in the minds of its bewitchers, that intelligent: the arm of data-driven marketing designed to "improve your brand's marketing effectiveness by changing consumer behaviour" and that of "data-driven campaigns" where "greater influence" is attained through "knowing your electorate better".
On the latter, it is boastful, claiming to have supported over 100 campaigns across five continents. "Within the United States alone, we have played a pivotal role in winning presidential races as well as congressional and state elections."
CA has donned its combat fatigues to battle critics.  Its Board of Directors has suspended CEO Alexander Nix, claiming that "recent comments secretly recorded by Channel 4 and other allegations do not represent the values or operations of the firm and his suspension reflects the seriousness with which we view this violation."
The comments in question, caught in an undercover video, show Nix offering a range of services to the Channel 4 undercover reporter: Ukrainian sex workers posing as "honey-traps" a video evidencing corruption that might be uploaded to the Internet; and operations with former spies. "We can set up fake IDs and Web sites, we can be students doing research projects attached to a university; we can be tourists."
The company has also attempted to debunk a set of what it sees as flourishing myths.  It has not, for instance, been uncooperative with the UK's data regulator, the Information Commissioner's Office, having engaged it since February 2017.  It rejects notions that it peddles fake news. "Fake news is a serious concern for all of us in the marketing industry."  (Nix's cavalier advertising to prospective clients suggests otherwise.)
In other respects, Cambridge Analytica also rejected using Facebook data in its political models, despite having obtained that same data.  "We ran a standard political data science program with the same kind of political preference models used by other presidential campaigns."  Nor did it use personality profiles for the 2016 US Presidential election. Having only hopped on board in June, "we focused on the core elements of a core political data science program."
The company's weasel wording has certainly been extensive.  Nix has done much to meander, dodge and contradict.  On the one hand, he would like to take credit for the company's product – the swaying of a US election.  But in doing so, it did not use "psychographic" profiles.
Surveillance capitalism is the rope which binds the actors of this latest drama in the annals of privacy's demise.  There are discussions that political data mining designed to manipulate and sway elections be considered in the same way political donations are.  But in the US, where money and political information are oft confused as matters of freedom, movement on this will be slow.  The likes of Cambridge Analytica and similar information mercenaries will continue thriving.

Load-Date: March 23, 2018


End of Document


Surface Clean-Up Technology Won't Solve Ocean Plastic Problem
Eurasia Review
August 4, 2020 Tuesday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 717 words
Body


Clean-up devices that collect waste from the ocean surface won't solve the plastic pollution problem, a new study shows.
Researchers compared estimates of current and future plastic waste with the ability of floating clean-up devices to collect it - and found the impact of such devices was "very modest". However, river barriers could be more effective and - though they have no impact on plastic already in the oceans - they could reduce pollution "significantly" if used in tandem with surface clean-up technology.
The study - by the University of Exeter, the Leibniz Centre for Tropical Marine Research, the Leibniz Institute for Zoo and Wildlife Research, Jacobs University and Making Oceans Plastic Free - focusses on floating plastic, as sunk waste is difficult or impossible to remove depending on size and location.
The authors estimate that the amount of plastic reaching the ocean will peak in 2029, and surface plastic will hit more than 860,000 metric tonnes - more than double the current estimated 399,000 - by 2052 (when previous research suggested the rate of plastic pollution may finally reach zero).
"The important message of this paper is that we can't keep polluting the oceans and hoping that technology will tidy up the mess," said Dr Jesse F. Abrams, of the Global Systems Institute and the Institute for Data Science and Artificial Intelligence, both at the University of Exeter.
"Even if we could collect all the plastic in the oceans - which we can't - it's really difficult to recycle, especially if plastic fragments have floated for a long time and been degraded or bio-fouled.
"The other major solutions are to bury or burn it - but burying could contaminate the ground and burning leads to extra CO2 emissions to the atmosphere."
Private initiatives proposing to collect plastic from oceans and rivers have gained widespread attention recently.
One such scheme, called the Ocean Cleanup, aims to clean the "Pacific garbage patch" in the next 20 years using 600m floating barriers to collect plastic for recycling or incineration on land.
The new study analysed the impact of deploying 200 such devices, running without downtime for 130 years - from 2020 to 2150.
In this scenario, global floating plastic debris would be reduced by 44,900 metric tonnes - just over 5% of the estimated global total by the end of that period.
"The projected impact of both single and multiple clean up devices is very modest compared to the amount of plastic that is constantly entering the ocean," said Dr Sönke Hohn, of Leibniz Centre for Tropical Marine Research.
"These devices are also relatively expensive to make and maintain per unit of plastic removed."
As most plastic enters the oceans via rivers, the authors say a "complete halt" of such pollution entering the ocean using river barriers - especially in key polluting rivers - could prevent most of the pollution they otherwise predict over the next three decades.
However, due to the importance of large rivers for global shipping, such barriers are unlikely to be installed on a large scale.
Given the difficulty of recycling and the negative impacts of burying or burning plastic, the study says reducing disposal and increasing recycling rates are essential to tackle ocean pollution. "Plastic is an extremely versatile material with a wide range of consumer and industrial applications, but we need to look for more sustainable alternatives and rethink the way we produce, consume and dispose of plastic," said Professor Agostino Merico, of Leibniz Centre for Tropical Marine Research and Jacobs University.
Dr Roger Spranz, an author of the study, is a co-founder of non-profit organisation Making Oceans Plastic Free.
"We have developed expertise in changing behaviour to break plastic habits and stop plastic pollution at its source," Dr Spranz said.
"We are registered in Germany but the focus of our activities and collaborations is in Indonesia, the second-largest source of marine plastic pollution.
"Working with local partners, the implementation of our Tasini campaign in Indonesia has to date helped to prevent an estimated 20 million plastic bags and 50,000 plastic bottles from ending up in coastal areas and the ocean."
The article Surface Clean-Up Technology Won't Solve Ocean Plastic Problem appeared first on Eurasia Review.

Load-Date: August 5, 2020


End of Document


More Evidence Of Causal Link Between Air Pollution And Early Death
Eurasia Review
June 29, 2020 Monday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 579 words
Body


Strengthening U.S. air quality standards for fine particulate pollution to be in compliance with current World Health Association (WHO) guidelines could save more than 140,000 lives over the course of a decade, according to a new study from Harvard T.H. Chan School of Public Health.
The study, published June 26, 2020 inSciences Advances, provides the most comprehensive evidence to date of the causal link between long-term exposure to fine particulate (PM2.5) air pollution and premature death, according to the authors.
"Our new study included the largest-ever dataset of older Americans and used multiple analytical methods, including statistical methods for causal inference, to show that current U.S. standards for PM2.5 concentrations are not protective enough and should be lowered to ensure that vulnerable populations, such as the elderly, are safe," said doctoral student Xiao Wu, a co-author of the study.
The new research builds on a 2017 study that showed that long-term exposure to PM2.5 pollution and ozone, even at levels below current U.S. air quality standards, increases the risk of premature death among the elderly in the U.S.
For the new study, researchers looked at 16 years' worth of data from 68.5 million Medicare enrollees--97% of Americans over the age of 65--adjusting for factors such as body mass index, smoking, ethnicity, income, and education. They matched participants' zip codes with air pollution data gathered from locations across the U.S. In estimating daily levels of PM2.5 air pollution for each zip code, the researchers also took into account satellite data, land-use information, weather variables, and other factors. They used two traditional statistical approaches as well as three state-of-the-art approaches aimed at teasing out cause and effect.
Results were consistent across all five different types of analyses, offering what authors called "the most robust and reproducible evidence to date" on the causal link between exposure to PM2.5 and mortality among Medicare enrollees--even at levels below the current U.S. air quality standard of 12 ?g/m3 (12 micrograms per cubic meter) per year.
The authors found that an annual decrease of 10 ?g/m3 in PM2.5 pollution would lead to a 6%-7% decrease in mortality risk. Based on that finding, they estimated that if the U.S. lowered its annual PM2.5 standard to 10 ?g/m3--the WHO annual guideline--143,257 lives would be saved in one decade.
The authors included additional analyses focused on causation, which address criticisms that traditional analytical methods are not sufficient to inform revisions of national air quality standards. The new analyses enabled the researchers, in effect, to mimic a randomized study--considered the gold standard in assessing causality--thereby strengthening the finding of a link between air pollution and early death.
"The Environmental Protection Agency has proposed retaining current national air quality standards. But, as our new analysis shows, the current standards aren't protective enough, and strengthening them could save thousands of lives. With the public comment period for the EPA proposal ending on June 29, we hope our results can inform policymakers' decisions about potentially updating the standards," said co-author Francesca Dominici, Clarence James Gamble Professor of Biostatistics, Population, and Data Science.
The article More Evidence Of Causal Link Between Air Pollution And Early Death appeared first on Eurasia Review.

Load-Date: June 29, 2020


End of Document


There's No End In Sight To The Zombie Economy – OpEd
Eurasia Review
June 2, 2020 Tuesday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 988 words
Body


By Andrew Moran*
The United States was waiting for the zombie apocalypse. The country was given a coronapocalypse instead. But could the two events merge and provide the nation with a dangerous economic trend? Corporate America's worst-kept secret had been the swelling number of zombies kept on life support and hidden away during the boom phase of the business cycle. Now that the coronavirus pandemic has exposed the fault lines underneath theeconomy, the zombification may accelerate due to a toxic concoction of Federal Reserve stimulus and congressional relief. Will zombies leave their graves, searching for freshly created US dollars and feasting on the carcass of the ailing marketplace?
The Walking Dead
Azombiecompany is a business that requires perpetual bailouts to keep its doors open, or it is a deeply indebted firm that can only repay the interest on its debt. The zombification has been eating away at Japan and China, and now it is gradually infecting the US economy. What's worse is that American zombie businesses employ about 2 million people, according to new Arbor Data Science figures.
Workers employed by zombie companies are found in many different industries. Arbor's research found that the top five sectors by employee headcount were:
Industrial conglomerates: 233,000Hardware, storage, and peripherals: 193,000Energy equipment and services: 185,000Hotels, restaurants, and leisure: 153,000Software: 142,000
But could the fragility of the market cause a huge number of workers to file for unemployment benefits? It might seem counterintuitive, but it has become a lot easier for these businesses to be resuscitated.
One institution has been supplying these walkers with brain nourishment: the Federal Reserve.
Feeding the Undead
Zombies have been finding it easier to borrow for a combination of reasons. The first is that interest rates are historically low, so it can be less difficult to repay the interest on the debt and use the cost savings to keep the lights on. The second is that the US central bank has taken unprecedented action by acquiringcorporate debtthrough the secondary exchange-traded fund (ETF) market.
Put simply, if it were not for accommodative monetary policy, these firms would have otherwise shut down by now. Once again, the Eccles Building is refusing to allow the invisible hand to rein in the excess for fear a liquidity crisis, credit crunch, and every other fancy way of saying we are in a dilly of a pickle.
But it is the entire market that is taking advantage of a desperate Fed. US companies are borrowing at the fastest pace in history year-to-date, issuing more than $1 trillion in new bonds, according to Bank of America Global. This is about double the number from 2019 during the same period. This eyebrow-raising number highlights two important facts in this environment: Companies are borrowing more cheaply than anybody would have anticipated last year, and investors are being paid so little to fund operations in an uncertain market.
MarketWatch alluded to AutoNation as a struggling company that recently borrowed $500 million from the bond market. The national chain of car dealerships posted a $232.3 million net loss in the first quarter, but it still attracted investor interest due to its 4.4 percent yield. Under present conditions, traders know that their investment is insured, because the central bank could just swoop in and rescue a troubled asset to avoid a powder keg from going off.
And it is not just Fed chair Jerome Powell going on a bond-buying spree. There is still obviously a demand for all kinds of bonds—investment grade and junk status—and this is fueling the rise of moribund companies that are gorging on debt. The undead can roam the streets for several more years if this is thede factomonetary policy.
Zombification
In Tokyo and Beijing, the typical walking dead are the banks. In the postcoronavirus economy, it is evident that the next generation of zombies will be cruise lines, retailers, and airlines. In a truly free market, these industries would eliminate a large number of companies, but because of cheap money and an accommodative Fed, that is not going to happen. Many of these businesses are also benefiting from creditors waiving or loosening previous debts, allowing some of the world's largest organizations to reach noteworthy agreements with lenders.
This is terrible news for Main Street, because now capital is being misallocated and transferred to unproductive enterprises. Businesses—large or small—with high growth prospects may not see their visions realized, because the money is going to prop up the Marriotts and Vail Resorts of the world.
With the death of small businesses potentially nigh, the entrepreneurial spirit may wither away before it even has a chance to be zombified, because it cannot access as much liquidity as the big boys.
Money Printer Go Brrr
The ramifications of these whatever-it-takes and money printer go brrr policies will only be felt in a few years, when the Fed chooses to tighten up and remove the training wheels. We have seen what happens when the Fed scales back its quantitative easing efforts: triple-digit losses on the stock market and ballooning debt-servicing payments. If this ever happens, a tidal wave of debt defaults and bankruptcies will swallow the US. Should the central bank refrain from embarking upon a prescription of tightening, a new type of economy will be born: the anti-productivity economy, comparable to Japan in the 1990s and China today. You could even make a movie out of it: Big Trouble in Little Tokyo: Dawn of the American Dead.
About the author: Andrew Moran is the Economics Correspondent at LibertyNation.com and is the author of The War on Cash. You can find more of his work at AndrewMoran.net.
Source: This article was originally published by LibertyNation.
The post There's No End In Sight To The Zombie Economy - OpEd appeared first on Eurasia Review.

Load-Date: June 2, 2020


End of Document


Limits To Strategic Foresight: Try Wisdom Of The Crowds – Analysis
Eurasia Review
May 21, 2020 Thursday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 1292 words
Body


Even for a nation with Singapore's foresight capability, the full-range of COVID-19's consequences could not have been foreseen. Would not now be the ideal time to revisit key tenets of Singapore's foresight enterprise – and by implication our national security framework – and perhaps limiting the impact of future strategic surprise?
By Shashi Jayakumar and Adrian W J Kuah*
Could Singapore, with its strategic foresight capability, have predicted COVID-19? It is reasonable to ask this, since Singapore is admired globally for its strategic foresight capability. Two milestones stand out in the development of this capability.
The first occurred in the mid-1980s, when the Government experimented with using long-term scenarios in defence planning and not long after rolled it out for broader use across the different ministries. This led to the practice of "National Scenarios" that forms, at least partly, the basis for long-term planning. The second was a major review which in 2004 led to the Strategic Framework for Singapore's National Security.
Swan, Elephant, or Something Else?
Deputy Prime Minister and Coordinating Minister for Security and Defence Dr Tony Tan's 2005Ministerial Statementon the Framework observed that "the most important recommendation" was to "set up a new Coordinating Structure in the centre of Government…in the Prime Minister's Office."
Dr Tan, in highlighting the importance of investing in "imaginative solutions to intractable problems", also announced the establishment of a Risk Assessment and Horizon Scanning capability intended "to help us anticipate and deal with shocks to our system."
The Risk Assessment and Horizon Scanning Programme Office was set up within NSCS, alongside the other strategic foresight apparatus of the Government such as the Centre for Strategic Futures in the Prime Minister's Office.
Returning to the question posed above: was COVID-19 inherently unpredictable? Was it a "black swan" that would have defied the best foresight capability? Or was COVID-19 a "black elephant", the sort of catastrophic problem identified in advance but ultimately ignored, sometimes wilfully?
But maybe there is a third possibility ? that catastrophic surprise is context specific: it is not simply that it is unforeseen and unforeseeable, but seems that way because of the limits imposed by one's perspective and experiences.
Limits to Whole-of-Government Approach?
Notwithstanding the lapse when it came to assessing the magnitude of risk to foreign worker dormitories, agencies have not been unprepared. They have for years been anticipating, and preparing for, disruptions to food security (through source diversification) and infectious diseases outbreaks (the opening of the National Centre for Infectious Diseases in 2019 being a case in point).
Still, COVID-19 should give us pause: there is the opportunity to use this crisis, and do deep thinking on the our strategic foresight enterprise, and cognitive diversity within.
A horizon scanning system, for example, combines data science and data analytics with the craft and intuition of the analyst, the scenario planner or the futurist, so that we can mitigate the failure of imagination and the lack of communication that allows people to feign ignorance and thereafter act surprised.
A well-designed horizon scanning system is underpinned by Ashby's Law of Requisite Variety, encapsulated in the aphorism, "it takes complexity to defeat complexity." To better able to intuit catastrophic surprises, it helps if we look in different directions, have different mental models, and are constantly and courageously challenging each other's interpretations of what is going on.
So, could a horizon scanning system have foreseen COVID-19? Alas, probably not, and certainly not in its painful detail. But COVID-19 has shown that the best whole-of-government efforts must be enhanced. To be sure, centralising strategic foresight and horizon scanning capability within the Government, albeit with frequent consultations with outsiders, certainly makes for greater efficiency.
Wisdom of the Crowds: Look Beyond Government
But it also creates a single point of failure. To prevent this, we need to inject the requisite diversity of viewpoints and expertise necessary to detect and tackle emerging threats. Needless to say, this more distributed approach will have to trade off efficiency for robustness (and messiness).
If complexity is required to defeat complexity, then the diversity of expertise, knowledge and insights that can help Singapore deal with these emerging threats may well come as much from outside of Government – from the usual quarters such as universities and think tanks, as well as unexpected ones such as NGOs and loving critics – as from within.
In his 2004 "The Wisdom of Crowds", James Surowiecki suggested that, under certain conditions, a sufficiently diverse and independent group could produce more accurate forecasts and guesses than so-called experts.
To mitigate that "single point of failure", the strategic foresight enterprise should be less of a "Government-hub and different spokes" model, and more of a decentralised network with diverse and independent nodes simultaneously complementing and countering each other in making sense of the world. This idea has been put into practice by various corporations and government agencies, in the form of hackathons, crowdsourcing for ideas, and prediction markets.
There is a practical consideration. No matter how enlightened a government or organisation is, there is a limit to inconvenient albeit useful insights it is willing to accept, much less embrace, especially if surfaced internally. Moreover, an internal group established to counter groupthink can itself on occasion be susceptible to groupthink.
By building capabilities outside of the Government or drawing on pre-existing ones, we can shift the cognitive odds in our favour even though the foresight enterprise remains a fallible one.
National Security: Time for Review
A distributed, extra-Government horizon scanning process provides a platform for multiple interpretations of ambiguous weak signals to occur. Incompatible interpretations of the same signal, far from causing consternation, are instructive: it tells you that it does not conform to any previous pattern, and we should pay it even more attention, instead of trying to force a convergence of views.
This is where culture comes into play: we should further develop a culture that takes on board potentially useful albeit unpalatable insights surfaced by outsiders.
We are now at a point where national security and strategic foresight need to encompass and embrace that emerging range of uncomfortable novelties: not just pandemics, but food security, climate change, and grey zone operations such as disinformation. Some 15 years ago, Singapore's national security enterprise placed resilience and whole-of-government coordination front and centre, and augmented its foresight capabilities with horizon scanning.
Given how the world has changed because of COVID-19 (though not only because of it), another review may be in order, one that expands the national security discourse from one that is whole-of-government to whole-of-society, in order to tap the wisdom of the crowds.
*Shashi Jayakumar is Head, Centre of Excellence for National Security (CENS) and Executive Coordinator for Future Issues and Technology (FIT) cluster at the S. Rajaratnam School of International Studies (RSIS), Nanyang Technological University (NTU), Singapore. Adrian W J Kuah is Director, Futures Office at the National University of Singapore. This is part of an RSIS Series.
The post Limits To Strategic Foresight: Try Wisdom Of The Crowds - Analysis appeared first on Eurasia Review.

Load-Date: May 21, 2020


End of Document


Baseball Illustrates Economics With Each Game – OpEd
Eurasia Review
December 28, 2019 Saturday


Copyright 2019 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 1417 words
Body


By Andrew Moran*
Legendarybaseballmanager Tommy Lasorda wrote, "No matter how good you are, you're going to lose one-third of your games. No matter how bad you are you're going to win one-third of your games. It's the other third that makes the difference." Baseball is a game of failure; you are going to groundout, flyout, and strikeout more than you are going to get a base hit or a walk. Are you batting .289? You are having a good season.
Baseball is a unique sport because nothing else can replicate it. The thinking man's game is also a great mechanism to learn abouteconomics. Let's explore some of the various economic principles that Major League Baseball (MLB) teaches us.
Creative Destruction
In economics, creative destruction, also known as Schumpeter's gale, is the process of ending conventional practices to make room for innovation. You will find creative destruction in every industry, whether it is transportation or finance. It is even ubiquitous in The Show.
Baseball is in a constant state of evolution. The greatest sport on the planet routinely innovates thanks to the ingenuity and creativity of players, managers, and front offices. From the way pitchers throw from the stretch to the way the defense is positioned on the diamond, ballclubs are always searching for a competitive edge, and the only way to achieve this is by employing unique strategies and tactics.
The best example of creative destruction in the MLB is the infield shift.Although the defensive shift has been around for a century, this realignment oddity has become the norm in the last decade (thanks, Joe Maddon!). You will see third basemen playing on the other side of second base to prevent left-handed batters from placing the ball into the gap between fielders. Also, thanks to advanced analytics, teams are further maneuvering players to spots where the hitters – left- and right-handed – are more likely to place their balls.
You are starting to witness, on occasion, the outfield doing the shift. It has become so prevalent due to its efficacy that there has been a discussion about banning the measure. Until then, the shift will be used and refined by franchises across the league.It might not be as exciting as seeing a bullet into the gap in shallow right-field, but the shift is effective.
Subjective Value
Thesubjective theoryof value is the concept that a good's or a service's worth is not innate but rather determined by consumers based on how much they want or need the object. This economic principle can be found in every component of professional baseball. Everyone is participating in it, too: the owners, the players, and even the fans.
Many people will say that it is ridiculous to pay one athlete $324 million over nine years. But that is exactly what the New York Yankees did when they signed Gerrit Cole. The Boston Red Sox thought it was worth signing David Price to a $217 million contract. The Los Angeles Angels opened the checkbook and inked an aging Albert Pujols to a 10-year, $240 million deal.
This is a lot of money to spend on just one guy, especially when you have nine players on the field and 25 men on the main roster. Owners who sign these players have a couple of things in mind when handing out such lucrative contracts, including marketing dollars and championships. If your team has a superstar, then the idea is that fans are more likely to see the team. If that player helps the club win a championship, then paying a single person $27 million a year was worth the cost.
Fans make similar decisions but on a much smaller scale. When they visit Fenway Park (Red Sox) or Citi Field (New York Mets), they are paying $30, $50, or $200 for tickets, proving that they value the tickets more than the cash. Plus, these fans will pay premium prices for hot dogs and beer, showing, once again, that they prefer to consume ballpark food and drinks than to keep their hard-earned money.
To non-baseball fans, It might seem like a waste of money and time. However, baseball aficionados might feel the same way aboutStar Warsnerd culture and grown men buying fake lightsabers and pretending to be Luke Skywalker at Comic-Con. It is all about subjective value.
Self-Regulating Markets
You may have heard about the Houston Astros' sign-stealing controversy from the 2017 season and postseason. If you have not, let's just say that it consisted of overhead cameras, trashcan banging, and whistling. While many teams and players suspected foul play on the part of the 2017 World Series champions, nothing was ever proven and little national attention was generated. The MLB has initiated an exhaustive investigation and is expected to release the results of the probe in early 2020.
Sign-stealing is common in baseball, just not on the alleged level of the Astros. A runner on second base, for example, can try to determine what the pitcher is going to throw and then convey to the hitter what type of pitch he can expect. A pitcher might have a tell, revealing to batters what he is going to toss from the mound. Teams are, rightfully, so paranoid about sign-stealing that they have taken matters into their own hands.
If you watch a random MLB broadcast, you might notice the back catcher delivering multiple signs tothe pitcher. The purpose of this act is to ensure that the opposing squad cannot quickly and accurately determine the signal if the coming pitch is high cheese with some hair on it or salad (consult your nearest Ecktionary for definitions).
Rather than wait for rule changes or bans, some teams are attempting to eliminate sign-stealing with complex signs and unique communication. Should clubs fail to correctly decipher what signs are being shown, theywillinevitably shut down operations and try to win games by just being the better team through tactics and athleticism. This is how markets regulate themselves without any state interventions.
Labor Negotiations
How did you land your job? Chances are, you worked on an arrangement with the hiring manager that satisfied both parties.Liberty Nationrecently reported on the $800 million in contracts that agent Scott Boras helped negotiate during the 2019 winter meetings in San Diego, CA. For all the insults thrown his way, he is an effective agent who gets the job done by generating top dollar for his clients. Ask Cole, ask Anthony Rendon, and ask Stephen Strasburg.
In baseball, there is no salary cap, meaning teams can spend any amount they wish on a player of their choosing. Depending on the state of the market and the teams' needs, elite athletes usually have the advantage. They typically demand a few things: dollar-amount, length of the contract, and average annual value (AAV). A team may offer a five-year deal worth $143 million, but a player may request a six-year deal valued at $146 million.
To get what they want, athletes will have the best people around them negotiating and ironing out the terms and conditions of a lucrative contract. General managers may attempt to sweeten the pot by throwing in incentives: If a player finishes in the top 10 of MVP voting each season, then he will earn an extra $2 million a year – or a pitcher may need three consecutive seasons of 200 or more innings to receive a $4 million bonus and a club option.
Sometimes it works and sometimes it doesn't. The point is all about labor relations and how employers and employees agree on a working partnership.
As American As Apple Pie
Today, every team's front office is filled with employees holding degrees in economics, mathematics, or data science. Baseball was always a game of strategy, but now this has morphed into intricate and obscure measurements, such as xFIP, BABIP, and REW – all Sabermetrics that would trigger headaches for non-baseball fans and non-STEM majors. Key performance indicators are integral to the sport, but baseball is also a fantastic physical display of the various laws and principles of economics. In a world where this discipline is portrayed with great disdain, baseball can breathe some life into the science.
*About the author: Economics Correspondent at LibertyNation.com. Andrew has written extensively on economics, business, and political subjects for the last decade. He also writes about economics at Economic Collapse News and commodities at EarnForex.com. He is the author of "The War on Cash." You can learn more at AndrewMoran.net.
Source: This article was published by Liberty Nation
The post Baseball Illustrates Economics With Each Game - OpEd appeared first on Eurasia Review.

Load-Date: December 27, 2019


End of Document


More Than A Lifetime Away: World Faces 100-Year Wait For Gender Parity
Eurasia Review
December 17, 2019 Tuesday


Copyright 2019 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 1993 words
Body


The time it will take to close the gender gap narrowed to 99.5 years in 2019. While an improvement on 2018 – when the gap was calculated to take 108 years to close – it still means parity between men and women across health, education, work and politics will take more than a lifetime to achieve. This is the finding of the World Economic Forum's Global Gender Gap Report 2020, published Monday.
According to the report, this year's improvement can largely be ascribed to a significant increase in the number of women in politics. The political gender gap will take 95 years to close, compared to 107 years last year. Worldwide in 2019, women now hold 25.2% of parliamentary lower-house seats and 21.2% of ministerial positions, compared to 24.1% and 19% respectively last year.
Politics, however, remains the area where least progress has been made to date. With Educational Attainment and Health and Survival much closer to parity on 96.1% and 95.7% respectively, the other major battlefield is economic participation. Here, the gap widened in 2019 to 57.8% closed from 58.1% closed in 2018. Looking simply at the progress that has been made since 2006 when the World Economic Forum first began measuring the gender gap, this economic gender gap will take 257 years to close, compared to 202 years last year.
Economic Gap Widening
The report attributes the economic gender gap to a number of factors. These include stubbornly low levels of women in managerial or leadership positions, wage stagnation, labour force participation and income. Women have been hit by a triple whammy: first, they are more highly represented in many of the roles that have been hit hardest by automation, for example, retail and white-collar clerical roles.
Second, not enough women are entering those professions – often but not exclusively technology-driven – where wage growth has been the most pronounced. As a result, women in work too often find themselves in middle-low wage categories that have been stagnant since the financial crisis 10 years ago.
Third, perennial factors such as lack of care infrastructure and lack of access to capital strongly limit women's workforce opportunities. Women spend at least twice as much time on care and voluntary work in every country where data is available, and lack of access to capital prevents women from pursuing entrepreneurial activity, another key driver of income.
"Supporting gender parity is critical to ensuring strong, cohesive and resilient societies around the world. For business, too, diversity will be an essential element to demonstrate that stakeholder capitalism is the guiding principle. This is why the World Economic Forum is working with business and government stakeholders to accelerate efforts to close the gender gap," said Klaus Schwab, Founder and Executive Chairman of the World Economic Forum.
Could the "Role Model Effect" close the gender gap?
One positive development is the possibility that a "role model effect" may be starting to have an impact in terms of leadership and possibly also wages. For example, in eight of the top 10 countries this year, high political empowerment corresponds with high numbers of women in senior roles. Comparing changes in political empowerment from 2006 to 2019 shows that improvements in political representation occurred simultaneously with improvements in women in senior roles in the labour market.
While this is a correlation, not a causation, in OECD countries, where women have been in leadership roles for relatively longer and social norms started to change earlier, role model effects could contribute to shaping labour market outcomes.
Gender Inequality in the Jobs of the Future
Possibly the greatest challenge preventing the economic gender gap from closing is women's under-representation in emerging roles. New analysis conducted in partnership with LinkedIn shows that women are, on average, heavily under-represented in most emerging professions. This gap is most pronounced across our "cloud computing" job cluster where only 12% of all professionals are women. The situation is hardly better in "engineering" (15%) and "Data and AI" (26%), however women do outnumber men in two fast-growing job clusters, "content production" and "people and culture".
According to our data, this reality presents leaders intent on addressing the gender gap in the future with two key challenges. The first and most obvious challenge is that more must be done to equip women with the skills to perform the most in-demand jobs. Indeed, there is an economic cost of not doing so as skills shortages in these professions hold back economic growth.
The second is possibly more complex. According to our data, even where women have the relevant in-demand skillset they are not always equally represented. In data science, for example, 31% of those with the relevant skillset are women even though only 25% of roles are held by women. Likewise, there is no gender gap in terms of skills when it comes to digital specialists, however only 41% of these jobs are performed by women.
These facts point to three key strategies that must be followed to hardwire gender equality into future workforces: to ensure women are equipped in the first place – either through skilling or reskilling – with disruptive technical skills; to follow-up by enhancing diverse hiring; and to create inclusive work cultures.
"Insights from LinkedIn's Economic Graph can help policymakers, business leaders, and educators understand and prepare for how women will be represented in the future workforce. Our data shows that meaningful action is needed to build the systems and talent pipelines required to close the gender gap in tech and ensure women have an equal role in building the future," said Allen Blue, Co-Founder and Vice-President, Product Strategy, LinkedIn.
What the Forum is Doing to Close the Gender Gap
The World Economic Forum's Platform for Shaping the Future of the New Economy and Society aims to close economic gender gaps through both in-country and global industry work. Through Closing the Gender Gap Accelerators, the Forum drives change by setting up action coalitions between relevant ministries and the largest employers in the country to increase female labour force participation, the number of women in leadership positions, closing wage gaps and preparing women for jobs of the future. Additionally, the global business commitment on Hardwiring Gender Parity in the Future of Work mobilizes businesses to commit to hiring 50% women for their five highest growth roles between now and 2022. Finally, the Forum has committed to at least double the current percentage of women participants at the Annual Meeting in Davos-Klosters, Switzerland, by 2030.
"To get to parity in the next decade instead of the next two centuries, we will need to mobilize resources, focus leadership attention and commit to targets across the public and private sectors. Business-as-usual will not close the gender gap – we must take action to achieve the virtuous cycle that parity creates in economies and societies," said Saadia Zahidi, Head of the Centre for the New Economy and Society and Member of the Managing Board, World Economic Forum.
The Global Gender Gap in 2020
Nordic countries continue to lead the way to gender parity. Iceland (87.7%) remains the world's most gender-equal country, followed by Norway (2nd, 84.2%), Finland (3rd, 83.2%) and Sweden (4th, 82.0%). Other economies in the top 10 include Nicaragua (5th, 80.4%), New Zealand (6th, 79.9%), Ireland (7th, 79.8%), Spain (8th, 79.5%), Rwanda (9th, 79.1%) and Germany (10th, 78.7%).
Among the countries that improve the most this year are Spain in Western Europe, Ethiopia in Africa, Mexico in Latin America, and Georgia in Eastern Europe and Central Asia. These countries all improved their positions in the ranking by more than 20 places, largely driven by improvements in the political empowerment dimension.
Western Europe is the best performing region for the 14th consecutive year. With an average score of 76.7% (out of 100), the region has now closed 77% of its gender gap, further improving from last edition. At the current pace, it will take 54 years to close the gap in Western Europe. The region is home to the four most gender-equal countries in the world, namely in order Iceland (87.7%), Norway (84.2%) and Finland (83.2%) and Sweden (82.0%), and one country (Spain, 8th) is among the most improved countries this year.
The North America region regroups the United States (72.4%, 53rd) and Canada (77.2%, 19th). Both countries' performances are stalling, especially in terms of economic participation and opportunity. At this rate it will take 151 years to close the gap.
The Eastern Europe and Central Asia region has closed 71.5% of its gender gap so far with a slight improvement since last year. To date the time to fully close its overall gender gap is estimated to be 107 years. The region has fully closed its educational gap and has improved women's political empowerment which however remains only closed at 15%. 21 of the 26 countries in this region have closed at least 70% and the top-ranked country, Latvia, 11th has closed 78.5% of its gap.
The Latin America and the Caribbean region has closed 72.1% of its gender gap so far, progressing 1 percentage points since last year. At this rate it will take 59 years to close the gender gap. The most noticeable improvement is on the Political empowerment dimension where the region closes its gap by 5 percentage points. Led my Nicaragua that has closed 80.4% of its gap (5th), 15 of the 24 countries covered by the report have improved their overall scores. Among the most improved countries, Mexico reduced its gender gap by 3.4 points on a year-over-year basis.
The Sub-Saharan Africa region has closed 68.0% of its gender gap so far. This result is a significant progress since last edition which leads to revise down the number of years it will take to close the gender gap, which is now estimated at 95 years. The region is home of one of the top-ten countries overall Rwanda (9th) while another 21 countries have improved their performances since last year, including Ethiopia (82nd) one of the best improved this year globally.
The East Asia and Pacific Region has closed 69% of the overall gender gap. If the region maintains the same rate of improvement as the 2006-2019 period, and given the current gap, it will take another 163 years to close the gender gap, the most time of any region. The region has improved on three of the four gender gap dimensions and has been the only region where political empowerment gap has widened (16% closed so far). The best performing country is New Zealand

6th, which has closed 79.9% of its gap. It is followed by the Philippines 16th with 78.1% closed and Lao PDR, 43rd with a score of 73.1%.
South Asia region has closed two thirds of its gender gap. The region's gender gap is the second largest despite a progress of 6 points over the past 14 years. If the rate of progress of the past 15 years was to continue it will take 71 years to close the region's gender gap. However, in contrast with the overall's performance, the region's Economic participation and opportunity gap widens this year. Bangladesh (50th) leads the region, while the second ranked country, Nepal, lags several positions behind (101th)
The Middle East and North Africa (MENA) region obtains the lowest score (61.1%) despite having narrowed its gap by 0.5 points since last year. Assuming the same rate of progress going forward it will take approximately 150 years to close the gender gap in the MENA region. The two most highly ranked countries in the region are Israel

(64th) with a closed gap to date of 71.8% and the United Arab Emirates (120th) with a score of 65.5%. 15 of the 19 countries in this region rank 130th or lower.
The post More Than A Lifetime Away: World Faces 100-Year Wait For Gender Parity appeared first on Eurasia Review.

Load-Date: December 16, 2019


End of Document


Music Is Universal
Eurasia Review
November 22, 2019 Friday


Copyright 2019 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 868 words
Body


Nearly 200 years ago, Henry Wadsworth Longfellow asserted "Music is the universal language of mankind." Today, scientists at Harvard have published the most comprehensive scientific study to determine if the American poet's words were mere cliché, or cultural truism.
The study was conceived by Samuel Mehr, a fellow of the Harvard Data Science Initiative and research associate in psychology, Manvir Singh, a graduate student in Harvard's department of Human Evolutionary Biology, and Luke Glowacki, formerly a Harvard graduate student and now a professor of anthropology at Pennsylvania State University.
They set out to answer big questions: Is music a cultural universal? If it is, which musical qualities overlap across disparate societies? If it isn't, why does it seem so ubiquitous?
To answer these questions, they needed a dataset of unprecedented breadth and depth. Over a five-year period, the team hunted down hundreds of recordings in libraries and private collections of scientists half a world away.
"We are so used to being able to find any piece of music that we like on the internet," said Mehr, who is now a principal investigator atHarvard's Music Lab. "But there are thousands and thousands of recordings buried in archives that are not accessible online. We didn't know what we would find: at one point we found an odd-looking call number, asked a Harvard librarian for help, and twenty minutes later she wheeled out a cart of about 20 cases of reel-to-reel recordings of traditional Celtic music."
Mehr and Singh added reel-to-reels, vinyl, cassette tapes, CDs, and digital recordings from anthropologists' and ethnomusicologists' private collections to the team's growing discography, combining it with a corpus of ethnography containing nearly 5,000 descriptions of songs from 60 human societies. Mehr, Singh, and Glowacki call this database The Natural History of Song.
Their questions were so compelling that the project rapidly grew into a major, international collaboration with musicians, data scientists, psychologists, linguists, and political scientists. Published inSciencethis week, it represents the team's most ambitious study yet about music.
One big answer: Music pervades social life in similar ways all around the world.
"As a graduate student, I was working on studies of infant music perception and I started to see all these studies that made claims about music being universal," Mehr said. "How is it that every paper on music starts out with this big claim but there's never a citation backing that up... Now we can back that up."
They looked at every society for which there was ethnographic information in a large online database, 315 in all, and found mention of music in all of them. For their own ethnographic portion, they collected around 5,000 descriptions of song from a subset of 60 cultures spanning 30 distinct geographic regions. For the discography, they collected 118 songs from a total of 86 cultures, again covering 30 geographic regions.
The team and their researchers coded the ethnography and discography that makes up the Natural History of Song into dozens of variables. They logged details about singers and audience members, the time of day, duration of singing, the presence of instruments, and more details for thousands of passages about songs in the ethnographic corpus. The discography was analyzed four different ways: machine summaries, listener ratings, expert annotations, expert transcriptions.
They found that, across societies, music is associated with behaviors such as infant care, healing, dance, and love (among many others, like mourning, warfare, processions and ritual), and that these behaviors are not terribly different from society to society. Examining lullabies, healing songs, dance songs, and love songs in particular, they discovered that songs that share behavioral functions tend to have similar musical features.
"Lullabies and dance songs are ubiquitous and they are also highly stereotyped," Singh said. "For me, dance songs and lullabies tend to define the space of what music can be. They do very different things with features that are almost the opposite of each other."
Definitely seeing music as cross-cultural excites Singh because he comes to the Natural History of Song project as a field anthropologist interested in cultural particularities and an evolutionary theorist interested in human universals. For him, the profound patterns of music demonstrate that human culture everywhere is built from common psychological building blocks.
For Mehr, who began his academic life in music education, the study looks toward unlocking the governing rules of "musical grammar." That idea has been percolating among music theorists, linguists, and psychologists of music for decades, but had never been demonstrated across cultures.
"In music theory, tonality is often assumed to be an invention of Western music, but our data raise the controversial possibility that this could be a universal feature of music," he said. "That raises pressing questions about structure that underlies music everywhere - and whether and how our minds are designed to make music."
The post Music Is Universal appeared first on Eurasia Review.

Load-Date: November 22, 2019


End of Document


Robots, Smart Helmets Deployed In Coronavirus Fight
Eurasia Review
March 20, 2020 Friday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 679 words
Body


Robots and artificial intelligence are being deployed across the globe in the fight against the coronavirus pandemic.
Robotics are being used to sanitise hospitals, some of which use ultraviolet to clean, in a bid to minimise health workers' exposure to the coronavirus.
In China, ground zero of the viral outbreak, robots are being used in hospitals to deliver food and medication and take patients' temperatures. Drones are deployed to transport supplies, spray disinfectants and do thermal imaging.
Police officers were also recently issued smart helmets with facial recognition technology and an infrared camera that automatically detects body temperature.
Beyond checking body temperature, artificial intelligence is being used to diagnose SARS-CoV-2. Infervision, software that automatically detects symptoms via CT scan images, can make diagnoses quicker and reduce the risk of human error.
"This system helps doctors save valuable time and increase accuracy of judgment," says Ming-Ming Cheng, a professor at Nankai University's College of Computer Science, in China.
In addition to engineering innovations, data science is contributing to the efforts to contain the COVID-19 pandemic.
Johns Hopkins University has a global map showing the number of confirmed COVID-19 cases across the world, based on official reports.
Healthmap, an initiative of organisations such as Harvard Medical School, Boston Children's Hospital and Northeastern University, takes a similar approach, although it also factors in social media posts when data mining.
Event Horizon features a mathematical model that predicts where the virus may spread based on international flight routes.
Animated map of confirmed COVID-19 cases spreading from 2020-01-12.. Image credit: Metropolitan/Wikimedia Commons, CC BY-SA 4.0.
"Data can help in managing population movement, and contact and detect and quickly isolate sources. Big data and information engineers have been playing an important role in this regard," Gong Ke, president of the World Federation of Engineering Organizations (WFEO), tells SciDev.Net.
Fears about the spreading coronavirus forced the inaugural World Engineering Day for Sustainable Development event, proposed by the WFEO, to be postponed.
Gong says engineers are working with scientists and doctors to develop more efficient methods for diagnosing the coronavirus, as well as in testing potential treatments.
"Solving the world's problems is an enormous collaborative undertaking involving both the public and private sectors and extending across disciplines, borders and demographics," says Kathy Renzetti, executive director of engineering advocacy groupDiscoverE, based in the United States.
The internet has made collaboration significantly easier among key players. Thesequencing done by Chinese scientists, which was made publicly available just weeks after the outbreak in Wuhan, is being studied by medical researchers and doctors around the globe, in a bid to develop COVID-19 vaccines and treatments.
Breakthrough genome sequencing carried out in Brazil was shared on virological.org, a forum for the analysis and interpretation of virus molecular evolution and epidemiology.
Other researchers have made their work publicly available via online databases, such asDrugVirus.info, a free platform that features information on existing compounds that may potentially be used for the treatment of COVID-19.
Drug repositioning, or drug repurposing, which seeks to find new therapeutic uses for existing drugs, is also being explored.
"It will take time to develop drugs against COVID-19. However, [the] drug repositioning approach will shorten it," explains Denis Kainov, associate professor at Norwegian University of Science and Technology's department of clinical and molecular medicine.
*Melanie Sison joined SciDev.Net in February 2018 and serves as the Digital and User Engagement Coordinator for its Asia & Pacific edition. This article was produced by SciDev.Net's Asia & Pacific desk.
The post Robots, Smart Helmets Deployed In Coronavirus Fight appeared first on Eurasia Review.

Load-Date: March 20, 2020


End of Document


Consider Workplace AI's Impact Before It's Too Late
Eurasia Review
February 13, 2020 Thursday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 829 words
Body


The consequences of workplace automation will likely impact just about every aspect of our lives, and scholars and policymakers need to start thinking about it far more broadly if they want to have a say in what the future looks like, according to a new paper co-authored by a Cornell University researcher.
"Mostly, people in our field wait until technology is implemented in a workplace to study it. And then we go in and say, 'How is work different?'" said Diane Bailey, the Geri Gay Professor of Communication in the College of Agriculture and Life Sciences. "But faced with a technology that has the potential to disrupt the landscape of work in such a universal way, immediately and simultaneously, we felt like we have to get in the barn before the horse leaves."
The paper, "Beyond Design and Use: How Scholars Should Study Intelligent Design Technologies," was published in December inInformation and Organization. Bailey co-authored the study with Stephen Barley, the Christian A. Felipe Professor of Technology Management in the College of Engineering at the University of California, Santa Barbara.
According to the paper, past examples of new technology suggest it will take longer than companies predict for workplaces to become fully transformed by AI, and some jobs might not be as easily replaceable as economists believe. This means researchers have more time to gain a deeper understanding of how workplace automation will affect society, in order to have more say in how it unfolds.
Fully understanding workplace automation, the researchers said, requires an interdisciplinary approach that considers everything from the power dynamics within tech companies to the design of our societal institutions. At Cornell, Bailey and Martin Wells, the Charles A. Alexander Professor of Statistical Sciences and chair of the Department of Statistics and Data Science, are heading a core team of nine other researchers from eight departments to follow this cross-disciplinary roadmap. The group is currently seeking funding to plan the creation of an institute to study AI and work.
In the paper, Bailey and Barley identified four factors scholars should study in order to assess AI's future impact: variation; power; ideology; and institutions.
"Maybe the reasons our neighborhoods work well is that so many of us are away from them during the day. We might have to rethink all of these things - and that's what the paper argues we should do."
Considering variety among jobs is important, Bailey said, because not all jobs - even in the same fields - are identical. Researchers generally use U.S. Department of Labor databases to predict how automation might affect certain job categories, but most studies don't consider differences in implementation, skills, tasks and work practices across organizations or locations.
Because designers and engineers don't function independently, power is another crucial factor, the paper said. Which AI technologies are pursued and how aggressively they're implemented depends on the dynamics within companies, as well as the priorities of the government entities that might fund or regulate those companies.
The ideology of design can provide insight into how technologists create new systems, Bailey said. According to the paper, the AI community often approaches design with its own culture, potentially emphasizing technical over social aspects. This could mean that some systems that are predicted to replace humans might still require them, though possibly in different roles.
"We have to understand how all of these market mechanisms operate if we're going to be savvy enough to work in that world and say, 'No, we want technology that looks like this' [or] 'Design something that operates this way,'" Bailey said. "We need to work backwards from some desired future that we want, to get the technologies that will help us get there."
Researchers also need to consider the potential impacts of automation - and the widespread unemployment it will likely bring - on our institutions, the paper said. For example, Bailey said, being home together all day - without the demands and concrete rewards of a paid job - could strain marriages and families. Roads, highways and transit systems that were designed to move people from home to work will need to be reconsidered.
"Maybe the reasons our neighborhoods work well is that so many of us are away from them during the day," Bailey said. "We might have to rethink all of these things - and that's what the paper argues we should do."
Researchers and policymakers also need to weigh the societal benefits of work, in order to make informed decisions about which jobs are worth saving.
"We have to think about what aspects of work have meaning and value to us," Bailey said. "We might decide, 'Maybe AI can do this better than a person, but we don't care, because we get some value out of it.'"
The post Consider Workplace AI's Impact Before It's Too Late appeared first on Eurasia Review.

Load-Date: February 13, 2020


End of Document


Irish And UK Research Helps To Unravel Secrets Behind Game Of Thrones
Eurasia Review
November 10, 2020 Tuesday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 679 words
Body


A researcher at University of Limerick in Ireland has played a key role in examining some of the secrets behind Game of Thrones.
What are the secrets behind one of the most successful fantasy series of all time? How has a story as complex as the one in George R.R. Martin's novels enthralled the world and how does it compare to other narratives?
Researchers from five universities across the UK and Ireland - including UL's Dr Padraig MacCarron - came together to unravel 'A Song of Ice and Fire', the books on which the TV series is based.
In a paper that has just been published by theProceedings of the National Academy of Sciences of the USA, the team of physicists, mathematicians and psychologists from Coventry, Warwick, Limerick, Cambridge and Oxford universities used data science and network theory to analyse the acclaimed book series by George R.R. Martin.
The study shows the way the interactions between the characters are arranged is similar to how humans maintain relationships and interact in the real world. Moreover, although important characters are famously killed off at random as the story is told, the underlying chronology is not at all so unpredictable, the research shows.
The team found that, despite over 2,000 named characters in 'A Song of Ice and Fire' and over 41,000 interactions between them, at chapter-by-chapter level these numbers average out to match what we can handle in real life. Even the most predominant characters - those who tell the story - average out to have only 150 others to keep track of. This is the same number that the average human brain has evolved to deal with.
While matching mathematical motifs might have been expected to lead to a rather narrow script, George R. R. Martin keeps the tale bubbling by making deaths appear random as the story unfolds. But, as the team show, when the chronological sequence is reconstructed the deaths are not random at all: rather, they reflect how common events are spread out for non-violent human activities in the real world.
"These books are known for unexpected twists, often in terms of the death of a major character, it is interesting to see how the author arranges the chapters in an order that makes this appear even more random than it would be if told chronologically," explained Dr Padraig MacCarron, a postdoctoral researcher at the Centre for Social Issues Research and Mathematics Applications Consortium for Science and Industry (MACSI) at UL.
"Social networks of the most connected characters, while seemingly extensive, mirrored the typical range of social networks that humans maintain. Furthermore, characters' social networks did not extend beyond the cognitive limit of social connections that humans are able to sustain.
"Although the time intervals between significant deaths in relation to the story's timeline may appear random, they are not told in chronological order. Re-arranging them in order of which they occur, they follow a pattern more commonly observed in reality," added Dr MacCarron.
'Game of Thrones' has invited all sorts of comparison to history and myth and the marriage of science and humanities in this paper opens new avenues to comparative literary studies. It shows, for example, that it is more akin to the Icelandic sagas than to mythological stories such as the Tain Bo Cuailnge or Beowulf. The trick in Game of Thrones, it seems, is to mix realism and unpredictability in a cognitively engaging manner.
"People largely make sense of the world through narratives, but we have no scientific understanding of what makes complex narratives relatable and comprehensible. The ideas underpinning this paper are steps towards answering this question," explained Professor Colm Connaughton, from the University of Warwick.
Fellow researcher Professor Robin Dunbar, from the University of Oxford, observed: "This study offers convincing evidence that good writers work very carefully within the psychological limits of the reader."
The article Irish And UK Research Helps To Unravel Secrets Behind Game Of Thrones appeared first on Eurasia Review.

Load-Date: November 11, 2020


End of Document


Spain Using Mobile Phone Data To Study Efficacy Of Lockdown On Spread Of COVID-19
Eurasia Review
April 14, 2020 Tuesday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 855 words
Body


A new CSIC [National Scientific Research Council] project uses computer science and data science techniques to observe how the lockdown measures taken to halt the spread of the disease COVID-19 are proving effective. The results will be key to improving social distancing strategies taken in future outbreaks of this disease and of others.
To carry out this research, a multi-disciplinary team with experts in computer science, demographics, physics and movement studies are analyzing anonymous and high resolution big data obtained from telephone operators and map servers. These data explain how mobility patterns and social contact have changed since the start of the lockdown.
How to lift the lockdown and when
Once all the data is gathered, the team simulates different scenarios and strategies for social distancing and helps with decision-making. The results are key both for deciding whether a stricter lockdown should be activated and to plan for the safe and effective lifting of the lockdown.
"We hope that the results serve to better understand the effects of the lockdown on the spread of the disease, but also help in decision-making related to the lifting of the measures, to see whether or not it is better to end the lockdown gradually," said Frederic Bartumeus.
"To achieve this goal the project includes several phases that are being carried out in parallel," said José Javier Ramasco. "Firstly, mobility is characterized, which is being coordinated by the IFISC based on the contribution from various data platforms: information, for example, from online social media and mobility patterns captured from mobile phone records. In this latter case, the data are collected by the operators and companies that are taking part in the project, which provide the research team with aggregated travel flows between different areas" specifies the researcher.
In no case is individual information accessed.
A second aspect is the change in conduct of people due to the perception of risk. The CEAB and IEGD are carrying out surveys and implementing mobile phone applications to quantify these changes, trying to estimate the adherence to personal protection measures by people and the changes in the amount and quality of personal contact.
"This information is crucial for understanding the contagion process," said  José Javier Ramasco.
Lastly, all these data form part of the computational models being developed by the IFISC and IFCA to study the different scenarios to exit the crisis.
"The lockdown has been widespread and relatively sudden, but to avoid new outbreaks it is necessary to use simulators capable of assessing scenarios with different rhythms to return to normality, both by sector and by geographic area," José Javier Ramasco said.
Epidemiology in the future
The project uses artificial intelligence tools and data science, and integrates big data in real time on human mobility, geo-localized surveys and computational models. This is a new way of undertaking epidemiology which combines computational epidemiology, digital demography and human mobility models.
"The study will take into account such important aspects as the spatial distribution of the population, their age structure, and the distribution and characteristics of social health centers (hospitals, local health centers, and care homes for the elderly). We can see how the contention measures have changed the mobility and conduct of people," said José Javier Ramasco.
The information and models to be developed during this research study will be made available to the public for their future use following an open data model under FAIR (Findable, Accessible, Interoperable, Reusable) principles.
A second long-term goal is to establish the basis for a computational epidemiology network in Spain, as in other countries, and a series of interoperable analytical tools based on epidemiological theories, data science and artificial intelligence, to report decisions to be taken in future situations of epidemiological crisis which, as the scientists say, is something that "has already happened on several occasions since 2009 and is likely to be recurrent in our globalized and interconnected world".
The project, pre-financed by the CSIC, thanks to the donation received from AENA, is coordinated by the scientists José Javier Ramasco, from the Institute of Complex Physics System (Spanish acronym: IFISC, a joint CSIC and University of Balearic Islands centre) and Frederic Bartumeus, from the Blanes Advanced Studies Centre (Spanish acronym: CEAB-CSIC) and the CREAF [Centre for Ecological Research and Forestry Applications]. It also involves the participation of teams from the Institute for Economy, Geography and Demography (Spanish acronym: IEGD-CSIC), from the Institute for Physics of Cantabria (Spanish acronym: IFCA-CSIC), from the National Biotechnology Centre (Spanish acronym: CNB-CSIC), as well as scientists from Pompeu Fabra University and the National Epidemiology Centre- Carlos III Health Institute (ISCIII).
The post Spain Using Mobile Phone Data To Study Efficacy Of Lockdown On Spread Of COVID-19 appeared first on Eurasia Review.

Load-Date: April 14, 2020


End of Document


Artificial Intelligence Can Predict Students' Educational Outcomes Based On Tweets
Eurasia Review
October 24, 2020 Saturday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 2100 words
Body


Ivan Smirnov, Leading Research Fellow of the Laboratory of Computational Social Sciences at the Institute of Education of HSE University, has created a computer model that can distinguish high academic achievers from lower ones based on their social media posts. The prediction model uses a mathematical textual analysis that registers users' vocabulary (its range and the semantic fields from which concepts are taken), characters and symbols, post length, and word length.
Every word has its own rating (a kind of IQ). Scientific and cultural topics, English words, and words and posts that are longer in length rank highly and serve as indicators of good academic performance. An abundance of emojis, words or whole phrases written in capital letters, and vocabulary related to horoscopes, driving, and military service indicate lower grades in school. At the same time, posts can be quite short--even tweets are quite informative. The study was supported by a grant from the Russian Science Foundation (RSF), and an article detailing the study's results was published in EPJ Data Science.
Smirnov's study used a representative sample of data from HSE University's longitudinal cohort panel study, 'Educational and Career Trajectories' (TrEC). The study traces the career paths of 4,400 students in 42 Russian regions from high schools participating in PISA (the Programme for International Students Assessment). The study data also includes data about the students' VK accounts (3,483 of the student participants consented to provide this information).
'Since this kind of data, in combination with digital traces, is difficult to obtain, it is almost never used,' Smirnov says. Meanwhile, this kind of dataset allows you to develop a reliable model that can be applied to other settings. And the results can be extrapolated to all other students--high school students and middle school students.
Posts from publicly viewable VK pages were used as a training sample--this included a total of 130,575 posts from 2,468 subjects who took the PISA test in 2012. The test allowed the researcher to assess a student's academic aptitude as well as their ability to apply their knowledge in practice. The study included only publicly visible VK posts from consenting participants.
When developing and testing the model from the PISA test, only students' reading scores were used an indicator of academic aptitude, although there are three tests in total: reading, mathematics, and science. PISA defines reading literacy as 'understanding, using, reflecting on and engaging with written texts in order to achieve one's goals, to develop one's knowledge and potential, and to participate in society.' The exam has six proficiency levels. Students who score a 2 are considered to meet only the basic, minimum level, while those who score a 5 or 6 are considered to be strong students.
In the study, unsupervised machine learning with word vector representations was performed on VK post corpus (totaling 1.9 billion words, with 2.5 million unique words). It was combined with a simpler supervised machine learning model that was trained in individual positions and taught to predict PISA scores.
'We represented each post as a 300-dimensional vector by averaging over vector representations of all its constituent words,' Smirnov writes. 'These post representations were used to train a linear regression model to predict the PISA scores of the posts' authors.'
By 'predict', the researcher does not refer to future forecasting, but rather the correlation between the calculated results and the real scores students earned on the PISA exam, as well as their USE scores (which are publicly available online in aggregated form--i.e., average scores per school). In the preliminary phase, the model learned how to predict the PISA data. In the final model, the calculations were checked against the USE results of high school graduates and university entrants.
The final model was supposed to be able to reliably recognize whether a strong student or a weak student had written a particular social media post, or in other words, differentiate the subjects according to their academic performance. After the training period, the model was able to distinguish posts written by students who scored highly or poorly on PISA (levels 5-6 and levels 0-1) with an accuracy of 93.7%. As for the comparability of PISA and the USE, although these two tests differ, studies have shown that students' scores for the two tests strongly correlate with each other.
'The model was trained using PISA data, and we looked at the correlation between the predicted and the real PISA scores (which are available in the TrEC study),' Smirnov explains. 'With the USE things gets more complicated: since the model does not know anything about the unified exams, it predicted the PISA scores as before. But if we assume that the USE and PISA measure the same thing -- academic performance -- then the higher the predicted PISA results are, the higher the USE results should be.' And the fact that the model learned to predict one thing and can predict another is quite interesting in itself, Smirnov notes.
However, this also needed to be verified, so the model was then applied to 914 Russian high schools (located in St. Petersburg, Samara and Tomsk; this set included almost 39,000 users who created 1.1 million posts) and one hundred of Russia's largest universities (115,800 people; 6.5 million posts) to measure the academic performance of students at these institutions.
It turned out that 'predicted academic performance is closely related to USE scores,' says Smirnov. 'The correlation coefficient is between 0.49 and 0.6. And in the case of universities, when the predicted academic performance and USE scores of applicants were compared (the information is available in HSE's ongoing University Admissions Quality Monitoring study), then the results also demonstrated a strong connection. The correlation coefficient is 0.83, which is significantly higher than for high schools, because there is more data.'
But can the model be applied to other social media sites? 'I checked what would happen if, instead of posts on VK, we gave the model tweets written by the same users,' Smirnov says. 'It turned out that the quality of the model does not significantly decrease.' But since a sufficient number of twitter accounts were available only for the university dataset (2,836), the analysis was performed only on this set.
It is important that the model worked successfully on datasets of different social media sites, such as VK and Twitter, thereby proving that is can be effective in different contexts. This means that it can be applied widely. In addition, the model can be used to predict very different characteristics, from student academic performance to income or depression.
Smirnov's study used a representative sample of data from HSE University's longitudinal cohort panel study, 'Educational and Career Trajectories' (TrEC). The study traces the career paths of 4,400 students in 42 Russian regions from high schools participating in PISA (the Programme for International Students Assessment). The study data also includes data about the students' VK accounts (3,483 of the student participants consented to provide this information).
'Since this kind of data, in combination with digital traces, is difficult to obtain, it is almost never used,' Smirnov says. Meanwhile, this kind of dataset allows you to develop a reliable model that can be applied to other settings. And the results can be extrapolated to all other students--high school students and middle school students.
Posts from publicly viewable VK pages were used as a training sample--this included a total of 130,575 posts from 2,468 subjects who took the PISA test in 2012. The test allowed the researcher to assess a student's academic aptitude as well as their ability to apply their knowledge in practice. The study included only publicly visible VK posts from consenting participants.
It is important that the scores on the standardized PISA and USE tests were used as an academic aptitude metric. This gives a more objective picture than assessment mechanisms that are school-specific (such as grades).
When developing and testing the model from the PISA test, only students' reading scores were used an indicator of academic aptitude, although there are three tests in total: reading, mathematics, and science. PISA defines reading literacy as 'understanding, using, reflecting on and engaging with written texts in order to achieve one's goals, to develop one's knowledge and potential, and to participate in society.' The exam has six proficiency levels. Students who score a 2 are considered to meet only the basic, minimum level, while those who score a 5 or 6 are considered to be strong students.
In the study, unsupervised machine learning with word vector representations was performed on VK post corpus (totaling 1.9 billion words, with 2.5 million unique words). It was combined with a simpler supervised machine learning model that was trained in individual positions and taught to predict PISA scores.
Word vector representations, or word embedding, is a numeric vector of a fixed size that describes some features of a word or their sequence. Embedding is often used for automated word processing. In Smirnov's research, the fastText system was used since it is particularly conducive to working with Russian-language text.
'We represented each post as a 300-dimensional vector by averaging over vector representations of all its constituent words,' Smirnov writes. 'These post representations were used to train a linear regression model to predict the PISA scores of the posts' authors.'
By 'predict', the researcher does not refer to future forecasting, but rather the correlation between the calculated results and the real scores students earned on the PISA exam, as well as their USE scores (which are publicly available online in aggregated form--i.e., average scores per school). In the preliminary phase, the model learned how to predict the PISA data. In the final model, the calculations were checked against the USE results of high school graduates and university entrants.
Results
First, Smirnov highlighted the general textual features of posts in relation to the academic performance of their authors (Fig. 1). The use of capitalized words (-0.08), emojis (-0.06), and exclamations (-0.04) were found to be negatively correlated with academic performance. The use of the Latin characters, average post and word length, vocabulary size, and entropy of users' texts on the other hand, were found to positively correlate with academic performance (from 0.07 to 0.16, respectively).
It was also confirmed that students with different levels of academic performance have different vocabulary ranges. Smirnov explored the resulting model by selecting 400 words with the highest and lowest scores that appear at least 5 times in the training corpus. Thematic clusters were identified and visualized (Fig. 2).
The clusters with the highest scores (in orange) include:
English words (above, saying, yours, must);Words related to literature (Bradbury, Fahrenheit, Orwell, Huxley, Faulkner, Nabokov, Brodsky, Camus, Mann);Concepts related to reading (read, publish, book, volume);Terms and names related to physics (Universe, quantum, theory, Einstein, Newton, Hawking);Words related to thought processes (thinking, memorizing).
Clusters with low scores (in green) include misspelled words, names of popular computer games, concepts related to military service (army, oath, etc.), horoscope terms (Aries, Sagittarius), and words related to driving and car accidents (collision, traffic police, wheels, tuning).
Smirnov calculated the coefficients for all 2.5 million words of the vector model and made them available for further study. Interestingly, even words that are rarely found in a training dataset can predict academic performance. For example, even if the name 'Newt' (as in the Harry Potter character, Newt Scamander) never appears in the training dataset, the model might assign a higher rating to posts that contain it. This will happen if the model learns that words from novel series are used by high-achieving students, and, through unsupervised learning, 'intuit' that that the name 'Newt' belongs to this category (that is, the word is closely situated to other concepts from Harry Potter in the vector space).
The article Artificial Intelligence Can Predict Students' Educational Outcomes Based On Tweets appeared first on Eurasia Review.

Load-Date: October 24, 2020


End of Document


Knowing The Model You Can Trust: The Key To Better Decision-Making
Eurasia Review
October 25, 2020 Sunday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 367 words
Body


As much of Europe is engulfed by a second wave of Covid-19, and track and trace struggles to meet demand, modelling support tools are being increasingly used by policymakers to make key decisions. Most notably, models have been used to predict the Covid-19 R0 rate - the average rate of secondary infections from a single infection, which has formed the basis for many lockdown decisions across the UK.
Models can represent the most effective tool for identifying interventions that can balance the risks of widespread infection and help assess socio-economic disruption until an effective treatment is established. However, not all models are equal, and differences in model predictions during the Covid-19 pandemic have caused confusion and suspicion.
A recent paper 'Three questions to ask before using model outputs for decision support' published inNature Communicationsaims to help decision makers choose the best available model for the problem at hand. The paper proposes three screening questions that can help critically evaluate models with respect to their purpose, organisation, and evidence, and enable more secure use of models for key decisions by policy makers.
One of the authors of the paper, Dr Alice Johnston, Lecturer in Environmental Data Science at Cranfield University, said: "From Covid-19 to the stock market, models are increasingly used by policymakers to support their decisions.
"However, different models are based on different assumptions and so can produce conflicting results, even when they represent the same system. Models used early on in the Covid-19 pandemic were a prime example of this, which led to confusion over which models to trust to support the decision-making process. This really highlights the need for clear communication of a model's context, so that policymakers have confidence in which models to trust.
"We propose that before engaging with a model, policymakers ask themselves three screening questions focusing on the model's purpose, organisation and evidence base, with the aim of bringing greater clarity to the decision-making process."
The article Knowing The Model You Can Trust: The Key To Better Decision-Making appeared first on Eurasia Review.

Load-Date: October 26, 2020


End of Document


Climate Signals Detected In Global Weather
Eurasia Review
January 2, 2020 Thursday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 765 words
Body


In October this year, weather researchers in Utah measured the lowest temperature ever recorded in the month of October in the US (excluding Alaska): -37.1°C. The previous low-temperature record for October was -35°C, and people wondered what had happened to climate change.
Until now, climate researchers have responded that climate is not the same thing as weather. Climate is what we expect in the long term, whereas weather is what we get in the short term - and since local weather conditions are highly variable, it can be very cold in one location for a short time despite long-term global warming. In short, the variability of local weather masks long-term trends in global climate.
A paradigm shift
Now, however, a group led by ETH professor Reto Knutti has conducted a new analysis of temperature measurements and models. The scientists concluded that the weather-is-not-climate paradigm is no longer applicable in that form. According to the researchers, the climate signal - that is, the long-term warming trend - can actually be discerned in daily weather data, such as surface air temperature and humidity, provided that global spatial patterns are taken into account.
In plain English, this means that - despite global warming - there may well be a record low temperature in October in the US. If it is simultaneously warmer than average in other regions, however, this deviation is almost completely eliminated. "Uncovering the climate change signal in daily weather conditions calls for a global perspective, not a regional one," says Sebastian Sippel, a postdoc working in Knutti's research group and lead author of a study recently published inNature Climate Change.
Statistical learning techniques extract climate change signature
In order to detect the climate signal in daily weather records, Sippel and his colleagues used statistical learning techniques to combine simulations with climate models and data from measuring stations. Statistical learning techniques can extract a "fingerprint" of climate change from the combination of temperatures of various regions and the ratio of expected warming and variability. By systematically evaluating the model simulations, they can identify the climate fingerprint in the global measurement data on any single day since spring 2012.
A comparison of the variability of local and global daily mean temperatures shows why the global perspective is important. Whereas locally measured daily mean temperatures can fluctuate widely (even after the seasonal cycle is removed), global daily mean values show a very narrow range.
If the distribution of global daily mean values from 1951 to 1980 are then compared with those from 2009 to 2018, the two distributions (bell curves) barely overlap. The climate signal is thus prominent in the global values but obscured in the local values, since the distribution of daily mean values overlaps quite considerably in the two periods.
Application to the hydrological cycle
The findings could have broad implications for climate science. "Weather at the global level carries important information about climate," says Knutti. "This information could, for example, be used for further studies that quantify changes in the probability of extreme weather events, such as regional cold spells. These studies are based on model calculations, and our approach could then provide a global context of the climate change fingerprint in observations made during regional cold spells of this kind. This gives rise to new opportunities for the communication of regional weather events against the backdrop of global warming."
The study stems from a collaboration between ETH researchers and the Swiss Data Science Center (SDSC), which ETH Zurich operates jointly with its sister university EPFL. "The current study underlines how useful data science methods are in clarifying environmental questions, and the SDSC is of great use in this," says Knutti.
Data science methods not only allow researchers to demonstrate the strength of the human "fingerprint", they also show where in the world climate change is particularly clear and recognisable at an early stage. This is very important in the hydrological cycle, where there are very large natural fluctuations from day to day and year to year.
"In future, we should therefore be able to pick out human-induced patterns and trends in other more complex measurement parameters, such as precipitation, that are hard to detect using traditional statistics," says the ETH professor.
The post Climate Signals Detected In Global Weather appeared first on Eurasia Review.

Load-Date: January 2, 2020


End of Document


Police Stop Fewer Black Drivers At Night When 'Veil Of Darkness' Obscures Their Race
Eurasia Review
May 6, 2020 Wednesday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 883 words
Body


The largest-ever study of alleged racial profiling during traffic stops has found that blacks, who are pulled over more frequently than whites by day, are much less likely to be stopped after sunset, when "a veil of darkness" masks their race.
That is one of several examples of systematic bias that emerged from a five-year study that analyzed 95 million traffic stop records, filed by officers with 21 state patrol agencies and 35 municipal police forces from 2011 to 2018.
The Stanford-led study also found that when drivers were pulled over, officers searched the cars of blacks and Hispanics more often than whites. The researchers also examined a subset of data from Washington and Colorado, two states that legalized marijuana, and found that while this change resulted in fewer searches overall, and thus fewer searches of blacks and Hispanics, minorities were still more likely than whites to have their cars searched after a pull-over.
"Our results indicate that police stops and search decisions suffer from persistent racial bias, and point to the value of policy interventions to mitigate these disparities," the researchers write in Nature Human Behaviour.
The paper culminates a five-year collaboration between Stanford's Cheryl Phillips, a journalism lecturer whose graduate students obtained the raw data through public records requests, and Sharad Goel, a professor of management science and engineering whose computer science team organized and analyzed the data.
Goel and his collaborators, which included Ravi Shroff, a professor of applied statistics at New York University, spent years culling through the data, eliminating records that were incomplete or from the wrong time periods, to create the 95 million-record database that was the basis for their analysis. "There is no way to overstate the difficulty of that task," Goel said.
Creating that database enabled the team to find the statistical evidence that a "veil of darkness" partially immunized blacks against traffic stops. That term and idea has been around since 2006 when it was used in a study that compared the race of 8,000 drivers in Oakland, California, who were stopped at any time of day or night over a six month period. But the findings from that study were inconclusive because the sample was too small to prove a link between the darkness of the sky and the race of the stopped drivers.
The Stanford team decided to repeat the analysis using the much larger dataset that they had gathered. First, they narrowed the range of variables they had to analyze by choosing a specific time of day - around 7 p.m. - when the probable causes for a stop were more or less constant. Next, they took advantage of the fact that, in the months before and after daylight saving time each year, the sky gets a little darker or lighter, day by day. Because they had such a massive database, the researchers were able to find 113,000 traffic stops, from all of the locations in their database, that occurred on those days, before or after clocks sprang forward or fell back, when the sky was growing darker or lighter at around 7 p.m. local time.
This dataset provided a statistically valid sample with two important variables - the race of the driver being stopped, and the darkness of the sky at around 7 p.m. The analysis left no doubt that the darker it got, the less likely it became that a black driver would be stopped. The reverse was true when the sky was lighter.
More than any single finding, the collaboration's most lasting impact may be from the Stanford Open Policing Project, which the researchers started to make their data available to investigative and data-savvy reporters, and to hold workshops to help reporters learn how to use the data to do local stories.
For example, the researchers helped reporters at the Seattle-based non-profit news organization, Investigate West, understand the patterns in the data for stories showing bias in police searches of Native Americans. That reporting prompted the Washington State Patrol to review its practices and boost officer training. Similarly, the researchers helped reporters at the Los Angeles Times analyze data that showed how police searched minority drivers far more often than whites. It resulted in a story that was part of a larger investigative series that prompted changes in Los Angeles Police Department practices.
"All told we've trained about 200 journalists, which is one of the unique things about this project," Phillips said.
Goel and Phillips plan to continue collaborating through a project called Big Local News that will explore how data science can shed light on public issues, such as civil asset forfeitures - instances in which law enforcement is authorized to seize and sell property associated with a crime. Gathering and analyzing records of when and where such seizures occur, to whom, and how such property is disposed will help shed light on how this practice is being used. Big Local News is also working on collaborative efforts to standardize information from police disciplinary cases.
"These projects demonstrate the power of combining data science with journalism to tell important stories," Goel said.
The post Police Stop Fewer Black Drivers At Night When 'Veil Of Darkness' Obscures Their Race appeared first on Eurasia Review.

Load-Date: May 6, 2020


End of Document


Model Beats Wall Street Analysts In Forecasting Business Financials
Eurasia Review
December 20, 2019 Friday


Copyright 2019 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 1237 words
Body


Knowing a company's true sales can help determine its value. Investors, for instance, often employ financial analysts to predict a company's upcoming earnings using various public data, computational tools, and their own intuition. Now MIT researchers have developed an automated model that significantly outperforms humans in predicting business sales using very limited, "noisy" data.
In finance, there's growing interest in using imprecise but frequently generated consumer data - called "alternative data" - to help predict a company's earnings for trading and investment purposes. Alternative data can comprise credit card purchases, location data from smartphones, or even satellite images showing how many cars are parked in a retailer's lot. Combining alternative data with more traditional but infrequent ground-truth financial data - such as quarterly earnings, press releases, and stock prices - can paint a clearer picture of a company's financial health on even a daily or weekly basis.
But, so far, it's been very difficult to get accurate, frequent estimates using alternative data. In a paper published this week in the Proceedings of ACM Sigmetrics Conference, the researchers describe a model for forecasting financials that uses only anonymized weekly credit card transactions and three-month earning reports.
Tasked with predicting quarterly earnings of more than 30 companies, the model outperformed the combined estimates of expert Wall Street analysts on 57 percent of predictions. Notably, the analysts had access to any available private or public data and other machine-learning models, while the researchers' model used a very small dataset of the two data types.
"Alternative data are these weird, proxy signals to help track the underlying financials of a company," says first author Michael Fleder, a postdoc in the Laboratory for Information and Decision Systems (LIDS). "We asked, 'Can you combine these noisy signals with quarterly numbers to estimate the true financials of a company at high frequencies?' Turns out the answer is yes."
The model could give an edge to investors, traders, or companies looking to frequently compare their sales with competitors. Beyond finance, the model could help social and political scientists, for example, to study aggregated, anonymous data on public behavior. "It'll be useful for anyone who wants to figure out what people are doing," Fleder says.
Joining Fleder on the paper is EECS Professor Devavrat Shah, who is the director of MIT's Statistics and Data Science Center, a member of the Laboratory for Information and Decision Systems, a principal investigator for the MIT Institute for Foundations of Data Science, and an adjunct professor at the Tata Institute of Fundamental Research.
Tackling the "small data" problem
For better or worse, a lot of consumer data is up for sale. Retailers, for instance, can buy credit card transactions or location data to see how many people are shopping at a competitor. Advertisers can use the data to see how their advertisements are impacting sales. But getting those answers still primarily relies on humans. No machine-learning model has been able to adequately crunch the numbers.
Counterintuitively, the problem is actually lack of data. Each financial input, such as a quarterly report or weekly credit card total, is only one number. Quarterly reports over two years total only eight data points. Credit card data for, say, every week over the same period is only roughly another 100 "noisy" data points, meaning they contain potentially uninterpretable information.
"We have a 'small data' problem," Fleder says. "You only get a tiny slice of what people are spending and you have to extrapolate and infer what's really going on from that fraction of data."
For their work, the researchers obtained consumer credit card transactions - at typically weekly and biweekly intervals - and quarterly reports for 34 retailers from 2015 to 2018 from a hedge fund. Across all companies, they gathered 306 quarters-worth of data in total.
Computing daily sales is fairly simple in concept. The model assumes a company's daily sales remain similar, only slightly decreasing or increasing from one day to the next. Mathematically, that means sales values for consecutive days are multiplied by some constant value plus some statistical noise value - which captures some of the inherent randomness in a company's sales. Tomorrow's sales, for instance, equal today's sales multiplied by, say, 0.998 or 1.01, plus the estimated number for noise.
If given accurate model parameters for the daily constant and noise level, a standard inference algorithm can calculate that equation to output an accurate forecast of daily sales. But the trick is calculating those parameters.
Untangling the numbers
That's where quarterly reports and probability techniques come in handy. In a simple world, a quarterly report could be divided by, say, 90 days to calculate the daily sales (implying sales are roughly constant day-to-day). In reality, sales vary from day to day. Also, including alternative data to help understand how sales vary over a quarter complicates matters: Apart from being noisy, purchased credit card data always consist of some indeterminate fraction of the total sales. All that makes it very difficult to know how exactly the credit card totals factor into the overall sales estimate.
"That requires a bit of untangling the numbers," Fleder says. "If we observe 1 percent of a company's weekly sales through credit card transactions, how do we know it's 1 percent? And, if the credit card data is noisy, how do you know how noisy it is? We don't have access to the ground truth for daily or weekly sales totals. But the quarterly aggregates help us reason about those totals."
To do so, the researchers use a variation of the standard inference algorithm, called Kalman filtering or Belief Propagation, which has been used in various technologies from space shuttles to smartphone GPS. Kalman filtering uses data measurements observed over time, containing noise inaccuracies, to generate a probability distribution for unknown variables over a designated timeframe. In the researchers' work, that means estimating the possible sales of a single day.
To train the model, the technique first breaks down quarterly sales into a set number of measured days, say 90 - allowing sales to vary day-to-day. Then, it matches the observed, noisy credit card data to unknown daily sales. Using the quarterly numbers and some extrapolation, it estimates the fraction of total sales the credit card data likely represents. Then, it calculates each day's fraction of observed sales, noise level, and an error estimate for how well it made its predictions.
The inference algorithm plugs all those values into the formula to predict daily sales totals. Then, it can sum those totals to get weekly, monthly, or quarterly numbers. Across all 34 companies, the model beat a consensus benchmark - which combines estimates of Wall Street analysts - on 57.2 percent of 306 quarterly predictions.
Next, the researchers are designing the model to analyze a combination of credit card transactions and other alternative data, such as location information. "This isn't all we can do. This is just a natural starting point," Fleder says.
The post Model Beats Wall Street Analysts In Forecasting Business Financials appeared first on Eurasia Review.

Load-Date: December 20, 2019


End of Document


March Madness Bracket Analysis Shows Picking Final Four First Leads To Better Brackets
Eurasia Review
March 5, 2020 Thursday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 619 words
Body


Data science researchers at the University of Illinois have some March Madness advice based on new research: Pick top-seeded teams as the Final Four in your March Madness bracket and work backward and forward from there. If you are going to submit multiple brackets-as you can in the ESPN, CBS Sports and Yahoo Challenges-starting with the Final Four is still a good strategy, but make sure you also diversify your brackets as much as possible.
A paper describing the research behind this advice is published in the American Statistical Association's (ASA)Journal of Quantitative Analysis in Sports(JQAS) by Sheldon H. Jacobson (computer science faculty), Ian Ludden (computer science graduate student), Arash Khatibi (former graduate student) and Douglas M. King (industrial and enterprise systems engineering faculty).
"If you can only pick one bracket, then leaning heavily on the top seeds makes sense," said Jacobson. "However, all bracket challenges allow you to submit multiple entries. A person does not need all of their brackets to score well; just one will do." Jacobson's research on basketball brackets over the past decade has focused entirely on seeds, not teams, making his body of seed-centered work distinct.
Given there are 2^63 possible brackets, which is more than 9 quintillion (9 x 10^18) combinations, picking a bracket with all 63 games correct is highly unlikely, even if you can submit multiple brackets. So, Jacobson suggests focusing on your Final Four teams first, and then building backward and forward from those games. "Once you pick a set of Final Four teams, 12 additional game outcomes become fixed, effectively reducing the number of games that you must pick," Jacobson said. "Our research suggests that anything that can be done to reduce the uncertainty in your picks, while simultaneously expanding the diversity of your pool, will give you a step up in having a good scoring bracket amongst your set of brackets." More information can be found on Jacobson's Bracket Odds website athttp://bracketodds.cs.illinois.edu/pool.html.
"For the 2016 through 2019 tournaments, our models produce many brackets that would have placed in the top 100 of the ESPN bracket challenge." Ludden said. "Our models that start by picking the Elite Eight or Final Four teams perform especially well, perhaps because they balance the two main risks: incorrect picks in the first two rounds, which may propagate through the tournament, and incorrect teams in the later rounds, where each game is worth more points."
Jacobson's seed-centered research has been integrated into the Bracket Odds website. Launched in 2012, the website-labeled as a University of Illinois STEM Learning Laboratory-draws together graduate and undergraduate students to apply data science methods to the tournament. The site has attracted more than 650,000 hits since its inception, providing insights and information for those interested in the mathematics of March Madness.
The website offers a smorgasbord of data analytics for people following the tournament. For example, one of the website calculators gives the probability of all number-one seeds reaching the Final Four to be 0.0155, or around once every 64 tournaments. Meanwhile, the probability of a Final Four comprised of only No. 16 seeds-the lowest-seeded teams in the tournament-is so small that it has a frequency of happening once every 13 trillion tournaments. (For perspective, if an entire tournament was played once every second, the lowest-seeded teams would only meet in the Final Four approximately once every 433,000 years.)
The post March Madness Bracket Analysis Shows Picking Final Four First Leads To Better Brackets appeared first on Eurasia Review.

Load-Date: March 5, 2020


End of Document


Influencing Electoral Outcomes: The Ugly Face Of Facebook - Analysis
Eurasia Review
March 27, 2018 Tuesday


Copyright 2018 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 1574 words
Body


By Munish Sharma*
Free and fair elections are the backbone of a democratic system of governance, and they are often celebrated as the "festival of democracy". Election campaigns of political parties and candidates employ a wide variety of strategies and tactics to influence voters. The digital era has added a whole new flavour, be it the eye-catching colossal digital campaigns or instances of foreign governments interfering in the electoral process. Last year, the Presidential elections in both the US and France were controversial due to hacking incidents and data leaks at the campaigns of the Democratic National Committee (DNC) and En Marche, respectively. In general, cyber means of intervention appear to be becoming an inevitable part of the electoral process. The Cambridge Analytica incident proves that India is no exception to this trend. During the next general elections in 2019, the Election Commission of India has an uphill task to thwart both external interference and the abuse of social media platforms to influence voter behaviour.
While there is a long history of external interference in elections both through covert and overt means, digital platforms add a new dimension. News and online content over digital platforms can spread at lightning speed, without paying heed to the credibility or authenticity of the source. Moreover, social media platforms generate vast amounts of data related to the socio-economic conditions, purchasing behaviour, interests, hobbies, and political inclinations or orientations of the users. These details are captured and treasured for commercial purposes. Business analytics feed on this data to generate business intelligence and derive monetary benefits for informed decision making. Present day electoral campaigns are also data driven and they are well-funded to let the campaigners harness data for their own political advantage. Data analytics tools can harvest data from user profiles and sift through the trove to support research, augment targeted campaigns and help political parties in assessing and evaluating their performance. These have been quite effective in targeting swing voters and behaviour forecasting.
As the popularity of social media platforms hits new heights, Facebook and Twitter in particular have been under the scanner of both intelligence agencies and election watchdogs. With close to 2.2 billion active users (by the end of 2017), Facebook alone sits on a stockpile of data which could be used to drive election campaigns towards any preferred outcome. Data in itself is worthless, but data science and the corresponding analytical tools turn it into a goldmine for both businesses and political strategists in the digital age.
Cambridge Analytica, the London-based political consultancy firm presently under the scanner, has an eight-year-old association with Indian elections. It undertook an in-depth electorate analysis for the Bihar Assembly Election in 2010 and, as per the case study details on its website, "the client (political party) achieved a landslide victory, with over 90 percent of total seats targeted by Cambridge Analytica being won."1 This was carried out through Ovleno Business Intelligence, which is an Indian affiliate of Cambridge Analytica's parent firm Strategic Communications Laboratories. The firm had hit media headlines for its association with Donald Trump's election campaign, which it has referred to as "A Full-Scale Data-Driven Digital Campaign". Bringing together the expertise of data scientists, researchers, strategists and content writers in three integrated teams (research, data science, and digital marketing), Cambridge Analytica's campaign helped Trump win the elections.2 The above case studies, mentioned in the Cambridge Analytica website, are prime examples of the vital role data science has begun to play especially in devising techniques to change voter behaviour in the targeted population or audience.
Facebook has played a central role in this entire episode. In a statement, Facebook has accepted that in 2015 a research app for psychologists with the name "thisisyourdigitallife", developed by a psychology professor at Cambridge University, was used for commercial purposes by Cambridge Analytica and other firms in violation of its platform policies. The app, meant for personality prediction, had around 270,000 downloads. Users revealed content related to their likes, preferences, and their own social circles according to their privacy settings.3 The access to Facebook content, in technical terms, was legitimate and through proper channels but the information was passed on to third parties likes Cambridge Analytica and Eunoia Technologies, which exploited it for commercial gains. However, Cambridge Analytica has outright denied allegations of using Facebook data as part of the services rendered to the Trump presidential campaign and while working on the Brexit referendum in the UK.4
As of January 2018, with 250 million users, India is the largest user-base for Facebook. It is also an important tool for the government to take forward its flagship programmes to the wider populace. Facebook is one of the top contenders for partnering with the government's societal development and digital inclusion plans. The Election Commission of India had also partnered with Facebook in 2017, launching a nationwide voter registration campaign.5 Indian users, paying little regard to the privacy terms and condition of social media platforms, uninhibitedly share images, pictures and other content, and are extremely vulnerable to the tools, techniques and campaigns devised for influencing both commercial and political behaviour. Against this backdrop, the government's concerns have been raised by Cambridge Analytica's alleged mining of data from the profiles of 50 million US Facebook users without their consent.6 If such an incident were to occur in India, it would constitute a serious violation of the IT Act. Not just in India, Cambridge Analytica is also at loggerheads with the Electoral Commission in the UK over its alleged role in the BREXIT vote and in Europe for violating EU privacy laws in collusion with Facebook.
Although Facebook has tendered an assurance of data security on its platform for the upcoming elections in India (2019) and Brazil (October 2018), the incident has caused severe damage to its reputation even as a development partner for governments in digital inclusion or other societal benefits plans. As the stakes in elections go up, political parties are unlikely to shy away from leveraging the technical expertise of data analytical firms like Cambridge Analytica fed with expansive data sets harvested from prominent social media platforms.
Data is being extensively harvested and harnessed for commercial purposes, targeted marketing campaigns and to influence consumer choices. It is ethically and legally controversial when information derived without the consent of the users or through dubious means is leveraged to influence political choices. Flourishing in the void of effective legal and regulatory regimes, such incidents seriously undermine the trust of people in the democratic process. To an extent, users understanding the perils of sharing unwanted details or content on social media platforms and aware of their privacy settings is pertinent for containing such instances of abuse. For India, as a functioning democracy, the Cambridge Analytica episode highlights the need to expedite the process of developing a data protection framework and probably amend the IT Act in accordance with the changing realities of cyberspace. The earlier this is realised, the better it would be for the healthy functioning of our democratic systems and processes.
Views expressed are of the author and do not necessarily reflect the views of the IDSA or of the Government of India.
About the author:

*Munish Sharma is Consultant at the Institute for Defence Studies and Analyses, New Delhi.
Source:

This article was published by IDSA.
Notes:
1. "Case Studies - India", Cambridge Analytica Political, available at https://ca-political.com /casestudies/casestudyindia, accessed 23 March, 2018.
2. "Case Studies – Donald J. Trump for President", Cambridge Analytica Political, available at            https://ca-political.com /casestudies, accessed 23 March, 2018.
3. Paul Grewal, "Suspending Cambridge Analytica and SCL Group from Facebook", Facebook Newsroom, March 16, 2018, available at            https://newsroom.fb.com /news/2018/03/suspending-cambridge-analytica/, accessed 25 March, 2018.
4. Cambridge Analytica, "Cambridge Analytica responds to false allegations in the media", available at            https://ca-commercial.com /news/cambridge-analytica-responds-false-allegations-media, accessed 25 March, 2018.
5. "Election Commission of India partners with Facebook to launch first nationwide voter registration reminder", Facebook, June 27, 2017, available at            https://www.facebook.com /notes/facebook/election-commission-of-india-partners-with-facebook-to-launch-first-nationwide-v/1672618862758040/, accessed 23 March, 2018.
6. Prashant Jha, "In eye of Facebook 'data breach' storm, Cambridge Analytica in talks with Cong, BJP for 2019", Hindustan Times, March 19, 2018, available at            https://www.hindustantimes.com /india-news/suspended-by-facebook-over-data-breach-cambridge-analytica-in-talks-with-congress-bjp-for-2019/story-g7J9rodV24lgCK7Xqu5eaO.html, accessed 23 March, 2018.

Load-Date: March 27, 2018


End of Document


How Climate Change Affects Crops In India
Eurasia Review
June 18, 2019 Tuesday


Copyright 2019 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 602 words
Body


Kyle Davis is an environmental data scientist whose research seeks to
increase food supplies in developing countries. He combines techniques 
from environmental science and data science to understand patterns in 
the global food system and develop strategies that make food-supply 
chains more nutritious and sustainable.
Since joining the Data Science Institute as a postdoctoral fellow in  September 2018, Davis has co-authored four papers, all of which detail  how developing countries can sustainably improve their crop production.  For his latest study, he focuses on India, home to 1.3 billion people,  where he led a team that studied the effects of climate on five major  crops: finger millet, maize, pearl millet, sorghum and rice.
These crops  make up the vast majority of grain production during the  June-to-September monsoon season - India's main growing period - with  rice contributing three-quarters of the grain supply for the season.  Taken together, the five grains are essential for meeting India's  nutritional needs.
And in a paper published in Environmental Research Letters,  Davis found that the yields from grains such as millet, sorghum, and  maize are more resilient to extreme weather; their yields vary  significantly less due to year-to-year changes in climate and generally  experience smaller declines during droughts. But yields from rice,  India's main crop, experience larger declines during extreme weather  conditions.
"By relying more and more on a single crop - rice - India's  food supply is potentially vulnerable to the effects of varying  climate," said Davis, the lead author on the paper, "Sensitivity of  Grain Yields to Historical Climate Sensitivity in India," which has four  co-authors, all of whom collaborated on the research.
"Expanding the area planted with these four alternative grains can 
reduce variations in Indian grain production caused by extreme climate, 
especially in the many places where their yields are comparable to 
rice," Davis added. "Doing so will mean that the food supply for the 
country's massive and growing population is less in jeopardy during 
times of drought or extreme weather."
Temperatures and rainfall amounts in India vary from year to year 
and influence the amount of crops that farmers can produce. And with 
episodes of extreme climate such as droughts and storms becoming more 
frequent, it's essential to find ways to protect India's crop production
from these shocks, according to Davis.
The authors combined historical data on crop yields, temperature,  and rainfall. Data on the yields of each crop came from state  agricultural ministries across India and covered 46 years (1966-2011)  and 593 of India's 707 districts.
The authors also used modelled data on  temperature (from the University of East Anglia's Climate Research  Unit) and precipitation (derived from a network of rain gauges  maintained by the Indian Meteorological Department). Using these climate  variables as predictors of yield, they then employed a linear mixed  effects modelling approach - similar to a multiple regression - to  estimate whether there was a significant relationship between  year-to-year variations in climate and crop yields.
"This study shows that diversifying the crops that a country grows 
can be an effective way to adapt its food-production systems to the 
growing influence of climate change," said Davis. "And it adds to the 
evidence that increasing the production of alternative grains in India 
can offer benefits for improving nutrition, for saving water, and for 
reducing energy demand and greenhouse gas emissions from agriculture."

Load-Date: June 18, 2019


End of Document


A COSMIC Approach To Nanoscale Science
Eurasia Review
March 6, 2021 Saturday


Copyright 2021 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 1887 words
Body


COSMIC, a multipurpose X-ray instrument at Lawrence Berkeley National Laboratory's (Berkeley Lab's) Advanced Light Source (ALS), has made headway in the scientific community since its launch less than 2 years ago, with groundbreaking contributions in fields ranging from batteries to biominerals.
COSMIC is the brightest X-ray beamline at the ALS, a synchrotron that generates intense light - from infrared to X-rays - and delivers it to dozens of beamlines to carry out a range of simultaneous science experiments. COSMIC's name is derived from coherent scattering and microscopy, which are two overarching X-ray techniques it is designed to carry out.
Its capabilities include world-leading soft X-ray microscopy resolution below 10 nanometers (billionths of a meter), extreme chemical sensitivity, ultrafast scanning speed as well as the ability to measure nanoscale chemical changes in samples in real time, and to facilitate the exploration of samples with a combination of X-ray and electron microscopy. Soft X-rays represent a low range in X-ray energies, while hard X-rays are higher in energy. Each type can address a different range of experiments.
COSMIC is setting the stage for a long-term project to upgrade the decades-old ALS. The effort, known as the ALS Upgrade (ALS-U), will replace most of the existing accelerator components with state-of-the-art technology, ensuring capabilities that will enable world-leading soft X-ray science for years to come. The upgrade will also further enhance COSMIC's ability to capture nanoscale details in the structure and chemistry of a broad range of samples.
The expected 100-fold increase in X-ray brightness that ALS-U will deliver will provide a similar increase in imaging speed at COSMIC, and a more than threefold improvement in imaging resolution, enabling microscopy with single-nanometer resolution. Further, the technologies being developed now at COSMIC will be deployed at other beamlines at the upgraded ALS, making possible microscopy with higher X-ray energies for many more experiments. The instrument is one of many highly specialized resources available to scientists from around the world for free through a peer-reviewed proposal process.
A journal article,published Dec. 16, 2020, inScience Advances, highlights some of COSMIC's existing capabilities and those that are on the way. The paper offers examples of 8-nanometer resolution achieved in imaging magnetic nanoparticles, the high-resolution chemical mapping of a battery cathode material during heating, and the high-resolution imaging of a frozen-hydrated yeast cell at COSMIC. (A cathode is one type of battery electrode, a component through which current flows.) These results serve as demonstration cases, revealing critical information about the structure and inner workings of these materials and opening the door for further insights across many fields of science.
Another journal article,published Jan. 19, 2021), inProceedings of the National Academy of Sciences, demonstrated the first-ever use of X-ray linear dichroic ptychography, a specialized high-resolution imaging technique available at COSMIC, to map the orientations of a crystal known as aragonite that is present in coral skeletons at 35-nanometer resolution. The technique shows promise for mapping other biomineral samples at high resolution and in 3D, which will provide new insights into their unique attributes and how to mimic and control them. Some biominerals have inspired humanmade materials and nanomaterials due to their strength, resilience, and other desirable properties.
"We use this user-friendly, unique platform for materials characterization to demonstrate world-leading spatial resolution, in conjunction with operando and cryogenic microscopy," said David Shapiro, the paper's lead author and the lead scientist for COSMIC's microscopy experiments. He also leads the ALS Microscopy Program. "Operando" describes the ability to measure changes in samples as they are occurring.
"There's no other instrument that has these capabilities co-located for X-ray microscopy at this resolution," Shapiro said. COSMIC can provide new clues to the nanoscale inner workings of materials, even as they actively function, that will lead to a deeper understanding and better designs - for batteries, catalysts, or biological materials. Equipping COSMIC with such a diversity of capabilities required an equally broad collaboration across scientific disciplines, he noted.
COSMIC contributors included members of Berkeley Lab's CAMERA (Center for Advanced Mathematics for Energy Research Applications) team, which includes computer scientists, software engineers, applied mathematicians, and others; information technology experts; detector specialists; engineers; scientists at the Molecular Foundry's National Center for Electron Microscopy; ALS scientists; and outside collaborators from the National Science Foundation's STROBE Science and Technology Center and Stanford University.
Several advanced technologies developed by different groups were integrated into this one instrument. Key to the demonstrations at COSMIC reported in the paper is the implementation of X-ray ptychography, which is a computer-aided image reconstruction technique that can exceed the resolution of conventional techniques by up to about 10 times.
With traditional methods, spatial resolution -the ability to distinguish tiny features in samples - is limited by the quality of the X-ray optics and their ability to focus the X-ray beam into a tiny spot. But conventional X-ray optics, which are the instruments used to manipulate X-ray light to see samples more clearly, are difficult to make, inefficient, and have short focal lengths.
Instead of relying on imperfect optics, ptychography records a large number of physically overlapping diffraction patterns - which are images produced as X-ray light scatters from the sample - each offering a small piece of the full picture. Rather than being limited by optics quality, the ptychography technique is limited by the brightness of the X-ray source - precisely the parameter that ALS-U is expected to improve a hundredfold. To capture and process the enormous amount of data and reconstruct the final image requires data processing facilities, computer algorithms, and specialized fast pixel detectors like those developed at Berkeley Lab.
"X-ray ptychography is a detector-enabled technique - first deployed with hard (high-energy) X-rays using hybrid pixel detectors, and then at the ALS with the FastCCD we developed," said Peter Denes, the ALS detector program lead who worked with lead engineer John Joseph on the implementation at COSMIC. "Much of the COSMIC technology benefited from the Laboratory Directed Research and Development (LDRD) Program, as did the FastCCD, which translated tools for cosmology into COSMIC observations." Berkeley Lab's LDRD Program supports innovative research activities that keep the Lab at the forefront of science and technology.
Ptychography utilizes a sequence of scattering patterns, produced as X-ray light scatters from a sample. These scattering patterns are analyzed by a computer running high-performance algorithms, which convert them into a high-resolution image.
In the Dec. 16, 2020, paper, researchers highlighted how ptychographic images made it possible to see the high-resolution chemical distribution in microscopic particles of a lithium iron phosphate battery cathode material (Li0.5FePo4). The ptychographic images showed nanoscale chemical features in the interior of the particles that were not visible using the conventional form of the imaging technique, called spectromicroscopy.
In a separate demonstration of ptychography at COSMIC, researchers noted chemical changes in a collection of LixFePO4 nanoparticles when subjected to heating.
Ptychography is also a source of COSMIC's heavy data demands. The beamline can produce several terabytes of data per day, or enough to fill a few laptop computers. The intensive computations required for COSMIC's imaging necessitate a dedicated cluster of GPUs (graphical processing units), which are specialized computer processors.
The ALS Upgrade will further drive its data demands up to an expected 100 terabytes per day, Shapiro noted. Plans are already being discussed for using more resources at Berkeley Lab's National Energy Research Scientific Computing Center (NERSC)to accommodate this pending ramp-up in data.
COSMIC is a stellar example of Berkeley Lab's Superfacility Project, which is designed to link light sources like the ALS and cutting-edge instrumentation including microscopes and telescopes with data and high-performance computing resources in real time, said Bjoern Enders, a data science workflows architect in NERSC's Data Science Engagement Group.
"We love data and computing challenges from instruments like COSMIC that venture beyond facility borders," Enders said. "We are working toward a future where it will be as easy as a button click to use NERSC's resources from a beamline." The addition of the new Perlmutter supercomputer at NERSC, he added, "will be an ideal partner for COSMIC in team science."
COSMIC started up in commissioning mode in March 2017, and opened to general scientific experiments about 2 years ago. Since this time, instrument staff have launched the operando capabilities that measure active chemical processes, for example, and rolled out linear and circular dichroic microscopy and tomography capabilities that further extend COSMIC's range of imaging experiments.
Its coherent scattering branch is now undergoing testing and is not yet available to external users. Work is also in progress to correlate its X-ray microscopy results with electron microscopy results for active processes, and to further develop its cryogenic capabilities, which will allow biological samples and other soft materials to be protected from damage by the ultrabright X-ray beam while they are being imaged. The combination of X-ray and electron microscopy can provide a powerful tool for gathering detailed chemical and structural information on samples, as demonstrated in an experiment involving COSMIC that was highlighted in the journalScience Advances.
Shapiro noted that there are plans to introduce a new experimental station to the beamline, timed with ALS-U, to accommodate more experiments.
One secret to COSMIC's success is that the instrument is designed for compatibility with standard sample-handling components. Shapiro said this user-friendly approach "has been really important for us," and makes it easier for researchers from academia and industry to design COSMIC-compatible experiments. "Users can just show up and plug (the samples) in. It increases our reach, scientifically," he said.
While COSMIC is loaded with features, it isn't bulky, and Shapiro described it as "streamlined in size and cost." He said he hopes it will be a model for future beamlines, both at ALS-U and at other synchrotron facilities.
"I think what is really attractive about it is that it is a very compact instrument. It is high-performance and very stable," he said. "It is very manageable and not very expensive. In that sense it should be very attractive for synchrotrons."
The article A COSMIC Approach To Nanoscale Science appeared first on Eurasia Review.

Load-Date: March 5, 2021


End of Document


Artificial Intelligence May Help Achieve UN's Sustainable Development Goals
Eurasia Review
February 20, 2021 Saturday


Copyright 2021 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 805 words
Body


Scientists from the Andalusian Research Institute in Data Science and Computational Intelligence, or DaSCI (University of Granada), together with the private company Ferrovial and the Spanish Royal Academy of Engineering, highlight the need for unified, accessible, and open data in developing projects to address many of the challenges of the UN's Sustainable Development Goals
Scientists from the Andalusian Research Institute in Data Science and Computational Intelligence, or DaSCI (University of Granada), together with the private company Ferrovial and the Spanish Royal Academy of Engineering (RAI), have conducted a study to analyse how engineering and technological solutions strongly linked to artificial intelligence (AI) can positively contribute to the 17 Sustainable Development Goals (SDGs) set by the United Nations (UN).
To protect the planet and ensure prosperity for all people, the UN established these 17 SDGs as part of its 2030 Agenda, which constitute a paradigm shift for companies and governments in the design of new business models and public policies based on sustainability. Governments, the private sector, and civil society all play an important role in achieving the goals.
The project, entitled "Engineering as a Facilitator of SDGs: Artificial intelligence and disruptive digital technologies", began in March 2020, focusing specifically on the study of AI and digital technologies and how these might be applied to further progress toward the 17 SDGs. The research is organised into three facets that broadly correspond to (i) an introduction to AI and digital technologies, (ii) analysis of their application to the SDGs, and (iii) recommendations for action that can help develop projects and support the achievement of associated goals. As part of this research, the authors reviewed the specialised scientific literature, including over a thousand bibliographical references relevant to the 169 targets that have been set to achieve the SDGs.
This work makes an important contribution to our understanding of the analytical capacity of engineering--under the umbrella of AI and digitalisation in support of the SDGs--and to addressing the challenges faced by the world economy and society in the 21st Century. It also provides insights into the three dimensions that characterise sustainability: the economic (including the different areas of life and economic and technological development); the social (including social development and equality); and the environmental (including resources and the environment).
One of the conclusions of this work is that data constitute the common element on which AI and digital technologies are based. Here, it is important to highlight the need for data that are unified, accessible, and open, as this supports the development of projects designed to address many of the global challenges. Governments and companies must converge toward this objective by generating and sharing data that allow them to successfully take on projects and design solutions to address the SDGs.
The researchers point out that it is imperative to strengthen the links between science and engineering, industry and governments, to reinforce dialogue and expand the different avenues toward achieving high-quality data.
Global targets
"The SDGs set targets to be achieved at the global level, but not all countries and regions of the world are currently in the same position in this race to achieve them. So the application of AI and digital technologies must obviously be adapted to the situation of each country and targeted at the most pressing SDGs," the authors explain.
Digital technologies are advancing at a rapid pace, which means it is important to look for alternative ways to measure the achievement of the SDGs--ways that are adapted to this accelerated pace of progress and the emergence of new digital paradigms. This is especially important given the current world scenario caused by the COVID-19 pandemic, which has undoubtedly had a profound impact on all dimensions of the SDGs, far beyond the strictly health-related aspect.
AI and digital technologies are fundamental tools for travelling the path we have to navigate during this decade, as we carry heavy moral and ethical responsibility toward today's world. Working toward the 17 SDGs is both a great opportunity and a major challenge.
A new book about the study, featuring original illustrations by Pablo García-Moral, is now available, which explores the latest thinking on this issue from those involved in the project. It was written by a team of 16 authors, coordinated by Rosana Montes (UGR), Francisco Herrera (UGR and RAI), Javier Pérez de Vargas (RAI), and Rosario Marchena (Ferrovial).
The article Artificial Intelligence May Help Achieve UN's Sustainable Development Goals appeared first on Eurasia Review.

Load-Date: February 19, 2021


End of Document


Study Claims Reparations For Slavery Could Have Reduced COVID-19 Infections And Deaths In US
Eurasia Review
February 12, 2021 Friday


Copyright 2021 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 2526 words
Body


New study suggests monetary reparations for Black descendants of people enslaved in the United States could have cut SARS-CoV-2 transmission and COVID-19 rates both among Black individuals and the population at large.
Researchers modeled the impact of structural racism on viral transmission and disease impact in the state of Louisiana.
The higher burden of SARS-CoV-2 infection among Black people also amplified the virus's spread in the wider population.
Reparations could have reduced SARS-CoV-2 transmission in the overall population by as much as 68 percent.
Compared with white people, Black individuals in the United States are more likely to be infected with SARS-CoV-2, more likely to end up in the hospital with COVID-19, and more likely to die from the disease.
Civil rights activists have long called for monetary reparations to the Black descendants of Africans enslaved in the United States as a financial, moral, and ethical form of restitution for the injustices of slavery.
Now, a study led by Harvard Medical School researchers suggests reparations could also have surprising public health benefits for Black individuals and the entire nation.
To estimate the impact of structural inequities between Black and white individuals, the researchers set out to capture the effect of reparation payments on the Black-white wealth gap in the state of Louisiana.
Their analysis, published online inSocial Science & Medicine, suggests that if reparations had been made before the COVID-19 pandemic, transmission of SARS-CoV-2 in the state's overall population could have been reduced by anywhere from 31 percent to 68 percent.
The work was done in collaboration with the Lancet Commission on Reparations and Redistributive Justice.
"While there are compelling moral and historical arguments for racial-injustice interventions such as reparations, our study demonstrates that repairing the damage caused by the legacy of slavery and Jim Crow racism would have enormous benefits to the entire population of the United States," said study senior author Eugene Richardson, assistant professor of global health and social medicine in the Blavatnik Institute at Harvard Medical School.
The disproportionate effects of COVID-19 on racial minorities--Black individuals in particular--have been well documented. Black people get COVID-19 at a rate nearly one and a half times higher than that of white people, are hospitalized at a rate nearly four times higher, and are three times as likely to die from the disease, according to the latest estimates from the U.S. Centers for Disease Control.
The greater disease burden among Black people has caused tremendous loss of life and unspeakable suffering across these already vulnerable and disadvantaged communities. Notably, these effects have also spilled over and are driving transmission rates of the virus in the overall population, the study authors said.
Addressing the structural inequalities at the roots of this disparity through monetary reparations would not only radically decrease the impact of COVID-19 among the people who received reparations, the authors said, but would reduce the overall toll of the disease on a broader scale, benefiting the entire population.
The findings, the researchers said, powerfully underscores the truly global nature of the pandemic and the notion that a society is only as strong as its most vulnerable members.
"If we extrapolate these results to the entire United States, we can imagine that tens or hundreds of thousands of lives would have been spared, and the entire nation would have been saved much of the hardship it has endured in the last year," said Richardson, who is also the chair of the Lancet Commission on Reparations and Redistributive Justice.
For their analysis, the researchers paired sophisticated data analytics and computational tools with commonly used epidemiologic modeling methods to calculate the impact of structural racism on infection rates among Black and white populations in Louisiana. They chose Louisiana as an exemplar of the impacts of structural racism in the U.S. because it was one of the few states that reported infection rates by race in the early stages of the pandemic. For a control group, the researchers chose the relatively egalitarian population of South Korea.
The researchers noted that although modeling is used to understand many factors in the spread of an infectious disease, such as differences in infection risk based on whether passengers on a train sit with windows open or closed or individual variations in mask-wearing habits, it has rarely been used to capture the effects of social factors that can create vast disparities between populations, such as those seen between Blacks and whites in the U.S.
Richardson's recent book Epidemic Illusions explores the ways conventional epidemiology is constrained from proposing solutions that address the root causes of health disparities derived from the combined weight of centuries of racism, imperialism, neoliberal politics, and economic exploitation. One of the goals of the paper is to challenge the narrow ways people who work in medicine and public health measure and think about problems and solutions and to broaden the public imagination, thus opening new conversations about what challenges and opportunities are worth considering in global health and social science, Richardson said.
The study examined the initial period of the outbreak, before infection control measures were implemented, so any differences in infection rates between populations at that time would have been driven mainly by differences in the social structures, the researchers said.
For example, Louisiana has a population heavily segregated by race, with Black people having higher levels of overcrowded housing and working jobs that are more likely to expose them to SARS-CoV-2 than white people. In comparison, South Korea has a more homogenous population with far less segregation.
To probe how such structural inequities impact transmission of SARS-CoV-2, the researchers examined infection rates over time for the first two months of the epidemic in each location. During the initial phase of the outbreak in Louisiana, each infected person spread the virus to1.3 to 2.5 more people than an infected individual during the same phase of the outbreak in South Korea, the analysis showed. The study also showed it took Louisiana more than twice as long to bring the early wave of the epidemic under control as South Korea.
Next, the researchers used next-generation matrices to gauge how overcrowding, segregation, and the wealth gap between Blacks and whites in Louisiana could have driven higher infection rates and how monetary reparations would affect viral transmission.
The model showed that greater equity between Blacks and whites might have reduced infection transmission rates by anywhere from 31 percent to 68 percent for every person in the state.
This research comes at a time when many Americans are already thinking about the larger societal costs of structural racism, the researchers said. They noted, for example, that the nationwide movement to protest police brutality against Black people has been fueled by many of the inequitable outcomes exemplified so painfully by the coronavirus pandemic in the U.S.
"This moment has made it possible for a lot of people who had no reason to think about these inequalities to be very aware of them," said study co-author and Lancet reparations commissioner Kirsten Mullen, who was a member of concept development team for the National Museum of African American History and Culture.
Anti-racism in action
Richardson said that the research was designed to explore how reparations payments might have altered the trajectory of the coronavirus pandemic in the U.S. and how a different response to the disease could have helped mitigate the disparities fueled by social conditions that are vestiges of slavery. Such conditions, Richardson noted, include ongoing discrimination and structural racism in the form of redlining, overcrowding, over-incarceration, and the heightened use of lethal force in policing experienced by Black people.
Richardson said that historian and anti-racist scholar Ibram X. Kendi's description of the differences between racism and anti-racism were helpful in designing the study. According to Kendi, a racist policy is any policy that produces or sustains inequality or promotes the power of one racial group over another, whereas an anti-racist policy is any measure that produces or sustains equity between racial groups.
Richardson said that one important goal of the project was to attempt to harness the power of mathematical modeling for an anti-racist response to the coronavirus and beyond.
"When you look at a formula for transmissibility, it looks like an objective calculation," he said. "But where is lethal policing in that formula?"
Richardson noted that it was important to call attention to the systemic and structural elements of racism that can get lost in simplified models of disease.
What are reparations?
Mullen and study co-author William Darity, who recently published a book on reparations and have written in the press about the case for using reparation payments to fight COVID-19, defined reparations as a program of acknowledgement, redress, and closure for a grievous injustice. In this case, Mullen said, the atrocities are associated with periods of enslavement, legal segregation and white terrorism during the Jim Crow era, and racial strife and violence of the post-Civil Rights Act era, including ongoing inequities in the form of over-policing, police executions of unarmed Black people, ongoing discrimination in regard to incarceration, access to housing, and, possibly most important, the Black-white gulf in wealth.
Successful reparations programs include three elements: admission of culpability on behalf of the perpetrators of the atrocity; redress, in the form of an act of restitution; and closure, wherein the victims agree that the debt is paid and no further claims are to be made unless new harms are inflicted.
In this case, Mullen said, reparations would take the form of financial restitution for living Black individuals who can show that they are descended from at least one ancestor who was enslaved in the U.S. and that they self-identified as Black on a legal document at some point during the 12 years prior.
The financial restitution is designed to help close the Black-white wealth gap. Darity noted that it is important to distinguish wealth from income. Wealth is how much you own, and income is how much you earn. Greater wealth translates to greater stability for individuals and families across time. Greater wealth is also more strongly associated with greater well-being than greater income, Darity said, and disparities in wealth manifest as health disparities.
"Wealth is more strongly associated with familial or individual well-being," said Darity, who is the Samuel DuBois Cook Distinguished Professor of Public Policy at Duke University and a Lancet reparations commissioner. He noted that, according to the Federal Reserve Board 2016 Survey of Consumer Finances, the average Black household had a net worth $800,000 lower than the average white household, and that Black people, who represent 13 percent of the U.S. population, only own 3 percent of the nation's wealth.
"This dramatically restricts the ability of Black Americans to survive and thrive," Darity said.
To assess the effect of reparation payments on the trajectory of the pandemic, the researchers based their calculations on a model that would pay $250,000 per person or $800,000 per household to descendants of enslaved individuals--one of several proposed reparation models.
Every transmission is a social transmission
"Every transmission has a social cause," said study co-author and Lancet reparations commissioner James Jones, associate professor of Earth System Science and a senior fellow at the Woods Institute for the Environment at Stanford University.
For a brief moment when AIDS was in the spotlight during the late 80s and early 90s, people interested in social behavior became interested in mathematical modeling of disease, Jones said. While that interest largely waned, the COVID-19 crisis has highlighted the need to think about social science, inequality, social structure, behavior patterns, and behavior change, as well as how they fit together with how we understand and respond to epidemics, Jones said.
Even the simplest model must account for a rudimentary social structure, Jones said. At its most basic, this can be represented with a generalized estimate of how likely an infected person is to come into contact with a susceptible person. He explained that this number, R0 or "R-naught," is the average number of people an infected individual transmits the virus to. When R0 is less than one, no epidemic is possible because the number of people infected decreases. When R0 is greater than 1 an epidemic is possible. R0 also determines the total number of people who could potentially become infected or how many people would need to be vaccinated to end the epidemic. It can also be used to calculate the so-called endemic equilibrium--which determines whether a disease will continue to exist within a population, simmering constantly in the background or bubbling up seasonally, like influenza.
"That's the theory of infectious disease control in a single parameter," Jones said.
That seeming simplicity can make it hard to focus on the complex ways that infectious diseases move through the real world, the researchers said.
"It's important to highlight that R0 is not simply a function of the pathogen," Jones said. "It's a function of the society." Social and environmental factors like mobility, segregation, and the nature of the built environment help determine rates of infection, he said.
This is one important reason that diseases don't hit all people the same. Global R0 is an average of very different R0s for different groups of people. Some groups are more likely to interact only with members of their own group, some groups are more likely to come in contact with infected people, and some are more susceptible to the disease for other reasons, Jones said.
In this case, the researchers used mathematical models to help understand the differences in R0 for Black people and white people in Louisiana and to help think about how things would change if racism were less prevalent in America.
Absent those interventions, the researchers noted that Black Americans remain at an elevated and inequitable risk of becoming infected and dying during the COVID-19 pandemic and that this inequity will continue to fuel the pandemic for all Americans.
"Increasing equality would have huge benefits on infection rates for everyone," said co-author Momin Malik, who was a data science postdoctoral fellow at the Berkman Klein Center for Internet & Society at Harvard University at the time the study was conducted.
The article Study Claims Reparations For Slavery Could Have Reduced COVID-19 Infections And Deaths In US appeared first on Eurasia Review.

Load-Date: February 12, 2021


End of Document


Global Ice Loss Increases At Record Rate
Eurasia Review
January 26, 2021 Tuesday


Copyright 2021 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 862 words
Body


The rate at which ice is disappearing across the planet is speeding up, according to new research.
And the findings also reveal that the Earth lost 28 trillion tonnes of ice between 1994 and 2017 - equivalent to a sheet of ice 100 metres thick covering the whole of the UK.
The figures have been published by a research team which is the first to carry out a survey of global ice loss using satellite data.
The team, led by the University of Leeds, found that the rate of ice loss from the Earth has increased markedly within the past three decades, from 0.8 trillion tons per year in the 1990s to 1.3 trillion tons per year by 2017.
Ice melt across the globe raises sea levels, increases the risk of flooding to coastal communities, and threatens to wipe out natural habitats which wildlife depend on.
The findings of the research team, which includes the University of Edinburgh, University College London and data science specialists Earthwave, are published in European Geosciences Union's journalThe Cryosphere.
The research, funded by UK Natural Environment Research Council, shows that overall, there has been a 65 % increase in the rate of ice loss over the 23-year survey. This has been mainly driven by steep rises in losses from the polar ice sheets in Antarctica and Greenland.
Lead author Dr Thomas Slater, a Research Fellow at Leeds' Centre for Polar Observation and Modelling , said: "Although every region we studied lost ice, losses from the Antarctic and Greenland ice sheets have accelerated the most.
"The ice sheets are now following the worst-case climate warming scenarios set out by the Intergovernmental Panel on Climate Change. Sea-level rise on this scale will have very serious impacts on coastal communities this century."
Dr Slater said the study was the first of its kind to examine all the ice that is disappearing on Earth, using satellite observations .
He added: "Over the past three decades there's been a huge international effort to understand what's happening to individual components in Earth's ice system, revolutionised by satellites which allow us to routinely monitor the vast and inhospitable regions where ice can be found.
"Our study is the first to combine these efforts and look at all the ice that is being lost from the entire planet."
The increase in ice loss has been triggered by warming of the atmosphere and oceans, which have warmed by 0.26°C and 0.12°C per decade since the 1980, respectively. The majority of all ice loss was driven by atmospheric melting (68 %), with the remaining losses (32%) being driven by oceanic melting.
The survey covers 215,000 mountain glaciers spread around the planet, the polar ice sheets in Greenland and Antarctica, the ice shelves floating around Antarctica, and sea ice drifting in the Arctic and Southern Oceans.
Rising atmospheric temperatures have been the main driver of the decline in Arctic sea ice and mountain glaciers across the globe, while rising ocean temperatures have increased the melting of the Antarctic ice sheet. For the Greenland ice sheet and Antarctic ice shelves, ice losses have been triggered by a combination of rising ocean and atmospheric temperatures.
During the survey period, every category lost ice, but the biggest losses were from Arctic Sea ice (7.6 trillion tons) and Antarctic ice shelves (6.5 trillion tons), both of which float on the polar oceans.
Dr Isobel Lawrence, a Research Fellow at Leeds' Centre for Polar Observation and Modelling, said: "Sea ice loss doesn't contribute directly to sea level rise but it does have an indirect influence. One of the key roles of Arctic sea ice is to reflect solar radiation back into space which helps keep the Arctic cool.
"As the sea ice shrinks, more solar energy is being absorbed by the oceans and atmosphere, causing the Arctic to warm faster than anywhere else on the planet.
"Not only is this speeding up sea ice melt, it's also exacerbating the melting of glaciers and ice sheets which causes sea levels to rise."
Half of all losses were from ice on land - including 6.1 trillion tons from mountain glaciers, 3.8 trillion tons from the Greenland ice sheet, and 2.5 trillion tons from the Antarctic ice sheet. These losses have raised global sea levels by 35 millimetres.
It is estimated that for every centimetre of sea level rise, approximately a million people are in danger of being displaced from low-lying homelands.
Despite storing only 1 % of the Earth's total ice volume, glaciers have contributed to almost a quarter of the global ice losses over the study period, with all glacier regions around the world losing ice.
Report co-author and PhD researcher Inès Otosaka, also from Leeds' Centre for Polar Observation and Modelling, said: "As well as contributing to global mean sea level rise, mountain glaciers are also critical as a freshwater resource for local communities.
"The retreat of glaciers around the world is therefore of crucial importance at both local and global scales."
Just over half (58 %) of the ice loss was from the northern hemisphere, and the remainder (42 %) was from the southern hemisphere.
The article Global Ice Loss Increases At Record Rate appeared first on Eurasia Review.

Load-Date: January 26, 2021


End of Document


Forecasting Coastal Water Quality
Eurasia Review
January 24, 2021 Sunday


Copyright 2021 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 806 words
Body


Less than two days of water quality sampling at local beaches may be all that's needed to reduce illnesses among millions of beachgoers every year due to contaminated water, according to new Stanford research.
The study, published inEnvironmental Science & Technology, presents a modeling framework that dependably predicts water quality at beaches after only a day or two of frequent water sampling. The approach, tested in California, could be used to keep tabs on otherwise unmonitored coastal areas, which is key to protecting the well-being of beachgoers and thriving ocean economies worldwide.
"This work combines knowledge of microbiology, coastal processes and data science to produce a tool to effectively manage one of our most precious resources and protect human health," said senior author Alexandria Boehm, a Stanford professor of civil and environmental engineering.
Measuring concentrations of fecal indicator bacteria (FIB) - which denote the presence of fecal matter and can lead to unsafe water conditions - at beaches ensures the health and safety of the public. While all ocean water contains some degree of pathogens, such as bacteria or viruses, they're typically diluted to harmless concentrations. However, changes in rainfall, water temperature, wind, runoff, boating waste, storm sewer overflow, proximity to waste treatment plants, animals and waterfowl can lead to an influx of water contamination. Exposure to these contaminants can cause many ailments, including respiratory diseases and gastrointestinal illnesses, along with skin, eye and ear infections to swimmers.
Protecting coastal waters and the people that use them remains essential for much of California's 840 miles of coastline. Over 150 million people swim, surf, dive and play at one of the state's 450 beaches annually, generating over $10 billion in revenue. According to the California State Water Resources Control Board, health agencies across 17 counties, publicly owned sewage treatment plants, environmental groups and several citizen-science groups perform water sampling across the state. However, not all waters are routinely checked due to accessibility issues, budget resource constraints or the season, despite their use by the public.
Another obstacle to safeguarding public health lies in the lag time between sampling and results - up to two-days - leading beach managers to make decisions reflecting past water quality conditions. When monitored waters contain high levels of bacteria and pose a health risk, beach managers post warning signs or close beaches. The delay in current testing methods could unknowingly expose swimmers to unhealthy waters.
To overcome these limitations, the researchers combined water sampling and environmental data with machine learning methods to accurately forecast water quality. While predictive water quality models aren't new, they have generally required historical data spanning several years to be developed.
The team used water samples collected at 10-minute intervals over a relatively brief timeframe of one to two days at beaches in Santa Cruz, Monterey and Huntington Beach. Among the three sites, 244 samples were measured for FIB concentrations and marked as above or below the acceptable level deemed safe by the state. The researchers then collected meteorological data such as air temperature, solar radiation and wind speed along with oceanographic data including tide level, wave heights and water temperature (all factors influencing FIB concentrations) over the same timeframe.
Using the high-frequency water quality data and machine learning methods, they trained computer models to accurately predict FIB concentrations at all three beaches. The researchers found hourly water sampling for 24 hours straight - capturing an entire tidal and solar cycle - proved enough for reliable results. Feeding the framework meteorological and tidal data from longer time periods resulted in future water quality predictions that were dependable for at least an entire season.
"These results are really empowering for communities who want to know what's going on with water quality at their beach," Searcy said. "With some resources to get started and a day of sampling, these communities could collect the data needed to initiate their own water quality modeling systems."
The framework code, which is publicly accessible, could also be developed for accurate predictions of other contaminants such as harmful algae, metals and nutrients known to wreak havoc on local waters. The researchers point out that more analysis is needed to better determine the exact timeframe these models remain accurate and note that continually assessing and retraining the models remains a best practice for accurate predictions.
The article Forecasting Coastal Water Quality appeared first on Eurasia Review.

Load-Date: January 24, 2021


End of Document


Climate Change To Alter Position Of Earth's Tropical Rain Belt
Eurasia Review
January 19, 2021 Tuesday


Copyright 2021 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 651 words
Body


Future climate change will cause a regionally uneven shifting of the tropical rain belt - a narrow band of heavy precipitation near the equator - according to researchers at the University of California, Irvine and other institutions. This development may threaten food security for billions of people.
In a study published in Nature Climate Change, the interdisciplinary team of environmental engineers, Earth system scientists and data science experts stressed that not all parts of the tropics will be affected equally. For instance, the rain belt will move north in parts of the Eastern Hemisphere but will move south in areas in the Western Hemisphere.
According to the study, a northward shift of the tropical rain belt over the eastern Africa and the Indian Ocean will result in future increases of drought stress in southeastern Africa and Madagascar, in addition to intensified flooding in southern India. A southward creeping of the rain belt over the eastern Pacific Ocean and Atlantic Ocean will cause greater drought stress in Central America.
"Our work shows that climate change will cause the position of Earth's tropical rain belt to move in opposite directions in two longitudinal sectors that cover almost two thirds of the globe, a process that will have cascading effects on water availability and food production around the world," said lead author Antonios Mamalakis, who recently received a Ph.D. in civil & environmental engineering in the Henry Samueli School of Engineering at UCI and is currently a postdoctoral fellow in the Department of Atmospheric Science at Colorado State University.
The team made the assessment by examining computer simulations from 27 state-of-the-art climate models and measuring the tropical rain belt's response to a future scenario in which greenhouse gas emissions continue to rise through the end of the current century.
Mamalakis said the sweeping shift detected in his work was disguised in previous modelling studies that provided a global average of the influence of climate change on the tropical rain belt. Only by isolating the response in the Eastern and Western Hemisphere zones was his team able to highlight the drastic alterations to come over future decades.
Co-author James Randerson, UCI's Ralph J. & Carol M. Cicerone Chair in Earth System Science, explained that climate change causes the atmosphere to heat up by different amounts over Asia and the North Atlantic Ocean.
"In Asia, projected reductions in aerosol emissions, glacier melting in the Himalayas and loss of snow cover in northern areas brought on by climate change will cause the atmosphere to heat up faster than in other regions," he said. "We know that the rain belt shifts toward this heating, and that its northward movement in the Eastern Hemisphere is consistent with these expected impacts of climate change."
He added that the weakening of the Gulf Stream current and deep-water formation in the North Atlantic is likely to have the opposite effect, causing a southward shift in the tropical rain belt across the Western Hemisphere.
"The complexity of the Earth system is daunting, with dependencies and feedback loops across many processes and scales," said corresponding author Efi Foufoula-Georgiou, UCI Distinguished Professor of Civil & Environmental Engineering and the Henry Samueli Endowed Chair in Engineering. "This study combines the engineering approach of system's thinking with data analytics and climate science to reveal subtle and previously unrecognized manifestations of global warming on regional precipitation dynamics and extremes."
Foufoula-Georgiou said that a next step is to translate those changes to impacts on the ground, in terms of flooding, droughts, infrastructure and ecosystem change to guide adaptation, policy and management.
The article Climate Change To Alter Position Of Earth's Tropical Rain Belt appeared first on Eurasia Review.

Load-Date: January 18, 2021


End of Document


Big Data To Analyze The Mystery Of Beethoven's Metronome
Eurasia Review
January 4, 2021 Monday


Copyright 2021 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 878 words
Body


Data science and physics research at the Universidad Carlos III de Madrid and UNED has analysed a centuries-old controversy over Beethoven's annotations about the tempo (the playing speed) of his works, which is considered to be too fast based on these marks. In this study, published in the PLOS ONE journal, it is noted that this deviation could be explained by the composer reading the metronome incorrectly when using it to measure the beat of his symphonies.
Ludwig van Beethoven (1770-1827) was one of the first composers to start using a metronome, a device patented by Johann Nepomuk Maelzel in 1815. At that time, he started to edit his works with numerical marks with metronome indications. Doubts about the validity of these marks date back to the 19th century and during the 20th century many musicological analyses were carried out, some of which already pointed to the hypothesis that the metronome was broken, an assumption that could never be verified.
In any case, most orchestra conductors have omitted these marks as they consider them to be too fast (Romanticism), whereas since the 1980s, other conductors (Historicism) have used them to play Beethoven. However, music critics and the public described these concerts as frantic and even unpleasant.
Previous scientific research, such as Sture Forsén's study in 2013, has pointed to several defects that may have affected the metronome, causing it to function slower, which would have led the composer from Bonn to choose faster marks than those actually proposed. In order to validate this explanation, researchers from the UC3M and UNED have systematically compared the metronomic marks with contemporary interpretations. This requires physical skills to model the metronome mathematically, analyse data, computing, usability, and, of course, music skills. Overall, they have analysed the tempo and its variations for each movement of 36 symphonies interpreted by 36 different conductors, a total of 169 hours of music.
"Our study has revealed that conductors tend to play slower than Beethoven indicated. Even those who aim to follow his directions to the letter! The tempi indicated by the composer are, in general, too fast, to the point that, collectively, musicians tend to slow them down," says Iñaki Ucar, one of the authors of this research, data scientist at the UC3M's Big Data Institute, and clarinetist. This slowing down follows, on average, a systematic deviation, so it is not random, but conductors tend to play consistently below Beethoven's marks.
"This deviation could be explained by the composer reading the scale of the apparatus in the wrong place, for example, under the weight instead of above. Ultimately, this would be a problem caused by using new technology," says Almudena Martín Castro, the other author of the study, user experience designer and pianist, who carried out this research within the framework of her Bachelor Thesis for her Degree in Physics at UNED.
In this study, researchers have developed a mathematical model for the metronome based on a double pendulum, perfected with three types of corrections which take the amplitude of its oscillation, the friction of its mechanism, the impulse force, and the mass of its rod, an aspect that had not been considered in previous work, into account.
"With the help of this model, we developed a methodology for estimating the original parameters of Beethoven's metronome from photographs that are available and the patent outline," the work explains. In addition to this, they dismantled a modern metronome to measure it and use it to validate both the mathematical model and methodology.
The researchers tried to identify a "break" in the metronome that gave rise to the slow tempi usually followed by musicians. They tried to change the metronome's mass (it may have been damaged and a piece may have fallen off), move it onto the rod, increase the friction (the metronome may have been poorly lubricated) and even testing the assumption that the apparatus may have been misplaced, leaning over the piano while the composer was creating his music.
"None of the hypotheses matched what the data told us, which is a homogeneous slowdown in the tempi on the entire scale. Finally, we considered the fact that the deviation matches the size of the metronome's weight exactly, and we also found the annotation '108 or 120' on the first page of the manuscript for his ninth symphony, which indicates that the composer doubted where he was reading at least once. Suddenly, it all made sense: Beethoven was able to write down a lot of these marks by reading the tempo in the wrong place," they explain.
This methodology could be applied when investigating the work of other classical composers, as they are able to extract the tempo from a musical recording and clean up the data so they can be compared.
"Studying the relationship between the tempo played and marks from other composers would be very interesting, or even looking for the 'correct tempo' for composers who did not leave any metronomic marks. Is it possible that there is an average tempo at which people usually interpret Bach's fugues, for example?" they ask.
The article Big Data To Analyze The Mystery Of Beethoven's Metronome appeared first on Eurasia Review.

Load-Date: January 4, 2021


End of Document


Understanding How Birds Respond To Extreme Weather Can Inform Conservation Efforts
Eurasia Review
August 24, 2020 Monday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 783 words
Body


When it comes to climate change, University of Wisconsin¬-Madison forest and wildlife ecology Professor Ben Zuckerberg says birds are the proverbial canary in the coal mine. They are both responsive and sensitive to changes in the environment, including the extreme weather events associated with a warming planet.
However, not all birds are the same, and not all weather events have the same impact. How do different bird species respond to extreme weather events that occur for different amounts of time, ranging from weekly events like heat waves to seasonal events like drought? And how do traits unique to different species -- for example, how far they migrate or how commonly they occur -- predict their vulnerability to extreme weather?
To answer these questions, ecologists would traditionally observe a small number of bird species at a few sites over a few years, and then draw general conclusions. However, Zuckerberg and UW-Madison postdoctoral researcher Jeremy Cohen, along with Daniel Fink of the Cornell Lab of Ornithology, had more ambitious goals: they looked at 109 species across eastern North America over a 15-year period, and integrated this information with fine-scale satellite temperature and precipitation data.
In a study recently published in the journalGlobal Change Biology, the researchers show that not all birds are equally vulnerable to the effects of extreme weather resulting from climate change. As the planet warms, some species will adapt while others may struggle without conservation measures. The results of this study could help conservationists target their efforts to vulnerable species, as well as locations where extreme weather events are predicted.
The researchers used data from eBird, a global citizen-science initiative where bird enthusiasts submit checklists of bird sightings online. These checklists include which species were seen, how many, the location and time, and other observations.
The researchers compiled more than 830,000 of these checklists and integrated each one with weather data summarized over the week, month and three months before the observation was recorded. They relied on advanced computing to manage this large amount of information.
"The study we did would not have been remotely possible without data science," says Cohen. The emerging field of data science involves the study, development or application of methods that reveal new insights from data.
Zuckerberg points out that the combination of citizen science and data science makes research possible at a scale that was previously unimaginable for ecologists. However, citizen science has its limitations. Researchers have less control over the scientific process, and data quality can vary.
"Someone can go out for five minutes or two hours and submit eBird data. They can submit a checklist for 10 species or 40 species," says Zuckerberg. "We've adopted data science methods for working with large, unstructured data sets."
After controlling for this noisy data, the researchers observed that some species are less sensitive to extreme weather, and populations are not equally exposed to its effects because some geographic areas are warming faster than others.
When it comes to heat waves, Cohen notes, "long-distance migrants were not super affected by really hot periods. They winter in tropical environments and should be tolerant of heat."
However, resident birds and short-distance migrants such as robins and red-winged blackbirds responded negatively to heat waves, with their numbers sometimes declining 10% to 30% over several weeks.
As for drought, commonly occurring species like crows were more resilient than rare birds, particularly if the drought was severe and long-lasting.
"Rarer species have more specialized habitat and food requirements -- this is a general rule in ecology," says Cohen. "More common species usually have more options. If habitat quality declines due to drought, a generalist can go somewhere else."
Cohen says this is the first large-scale study, spanning half a continent, to look at how birds respond immediately after weather events. Because of the scope of the project, conservationists can better understand how many different bird species are likely to be affected by climate change, and mitigate some of the negative effects.
"If birds are truly winged sentinels of climate change, the greater likelihood of drought, flooding and extreme temperature conditions like heat waves will have significant consequences," says Zuckerberg. "We need to think about how we help species adapt to climate extremes."
The article Understanding How Birds Respond To Extreme Weather Can Inform Conservation Efforts appeared first on Eurasia Review.

Load-Date: August 25, 2020


End of Document


The Danger Of Weaponizing Trade For The Environment – Analysis
Eurasia Review
November 30, 2020 Monday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 984 words
Body


By Ken Heydon
Pressure around the world is growing to apply penalty tariffs on imports from perceived environmental free riders. But such policies are a threat to trade and are unlikely to help the environment. Fortunately, there are better policy alternatives to deal with trade and environment linkages, including tackling fossil fuel subsidies.
Prominent economists such as William Nordhaus and Thomas Piketty are advocating the imposition of carbon border taxes on imports from polluting countries. These calls are founded on the fear that levies on carbon-intensive production simply push production to countries where it is not taxed. Those imposing such border taxes might also claim legitimacy under General Agreement on Tariffs and Trade (GATT) Article XX which allows measures necessary to protect human, animal or plant life.
There is howeverno evidence of growthof widespread pollution havens. The International Energy Agency reports that by 2019 global energy-related carbon dioxide (CO2) emissions had flattened, withstrong renewables growthin China and India. China, Japan and South Korea have each recently set target dates for zero net carbon emissions.
Trade sanctions carry the risk of protectionist capture and of being a brake on the very economic development needed to fund the transition to cleaner energy and, now, to tackle theeconomic disruption from COVID-19and the associated acceleration of digitisation.
Carbon border adjustments— tax levied on imports from countries without carbon pricing mechanisms — are becoming an integral part of EU trade policy, and US president-elect Joe Biden has also expressed support for them.
These sanctions may take the form of unilateral action by Europe and the United States or be applied through EU and US preferential trade agreements (PTAs) in the Asia Pacific and elsewhere. The EU–Japan agreement, for example, contains commitments that the European Union could invoke to promote a more aggressive approach to trade and the environment, including that parties shall cooperate to promote the contribution of trade to the transition to low greenhouse emissions (Article 16.4). Similar risks are inherent in negotiation of the Australia–EU PTA.
As for the United States, Joe Biden has made it clear that any consideration of US re-engagement in the Trans-Pacific Partnership will depend on stronger commitments being made on the environment and labour. Such commitments could involve trade penalties.
Advocacy of these measures should be rebutted at every opportunity, but it is not enough just to say 'no' to trade sanctions. The energy transition is not assured and more needs to be done. Despite their movement in the right direction, China, Japan and South Korea still fund the majority of new coal-fired power plants. Fortunately, there are other trade-related measures that can be taken to serve environmental ends and which, importantly, involve reducing rather than increasing distortions to trade.
Two such measures have been on the World Trade Organization's (WTO) agenda for years but are proving frustratingly difficult to advance: attempts to reduce fishing subsidies and negotiations to liberalise trade in environmental goods and services. While this work should be maintained and accelerated — within a hopefully revitalised WTO — two other measures might yield more immediate results.
The first is action in the WTO to reduce fossil fuel subsidies. The elimination of fossil fuel subsidies would, according to the International Monetary Fund, reduce global CO2 emissions by up to 23 per cent. Some WTO disputes have targeted government support for renewable energy, giving grounds to also target policies supporting fossil fuel-based energy.
Two implementation challenges would need tackling. First, the link betweendomesticsubsidies and trade needs to be demonstrated. This can be done by invoking the Anti-Dumping Agreement to determine that energy-subsidised exports constitute exporting at less than normal value and so are open to retaliation. Second, to avoid social disruption in fossil-fuel-dependent developing countries, mitigating development assistance policies need to be implemented, coordinated by a body such as the G20.
Global fossil fuel subsidy reductions could spur reform in Australia. Support to fossil fuel consumption has increased significantly in Australia over the past decade, with revenue forgone equivalent to over 40 per cent of the energy-related tax take, a high share byOECD standards.
A second necessary area of action — again with implications for Australia — is to ensure that the ongoing US–China tech war and ill-advised pursuit of decoupling does not bring further collateral damage to vital cooperation with China on renewable energy. Australia's export of education services in electrical engineering to Chinese (and other) students has supported the development of solar PV panel manufacturing plants in China for export to Australia and the rest of the world. Trade openness is thus vital to the energy transition.
Looking ahead, as countries move to 'smart energy' policies that depend on digital grids, Australia's expertise in information and communications technology, and data science will become an increasingly valuable tradable service. Trade has a direct role to play in the pursuit of environmental goals as a facilitator, not a weapon, that can benefit all countries involved.
*About the author: Ken Heydon is a visiting fellow at the London School of Economics. He is formerly an Australian trade official, Deputy Director-General of the Office of National Assessments and senior member of the OECD secretariat. His latest book is The Political Economy of International Trade: Putting Commerce in Context (Polity, 2019).
Source: This article was published by East Asia Forum
The article The Danger Of Weaponizing Trade For The Environment - Analysis appeared first on Eurasia Review.

Load-Date: November 30, 2020


End of Document


Forecasting Urbanization
Eurasia Review
May 9, 2020 Saturday


Copyright 2020 Buzz Future LLC Provided by Syndigate Media Inc. (Syndigate.info) All Rights Reserved


Length: 1071 words
Body


University of Delaware data scientist Jing Gao is fascinated by the ways that cities and towns grow over time. This concept is known as urbanization.
Take Chicago, Los Angeles and New York. All of these are cities, but they each grow differently, in terms of how the city's land areas expand. The same is true globally, from New Delhi, India, to Paris, France.
Gao, an assistant professor of geography and spatial sciences in UD's College of Earth, Ocean and Environment, and collaborator Brian O'Neill, a UD alumnus and professor from the University of Denver, have created a new global simulation model to predict how urban land will change over the next 100 years under different social and economic conditions.
The research leverages data science and machine learning to provide a first long-term look at how urbanization will unfold - decade by decade.
The researchers describe their simulations in a paper published in the journal Nature Communications.
Data science helps long-term forecasting
According to Gao, until recently it has been difficult to generate long-term, global forecasts of urban expansion. This is because while urbanization is a global trend, the way cities develop (buildings, roads, people, economics) can change over time. Additionally, this development can vary widely country to country, and even within different parts of the same country.
To understand how this change occurs, Gao and O'Neill used data science to analyze 15 global data sets depicting various aspects of urbanization, including a newly available global map series showing urban land change over the past 40 years based on satellite images of Earth. The global maps are accurate to within approximately 125 feet (38 meters) and provide a uniquely detailed look at past urban development that was not previously possible with this degree of specificity.
"Mining historical data revealed that there are three different urbanization styles: urbanized, steadily urbanizing and rapidly urbanizing," Gao said. "And countries evolve from rapidly urbanizing to steadily urbanizing to urbanized over time."
It should come as no surprise that the United States and most western European countries are already urbanized. India and China, which previously experienced rapid development, have now transitioned to steadily urbanizing. Rapidly urbanizing countries at present include many countries in Africa.
And here's the data science part. Understanding these broad styles is not enough to capture - globally - how urbanization is playing out on the ground at a local scale.
To do this, the researchers divided the world into 375 small regions and ran a unique model for each region simultaneously, then pieced results from all models together to develop a global map. This information can shed light on how our cities may change and reveal potential impacts of urbanization that can inform local to global urban planners and policymakers.
The research team's projections show that the total amount of urban areas on Earth can grow anywhere from 1.8 to 5.9-fold by 2100. On average, if past urbanization trends continue, the world will build approximately 618,000 square miles (1.6 million square kilometers) of new urban areas globally over the century. This is an area roughly 4.5 times the size of Germany, or, more than 225 million football fields.
How this urban expansion occurs, however, largely depends on societal trends in the years to come. This includes trends in economic growth, population change and lifestyle habits, and what level of consideration is given to how our habits affect the environment.
For both developed and developing countries, for example, countries in Europe and Southeast Asia, urban expansion is expected to roughly triple if society favors materialistic and fossil-fuel driven development instead of adopting a sustainability mindset.
In the U.S., the least urban expansion occurs if people are focused on sustainability, such as green development and environmental awareness. In this case, urban land is expected to grow by 1.3 times by 2100. But if people favor highly materialistic development over the same timeframe, with high consumption of fossil fuels and a material-driven society, sprawl-like urban expansion is expected, with close to four times the amount of expansion the U.S. had at the beginning of the century.
The U.S. already is among the countries with the largest amount of developed land, so four-fold growth in urban expansion is a lot.
"This is where our projections can inform policy and planning," said Gao. "These projections can help researchers and analysts understand how large-scale changes that occur over a long time period, such as climate change, may affect local urban areas."
Most individuals do not realize how changes to the landscape, such as buildings and roads, may affect their lives. In Delaware, for example, second homes being built near the coast often come at the cost of agricultural farmland. While these developments may increase an area's economic prosperity, they can have other unintended consequences, such as increased potential exposure to coastal flooding and sea level rise.
And, no matter what socio-economic scenario was selected, the simulations show that most countries will become urbanized by the end of the century.
One interesting finding from the work is that although prevailing thought is that urbanization is primarily happening in the developing world, Gao said this may not be the case.
"If you look at the data collected over the past 40 years, the absolute amount of new urban land construction in the developed world is comparable to the developing world," she said. "However, the changes seem faster in the developing world because there currently is much less developed land there, so the rate of change appears greater."
This begs the question: as developing countries in Africa continue to grow, will they ever catch up to or surpass developed countries like the United States in terms of urbanized land?
"According to today's definition, Africa is expected to become urbanized by 2100," said Gao. "But even if it continues developing at a very fast rate relative to the rest of the world throughout the century, it won't catch up to developed countries like the U.S. because the difference at the outset is large and the developed world still keeps expanding its urban area."
The post Forecasting Urbanization appeared first on Eurasia Review.

Load-Date: May 10, 2020


End of Document


LOWER FEES, BETTER TECH?; NEW VOICES ARE VYING FOR SPACE IN THE FOOD-DELIVERY WORLD
Los Angeles Times
March 7, 2021 Sunday
Final Edition


Copyright 2021 Los Angeles Times All Rights Reserved
Section: FOOD; Food Desk; Part FO; Pg. 1
Length: 1031 words
Byline: STEPHANIE BREIJO
Body


Nabeel Alamgir keeps waiting for a cease-and-desist letter. If he gets one, he says, he'll view it as a badge of honor.
In January, the food-tech entrepreneur launched NotGrubhub.org, a map-based website that points customers to restaurants that take food orders directly. It was designed to bypass Grubhub and third-party food delivery apps and platforms that can legally charge up to 20% in commissions or marketing fees from restaurants in Los Angeles.
Any restaurant can add itself to the database, which is free to owners and customers and lists more than 120 businesses nationwide.
"That's just an awareness campaign," says Alamgir, who used to work in food service. His website is one of a number of emerging voices vying for space in the food-delivery world, offering fees that are typically lower than those of major players such as Postmates, Uber Eats and Grubhub.
The former chief marketing officer of Bareburger said he watched as his chain's profits diminished over the years and realized if a national chain was hurting from app commissions and fees, mom-and-pop restaurants could be too.
In 2019 Alamgir cofounded Lunchbox, a platform that charges restaurants a monthly flat fee -- as opposed to sales commissions -- that range from $88 to $490, depending on the service package; most clients, he says, pay $200. The platform hosts and designs apps, websites and ordering pages; maintains the virtual end of cloud kitchens; and creates marketing materials such as Instagram ads. Lunchbox also hires the same delivery drivers used by the big third-party platforms at the same cost of roughly $6 per driver per order and makes marketing technology and materials easier for restaurateurs and chefs to use.
"A very messy system was created because restaurant people are not starting these tech companies -- tech people are," Alamgir says. "Restaurateurs are not tech people; they want to be hospitable and create amazing food, and then we told them, 'You've got to be amazing with tech as well or you're dead. Your business is dead.' "
The new contenders tend to promise restaurants one of two things to remain competitive with larger, more established platforms: flat-rate fees, like Lunchbox, or commission rates that can hover as low as 2%, as well as marketing capabilities to help restaurants stay visible.
Grassroots and more locally focused newcomers also can offer hypertailored curation, privacy and a shift from third-party delivery systems entirely.
The owners of Pasadena-based DiNG -- not to be confused with Ding Menu, a new commission-free restaurant ordering tool -- say they want it to become the Spotify of food platforms by tailoring meal recommendations based on a two-minute quiz.
Former chef and DiNG cofounder Mike Chen said his background in data science helped inform the company's algorithm, which is based on answers to questions such as, "What looks good for a cold winter night?" (Your options might be fish fillet in chile sauce; braised pork belly; or a soup of salted pork with bamboo shoots.)
The Asian-cuisine-focused operation also allows menu ordering, but the format is arranged by dishes or even region, as opposed to separate restaurants, creating a sort of editors' pick of noodles, poached chicken, curries, stews and more from a mix of restaurants largely located in the San Gabriel Valley.
The platform charges a commission of less than 5%. The startup also focuses on user privacy: Hired drivers, also used by third-party apps, pick up food from the restaurants and deliver them to designated DiNG handoff points. From there, the company's own drivers deliver on the last leg of the route to avoid providing home addresses to the major platforms.
DiNG, spurred by the pandemic and still in its nascency, offers daily service within a limited radius. By the end of 2021, Chen hopes it will provide the same daily service to all of Los Angeles County as well as Ventura and Orange counties.
Some innovators never intended to enter the delivery business at all. Jared Jue envisioned MAMA as a restaurant-recommendation site, but as the pandemic began to shutter independent restaurants, the founder felt a need to preserve businesses in need -- many of which are underrepresented in media and run by immigrant families.
With the help of Alice Han, MAMA launched Drive-By Kitchen, which picks up an ever-changing lineup of dishes from multiple restaurants throughout Los Angeles and Orange counties and then delivers them to three pickup locations: the Westside, Koreatown and Alhambra.
The service is not offered on a daily basis; rather, it is scheduled approximately every other week. Customers must order in advance, almost like an event, and "tickets" are limited.
Drive-By Kitchen participants are frequently restaurants that can't afford to join the major delivery apps or aren't tech-savvy, and they keep 100% of the profits. MAMA's only charge to customers? A credit card service charge, along with a small fee for team members and gasoline.
"Businesses were kind of living and dying by the phone waiting for some sort of Doordash or Grubhub or Uber to come through, and I think it was mentally just draining because they weren't getting what they needed," Jue says. "We knew what we wanted to do was focus on placing large orders with the restaurants so that they could have something concrete, in a way."
The format ensures restaurants don't lose money on food costs, while the "combo-meal" format gives diners a new lineup every time.
This year Drive-By Kitchen, which Han oversees, also launched a meal-matching program, where every meal sold gets matched by a charitable partner that buys a second meal -- doubling the restaurants' revenues and donating that second portion to those in need, such as seniors in Chinatown.
"In terms of the big players in the market, we're not trying to compete with them by any means; I think that they're business-first, and our mission is more culturally relevant-first," says Jue. "We're really trying to preserve the dishes, the recipes, the restaurants -- those kinds of things that will actually disappear at the end of this whole thing. Is it sustainable? I hope so. I hope we can change the conversation."

Graphic

 
PHOTO: (no caption)  PHOTOGRAPHER:Kay Scanlon For The Times; Getty 

Load-Date: March 7, 2021


End of Document


How to erase the digital divide
Los Angeles Times
March 1, 2021 Monday
Final Edition


Copyright 2021 Los Angeles Times All Rights Reserved
Section: MAIN NEWS; Opinion Desk; Part A; Pg. 1
Length: 870 words
Byline: Meka Egwuekwe, Meka Egwuekwe is executive director of CodeCrew.
Body


The computer sci-ence industry is filled with talented techies, most of whom are white or Asian. Black professionals make up only 5% of the tech workforce, a segment that increased by only 1 percentage point between 2014 and 2020, according to a recent report. That's unacceptable.
Much is made of the digital divide, but little has been done to eradicate it.
Bias in artificial intelligence, its algorithms and its training data are direct byproducts of the divide. These biases are more dangerous than ever because they can impact decision-making on a massive scale and at breakneck speeds, hindering business in all sectors of the economy.
To help solve this problem, we need to get more underrepresented communities into careers in computing and engineering, especially data sci-ence. More, and different, perspectives can only help lead to better products and services.
At the same time, we can truly advance a Black and brown middle class, and create generational wealth, boosting economic growth and providing an entire new set of industries and opportunities across the nation.
It won't be easy to grow such a culture of tech producers in underrepresented communities, but it is absolutely doable.
The organization I lead, Memphis-based CodeCrew, and others such as the California-based Black Girls Code, are working to build that culture into the heart of Black and brown communities.
The potential economic impact is significant. Computer science graduates of CodeCrew's adult coding boot camp make an annual average starting salary of more than $51,000. Before their training, they made an average of just $15,000 per year.
That boost in income can be life-changing, providing a comfortable middle-class life in Memphis and many other parts of the country.
If we are to truly diversify the tech community we are going to need help -- a lot of it -- from the business and technology communities.
What could they do?
Waive the four-year college requirement for entry-level computer science positions at their companies. That is often a barrier for qualified CodeCrew grads.
Set goals and targets for diversity in workplaces, especially on engineering and leadership teams, and hold themselves publicly accountable by annually publishing how they are doing, as Netflix did recently. In three years, the company doubled the number of Black employees in its workforce and on its leadership team.
At the same time, companies should take an honest look at barriers in their workplaces that prevent them from retaining diverse talent and then eliminate discriminatory or unwelcoming environments and conditions. That could include establishing equitable pay and promotion opportunities, and initiating zero-tolerance policies for harassment, stereotypes or microaggressions.
Allow a small percentage of company time for employees to participate in mentoring programs that target underrepresented groups. WorldQuant Predictive's data scientists are working with CodeCrew students to bridge this gap and bring new skills to a diverse workforce. Google encourages employees to explore tech ideas not directly related to their assigned projects, a policy that has been credited with launching Gmail and Google News.
Why not have that same moonshot mentality with respect to diversity, equity and inclusion goals? Mentoring is a simple but highly effective means to transmit knowledge and skills.
Set up data science/software engineering offices in more regions where there are disproportionate populations of underrepresented groups -- including such places as Memphis and New Orleans.
Partner with historically Black colleges and universities, which have been working on these issues for a long time, despite being under-resourced. Then make sure to include these schools, along with diverse coding boot camps, as sources for talent -- and increase financial support for them.
Lobby the federal government to subsidize or otherwise make broadband internet access affordable in low-income communities, much like food is subsidized. Broadband is no longer a luxury, but a necessity, and should be treated as a utility, like electricity, gas and water.
Doing these things can create a new environment for data science and AI. But that is just a means to an overall greater outcome, which is building businesses by increasing the number of people of color in technology.
In doing so, we could become more internationally competitive with a rising China, advancing India, resurging Russia and unified European Union.
To bridge the digital divide, we need to catch future techies early. Every child should have access to computer science and computational thinking in school.
CodeCrew is working in Tennessee to help secure passage of legislation that requires every high school in the state to offer computer science classes and provide the necessary funding to train teachers to lead these classes.
Of the 500 kids CodeCrew works with each week, more than 90% are Black and Latinx youth. The vast majority of our students are more likely to go on to study computer science. With an assist from businesses and tech companies, overdue change like this could happen on a national level -- and businesses would ultimately be the biggest beneficiaries.

Load-Date: March 1, 2021


End of Document


CORONAVIRUS IN CALIFORNIA; We don't know the half of it; Did 50% of L.A. residents catch COVID-19? It all depends.
Los Angeles Times
March 5, 2021 Friday
Final Edition


Copyright 2021 Los Angeles Times All Rights Reserved
Section: CALIFORNIA; Metro Desk; Part B; Pg. 1
Length: 1106 words
Byline: SANDY BANKS
Body


I've grown accustomed to conflicting views when it comes to the pandemic.
We can gather in the library, but our kids can't go to school. I can finally get my hair done, but a facial is not allowed. You shouldn't wear a mask, you have to wear a mask, you really should be wearing two masks.
I understand the inconsistency. This virus is so new that all of us -- from CDC scientists to supermarket cashiers -- are still trying to navigate a steep learning curve.
And I like to think that nothing surprises me anymore. But then something comes along that shocks me all over again. Last week, it was the news about how many people locally already carry antibodies to the virus.
According to some estimates, as reported in The Times and elsewhere, as many as half of Los Angeles County's 10 million people have already been infected. And that's even though tests for COVID-19 have confirmed fewer than 1.2 million local infections.
The prospect of that many millions of uncounted infections seemed mind-boggling to me. How could more than 3 million people slip through our testing apparatus?
I wanted to know how those numbers were calculated, so I called L.A. County's chief prognosticator, Dr. Roger Lewis. His job is to quantify and model the spread of COVID-19, to help make sure that the county's hospital system is prepared to meet pandemic healthcare needs.
He is not surprised by the high immunity estimates, but he noted that calculations vary. "I've seen different estimates, from 1 in 4 to more than 50%," he said.
The county's official immunity estimate is near the middle of that spread, at approximately 3 in 8 people, or 37.5% of county residents. And that does not take into account the almost 2 million people who have received at least one dose of the vaccine.
Measuring immunity, it turns out, is more sophisticated that just counting positive tests and vaccinations. When the virus first emerged last year, California researchers began "antibody surveillance" -- tests of random people to check for COVID-19 antibodies.
"They found that the fraction of people with antibodies was much larger than the fraction of people who knew they'd been sick or people who'd tested positive," Dr. Lewis said. Many people with evidence of having been infected had never experienced symptoms of the illness; others may not have had access to tests.
Projecting that forward suggests there are now millions of uncounted COVID-19 survivors who were never tested, or at least not at a point when they would register as positive.
That early antibody testing clued researchers in to the phenomenon of asymptomatic infection and surreptitious spread. Further study of virus replication, infection demographics, hospitalizations, deaths and more led them to believe that approximately 40% of COVID-19 carriers will have no symptoms and may not show up in testing statistics.
That information established a baseline for measuring how many people might be immune -- and, by extension, how many others are still vulnerable to the disease.
But immunity estimates are also influenced by what assumptions researchers make and what trajectory they expect.
"If you talk to enough people, you're bound to get a mix of opinions," said Dr. George Rutherford, professor of epidemiology and biostatistics at UC San Francisco. "Some might [estimate] conservatively, and some are more gung-ho," he told me.
In other words, they all use the same basic formula, but the ingredients and measurements might differ a bit.
"We know the natural progression of this disease," explained David Conti, a USC professor of preventive medicine and specialist in data science integration. "We can describe that with a mathematical model."
But in real life "the number of new cases each day bounces around a lot," Conti said. "So as soon as we come up with a model, it's out of date."
The USC researchers' recent model offers a wide range of estimated cumulative infections, concluding that between 3 million and 5.5 million people in Los Angeles County have likely already been infected by COVID-19.
That translates to between 30% and 55% of the county's population. It's the higher number -- painting us as a hotbed of infection but also as a region drawing closer to herd immunity -- that has caught people's attention.
But that doesn't mean that half of the people you know have already been infected with COVID-19.
"Our model is on the level of the entire L.A. County," where infection rates vary drastically by neighborhood, USC researcher Abigail Horn said. "People want to take a number and make a statement about it. But it really is about the local communities ... and how this [pandemic] has amplified healthcare disparities."
That is a message we can't afford to forget. The burden of disease is distributed unevenly -- and a 50% immunity rate looks different depending on where you live. It's a worrying reflection of the dismal job we've done protecting working-class families from infection -- or it's an encouraging prospect, because the remaining pool of prospective virus spreaders is smaller than presumed.
Considering that sort of ambiguous messaging, I couldn't help but wonder how vested we should be in the barrage of statistics that accompanies our journey through COVID-19.
I put that question to Dr. Rutherford, who specializes in the study of infectious diseases. He thinks a break from tracking the stats may be just what people like me need. "When you're listening to all these statistics and opinions," he said, "people may lose the forest for the trees."
The "forest," to him, is the rapid creation of effective COVID-19 vaccines, which he sees as "the greatest miracle of modern molecular biology since the discovery of DNA," he says. "That's the overwhelming good news right now ... the culmination of 70 years of [scientific] advances."
The "trees" are the numbers we're being bombarded with. "There were 64,000 [research] papers published about COVID-19 by the end of October," he said. "There's a lot of information to digest."
We are not going to find the reassurance we seek in the minutiae of statistics. "A vaccine that is 90% effective, is the same as one that is 85%," Dr. Rutherford said. "You're not being cheated out of that 5%." Yet people are obsessing about which vaccine to get.
He reminded me that our health rests not on the numbers, but on the precautions we take. We may not know what number equals herd immunity, but we do know how to protect ourselves, the doctor says:
"Wear a mask, keep your distance from people outside your pod, avoid crowded indoor spaces. Don't go to Costco on a Saturday afternoon. And get vaccinated as soon as your turn comes."

Graphic

 
PHOTO: NURSE Marijorie Tabago administers a COVID-19 test on Mercedes Madrano at an Ontario testing site.  PHOTOGRAPHER:Irfan Khan Los Angeles Times PHOTO: NURSE Celeste Montoya, left, inoculates Winnetka resident Ana Vidales Torres as her daughter Talia, 6, gets a close-up look at a mass vaccination site at Valley Crossroads Seventh-day Adventist Church in Pacoima.  PHOTOGRAPHER:Mel Melcon Los Angeles Times PHOTO: DRIVERS remain in their vehicles while being monitored to ensure they suffer no adverse reaction to the COVID-19 vaccine at the Forum in Inglewood in January.  PHOTOGRAPHER:Al Seib Los Angeles Times 

Load-Date: March 5, 2021


End of Document


CITY BEAT; A tail-wagging tale of man and corgi
Los Angeles Times
January 30, 2021 Saturday
Final Edition


Copyright 2021 Los Angeles Times All Rights Reserved
Section: CALIFORNIA; Metro Desk; Part B; Pg. 1
Length: 1471 words
Byline: NITA LELYVELD
Body


Nearly a year ago, I told you about a small act of kindness to a homeless man and his corgi that had grown into something much larger for all those it touched.
It pulled at a lot of your heartstrings -- in good part, I think, because dog love runs deep, and corgi lovers are particularly ardent.
I told you about the man and his corgi and the couple who had stepped in to help them early last March, less than two weeks before COVID-19 shut down California.
And, despite how many more people have found themselves in dire straits in the ensuing crisis, I've never stopped hearing from those eager for the latest on how dog and man were faring.
I'm glad to say that the update I'm bringing you at long last may offer some respite from the current grim news. It gives me pleasure to say, too, that much of the good I have to share springs directly from the generosity of readers.
Still, that's really not my reason for telling it. I do so in this time of widespread need to reiterate the central point I set out to make in my first corgi column -- about the enormous potential benefit to both givers and receivers of doing something, however little, to help others.
But first, a recap.
The corgi I introduced you to last March had been having a hard time since his person lost his good job, then his home. Over four tough years, man and dog had ended up homeless -- and neither had adjusted easily.
The corgi's person had a PhD and had worked for years in a highly specialized field with few job openings. He was earning six figures in the job he lost, and when he couldn't find another one, his circumstances steadily worsened until he and his corgi no longer had a place to live. They bounced around, with stops at a group home and a single-room occupancy hotel on skid row in downtown L.A.
The man started driving for Uber while he tried to teach himself skills for a new line of work. When he no longer could make payments on his car, he rented cars from Uber by the hour -- working longer for less gain, with less time to spend prepping for a new career and less chance of climbing out of his hole.
And all the while, as he reached out to social-service organizations for short-term emergency housing, many doors remained closed because he had a dog.
Which is where the initial act of kindness came in.
Loath to be alone, but desperate, the man wrote a Craigslist post in the fall of 2019 asking for someone to step in and help him by fostering his corgi for a few months. A friend of Ted and Sandy Rogers of Hollywood forwarded the post to them, thinking that helping the dog might also help them as they grieved the loss late that summer of their own beloved corgi, Sienna, due to cancer.
Ted has diabetes, and Sienna alerted him with licks when his blood sugar dipped low. Sienna had been the center of their lives.
The foster corgi they took into their home was traumatized from all the instability. Ted and Sandy made it their mission to restore his sense of calm and security.
Meanwhile, the corgi's person showed up on Sunday afternoons to visit -- and while he and his dog cuddled and played, he gradually opened up and told Ted and Sandy all that he'd been through. He also needed their emotional support, and they gave it to him.
The corgi's person, I have to stop and say here, continues to feel a good deal of shame for his fall. Even now, after the pandemic has imploded so many other previously secure lives. For that reason, he continues to ask me to maintain his anonymity -- which I do, as I pray that he will stop beating himself up over time.
When I wrote the first column about him, he was still searching for a job, still separated from his corgi. I asked if anyone could help him find a stable home or a job in data science, the field he was trying to enter. I also said I hoped that soon he and his corgi would be reunited and that Ted and Sandy would find a new dog to love.
I put those wishes out to the universe, which instantly began delivering.
Some people sent the corgi's person loving notes, telling him that what had happened to him could happen to any of us. Others donated to a GoFundMe campaign set up to make his life easier. And within hours of the column's appearance, multiple people had written with job leads and offers to help him with housing.
One of the first came from Mike Kilroy, a real estate investor who offered the corgi and his person -- sight unseen, no strings attached -- a studio apartment in Palm Springs, rent-free for a year. Whenever Ted and Sandy wanted to visit, he said, he'd give them a free place to stay, too -- and the same for me if I wanted to come see how dog and man were doing.
"It was something I could do, so why not?" Kilroy told me simply when I asked him about it this week. "It's all well and good to talk about solving your problems and moving ahead. But you can't do that without a stable base."
The warmth of the offer proved irresistible, even if it meant relocation. On March 22, just a few days after California's stay-at-home order kicked in, the man went to get his corgi from Ted and Sandy and take him to their new Palm Springs home.
Around the same time, he got a job offer in data science. And even though he was told that, because of closures, it would not start until May, he had more than $4,000 in GoFundMe donations to help tide him over until his first paycheck.
Ted and Sandy, meanwhile, found themselves once again dogless and missing their foster corgi terribly.
Then Sandy lost the job she'd had for 18 years, doing accounts payable for a suddenly shuttered office. (Any leads, anyone? You've come through before.)
Their stint at fostering had readied them for a new dog -- and they wanted to rescue a corgi in need. But once the pandemic hit, people everywhere suddenly seemed to want a pet for companionship. When their corgi rescue searches came up empty, they started looking at other breeds and on the websites of corgi breeders.
Many corgi enthusiasts reached out.
Ted and Sandy felt like rock stars as their fame spread online through corgi groups. One owner who had read the column recognized Ted, Sandy and the foster corgi at Trader Joe's. Another who works in tech started trying simultaneously to find a job for their foster corgi's person and a corgi for Ted and Sandy.
Julie, who for privacy reasons asked that I use her first name only, said she's active with a local corgi rescue group that, pre-pandemic, organized frequent social hikes. She'd also researched breeders when looking for her own corgi, Truffle (who has her own Instagram profile), and had learned of one in Wyoming who is the friend of a friend she'd met running overnight relay races.
One Saturday in July, she saw a newly listed corgi puppy on that Wyoming breeder's site and immediately told Ted and Sandy. They put down a deposit that day.
And because Ted and Sandy don't drive and feared public transportation because of the coronavirus, Julie and her running friend did a relay of sorts to bring the 3-month-old corgi to them.
Julie's friend picked the puppy up in Wyoming and drove her to Utah. Julie rented a car and drove from L.A. to Utah to bring the dog to Ted and Sandy. Ted, Sandy and Julie met for the first time in person on delivery -- and realized they were neighbors. Now Ted and Sandy's puppy -- Tazzy, short for Tasmanian Devil -- sometimes meets Truffle for neighborhood play dates.
I met Tazzy the other day. She smooched me, as she smooches everyone she meets, person and dog alike. She zooms around Ted and Sandy's apartment, causes havoc, destroys toys and delivers endless joy. Ted and Sandy are thoroughly smitten, even though she likes to pull Sandy's hair.
As for dog and man in Palm Springs, the corgi's person sent me a selfie he took when they moved back in together that could serve as a visual definition of love. It hasn't all been easy. He initially felt lonely in a new city in a pandemic. His first job didn't last because he failed to get security clearance within six months -- likely due to the many moves in his recent history.
But then a friend in his old field asked him for a job reference -- and he asked her if she'd return the favor for the job she was leaving. He got that job. He's now back to six figures. He credits his rent-free year with giving him the grounding to be capable of leaping at the chance when offered. He, Ted and Sandy text back and forth every day. He paid the expenses for Tazzy's travels from Wyoming to Hollywood. He credits Ted and Sandy with rescuing him just as much as they rescued his corgi.
And he credits every single person who offered up words of encouragement, donations, positive thoughts, advice and assistance with playing a part in his comeback. He said he wished I could name every name.
Alas, I can't -- I can't even name his -- but I offer my thanks to all who helped him in so many ways.

Graphic

 
PHOTO: TED AND SANDY ROGERS walk their new corgi, Tazzy, in Hollywood. The couple had previously lost their 13-year-old corgi and fostered one for a man who was jobless and homeless but has gotten back on his feet.  PHOTOGRAPHER:Wally Skalij Los Angeles Times 

Load-Date: January 30, 2021


End of Document


In the West, wildfire smoke accounts for more pollution; Air quality declines again after years of steady gains, study finds
Los Angeles Times
January 15, 2021 Friday
Final Edition


Copyright 2021 Los Angeles Times All Rights Reserved
Section: CALIFORNIA; Metro Desk; Part B; Pg. 1
Length: 1047 words
Byline: Tony Barboza
Body


Wildfire smoke now accounts for up to half of all fine-particle pollution in the Western U.S., according to a new study that blames climate change for worsening air quality and health risks in both urban and rural communities in recent years.
The study by researchers at Stanford University and UC San Diego found that the concentration of tiny, lung-damaging pollutants known as PM2.5 that are attributable to wildfire smoke roughly doubled between 2006 and 2018, while the share of pollution from other sources like car and truck exhaust declined.
The trend is most pronounced in Western states and highlights the rapidly growing health threat of wildfire smoke. This became shockingly apparent to millions during last year's record-breaking firestorm, which enveloped much of the West Coast in an unhealthy pall for weeks.
Levels of PM2.5 had been steadily improving over the last two decades in which they have been routinely monitored, as a result of regulations that have cut emissions from vehicles and power plants. But those gains started to slow, then reverse, over the last decade or so, according to the study.
"The overall picture is of a stalled and reversed improvement, which is a result of other sources getting cleaner and wildfires getting a lot worse," said Marshall Burke, a professor of Earth system science at Stanford University and lead author of the study published in the Proceedings of the National Academy of Sciences.
The two major factors driving the increase in wildfire smoke are the warming climate and decades of fire suppression that have allowed fuels to build up, according to researchers.
They made their estimates by developing a statistical model using fire and smoke data from satellites and readings from ground-based air quality monitoring stations.
Nationwide, wildfires are now responsible for up to 25% of fine-particle pollution, the study found.
"We know wildfires generate smoke. We know smoke is bad for health. But we really didn't have a comprehensive national picture for how much wildfires are contributing to poor air quality," Burke said.
Francesca Dominici, a professor of biostatistics at Harvard's T.H. Chan School of Public Health who was not involved in the research, called it "an excellent study that relies on sophisticated data science approaches" and "provides strong evidence that wildfires are an increasing threat to human health."
Dominici said its findings are concerning, "especially at the time where the U.S. EPA has recommended to retain the current standards for PM2.5 pollution and as we are fighting COVID-19 that is attacking our lungs."
"I hope that reducing the risk of climate change- related disasters, such as wildfires, will be a priority for the new administration," she added.
For decades, motor vehicles and industrial emissions have been responsible for most of the West's PM2.5, though at least some of that type of pollution has always come from fires. Previous studies have predicted that greenhouse gas emissions will dramatically increase wildfire smoke in the in the Western U.S. in the coming years as temperatures rise and dry the landscape.
In the latest study, Stanford and UC San Diego researchers predicted dramatic health impacts if nothing is done to slow climate change by slashing emissions and better managing forests. Within decades, they found, exposure to wildfire smoke alone could increase dramatically to the point of being one of the deadliest climate impacts.
The study projects an additional nine to 20 smoke-related deaths per 100,000 people by midcentury if emissions continue at their current pace, which is close to the roughly 24 additional deaths per 100,0000 people predicted directly from rising heat -- the deadliest effect of climate change on people.
"Wildfires are going to be the way that many of us experience climate change, as important as these direct heat impacts," Burke said. "These changes in wildfire risk are the combination of two main things we have done: a century of wildfire suppression and climate change. None of these future estimates are an inevitability. They are a choice."
The analysis also found that while people of color continue to be exposed to higher levels of total PM2.5 -- as has long been the case -- higher-income counties with a higher proportion of white people are on average more exposed to higher levels of PM2.5 from wildfire smoke.
Researchers acknowledge that measuring outdoor pollution does not necessarily correspond to people's actual exposure because it does not factor in how much time they spend outdoors or the age and quality of their home.
Past research shows that more outdoor pollutants seep into "older, smaller homes and for lower-income households and these differences could lead to disparities in overall individual exposure even if ambient exposures are not different," according to the study.
Scientists suspect the 2020 wildfires inflicted widespread health damage by fouling the air of nearly 96 percent of Californians with smoke levels exceeding federal standards, according to the state Air Resources Board. The weeks-long siege of smoky air generated both the highest readings and most widespread unhealthy levels of fine-particle pollution since continuous monitoring began in the late 1990s.
Of greatest concern are the microscopic particles in smoke that can be inhaled deep into the lungs and enter the bloodstream. Not only do those pollutants irritate the eyes, nose and throat, tighten the chest and cause difficulty breathing, they can trigger asthma attacks, strokes and heart attacks. Wildfire smoke poses serious risks to young children and the elderly, and people with chronic health conditions such as asthma, lung disease and heart disease face increased risk of hospitalization and death.
Scientists know from decades of research that breathing the fine-particle pollution in urban smog can lead to long-lasting health problems.
Though less is known about the long-term damage from the fine-particle pollution in wildfire smoke, early research suggests it impairs people's lungs long after the smoke clears. An ongoing health study in Montana reported last year that people in a community that was blanketed with wildfire smoke for 49 days in 2017 still had decreased lung function two years later.

Graphic

 
PHOTO: A WATER TRUCK operator is overcome with smoke during the Silverado fire on Oct. 26 in Irvine. A new study blames climate change for worsening health risks.  PHOTOGRAPHER:Allen J. Schaben Los Angeles Times PHOTO: MILL VALLEY RESIDENTS Patrick Kenefick, left and Dana Williams record video of San Francisco's Golden Gate Bridge on Sept. 9. Last year's wildfires covered the West Coast in an unhealthy pall for weeks.  PHOTOGRAPHER:Eric Risberg Associated Press PHOTO: FIREFIGHTER Ricardo Gomez works on the Creek fire Sept. 6 along Highway 168 in Shaver Lake, Calif.,  PHOTOGRAPHER:Kent Nishimura Los Angeles Times 

Load-Date: January 15, 2021


End of Document


Back to the drawing board, maps
Los Angeles Times
November 13, 2020 Friday
Final Edition


Copyright 2020 Los Angeles Times All Rights Reserved
Section: CALENDAR; Entertainment Desk; Part E; Pg. 1
Length: 831 words
Byline: Carolina A. Miranda
Body


To survive a presidential election is to have the U.S. electoral map branded into our skulls. These cartographic depictions take our vast United States and reduce every nuance of politics down to a dichotomy of blue and red -- no texture permitted.
President Trump may have won Texas with nearly 5.9 million votes (as of Thursday afternoon), but former Vice President Joe Biden still raked in more than 5.2 million votes in the state. This reality is not reflected in the electoral maps blanketing every news website -- leaving one to think that everybody in Texas is a Republican. (The vote tally indicates that more than 5 million people would beg to disagree.)
And what about blue-blue California, where the map overlooks the nearly 5.5 million people who voted for Trump? Or Nebraska, which actually splits its electoral votes? Biden won one of the five electoral votes in that state, yet Nebraska is nonetheless shown on most electoral maps as a sea of red -- which is not just a distortion, it's patently incorrect. (Kudos to MSNBC's exuberant Steve Kornacki for showing Nebraska marked by blue and red stripes.)
In addition, the electoral map as we know it distorts the size of the different voting populations. The state of my birth, Wyoming, is the 10th largest U.S. state by area, but with a population of only 579,000 people, it has only three electoral votes. Yet its broad swath of redness plays an outsize role in our electoral maps. New Jersey, by contrast, with 9 million people and 14 electoral votes, takes up far less room.
Naturally, this design reflects the design of our electoral system, in which the votes for president are tabulated by state in a winner-takes-all fashion -- part of the electoral college system (which, frankly, has no place in a 21st century democracy). But as these maps continue to be widely employed by media across the political spectrum, they systematically reinforce the idea that the U.S. is either Red Territory or Blue Territory without overlap or exception. And that is wrong.
In response, designers have been trying to find better, more nuanced ways to visualize election data -- such as the unusual honeycomb-patterned map, created by the politics site FiveThirtyEight, which shows electoral college votes from the 2016 election to scale. Likewise, Greg Albers, an L.A.-based digital publisher who writes about issues of digital literacy, created an experimental map that shows the U.S. in gradients of purple.
During this election, France's Le Monde has experimented with more proportional graphics, showing the number of electoral votes for each state laid over a geographical map -- so Wyoming is marked by three red blocks, while New Jersey gets 14 blue ones. And the New York Times has also been experimenting with map design on its election site, including a Blokus-style electoral votes map, but the page opens with a traditional electoral map and you have to click through to find the alternates.
Last week, a series of GIF maps created by Karim Douieb, co-founder of Jetpack.AI, a data science company based in Brussels, went viral for showing different ways in which election data might be displayed in map form. His designs, first released in 2019, visualize election returns in ways that give more texture to where and how Americans are voting.
As he notes in an online presentation: "Acres don't vote, people do."
Douieb says he was inspired to create his maps after seeing a tweet posted last year by Lara Trump (a former TV producer who is married to Eric Trump) that showed a traditional county-by-county map along with the inscription, "Try to impeach this."
"I took that as a challenge," he tells me via email. "The picture she was showing was plain wrong given the context. By that I mean that it is an accurate map (displaying what party won each county) but used in a misleading way."
While designing better maps won't fix the fractures in our electoral system, they can help tell a better story about who we are and the spaces that we share.
"I believe traditional electoral maps are not bad as such," Douieb says, "but they are often used in a misleading way as they advantage geographical accuracy over electoral importance. ... [And] they don't represent how the people voted nor are they a good representation of how the electoral system works."
Our country is more complicated than blue and red. To illustrate that, I'd like to go back to Wyoming -- specifically, Casper, the town where I was born. In 2017, Marcus Patrick Ellsworth reported this very wonderful story about the LGBTQ community in Casper for MTV News.
"There is a tendency among certain swaths of liberals and leftists to disregard entire states like Wyoming," he wrote, "places where the populace tends to vote Republican and pass regressive laws."
But look beyond the red, he adds, and you'll find that "places like Casper, homes of fearless resistance waged out of necessity, shine bright with hope when seen up close."
We need maps that speak to that.

Graphic

 
PHOTO: AN ELECTORAL map by Karim Douieb shows voting by population, bottom, rather than by geography. Instead of vast swaths of red or blue, the map reveals the mixed nature of voting patterns  PHOTOGRAPHER:Jetpack.AI PHOTO: AN ELECTORAL map by Karim Douieb shows voting by population, bottom, rather than by geography. Instead of vast swaths of red or blue, the map reveals the mixed nature of voting patterns.  PHOTOGRAPHER:Jetpack.AI 

Load-Date: November 13, 2020


End of Document


Asian Americans split on affirmative action at UC; Proposition 16 divides university system's most overrepresented group of students.
Los Angeles Times
November 1, 2020 Sunday
Final Edition


Copyright 2020 Los Angeles Times All Rights Reserved
Section: CALIFORNIA; Metro Desk; Part B; Pg. 1
Length: 1641 words
Byline: Teresa Watanabe, Jennifer Lu
Body


Angela Li and Vivrd Prasanna have achieved the pinnacle of a public university education -- she a senior at UCLA, he a freshman at UC Berkeley. Both are children of immigrants, with Li's parents from China and Prasanna's from India.
They share values of hard work and high expectations. Li checked out school textbooks during the summer to get a head start on fall classes and in high school took test prep courses with money her working-class parents saved by giving up family vacations. Prasanna took college classes in data science as a high school student.
But when it comes to Proposition 16, the Tuesday ballot measure that would once again allow affirmative action in public education, contracting and hiring, the two UC students and their families sharply diverge.
Li supports the measure as a way to expand diversity in education -- but her parents oppose it, suspicious that it will limit the enrollment of Asian Americans. Prasanna opposes it as the wrong way to deal with root causes of educational inequity, while his parents are torn over their twin desires to stand for civil rights and to ensure equal opportunity for their community.
Their diverse views reflect the complexity of the affirmative action issue among Asian Americans, who represent more than 50 ethnic subgroups with varying politics, histories in the United States and levels of income, education and English language ability. Those differences play out in their views of Proposition 16 and concerns about their place at the University of California.
Asian Americans predominate at UC and are significantly overrepresented -- making up 40.3% of in-state freshmen last year compared with their 19.9% share among California high school graduates eligible for UC admission. By comparison, Latinos made up 31.5% of UC freshmen and 44.7% of that qualified pool; whites were 20.6% at UC and 27% of eligible students and Black freshmen were 4.5% at UC and 4.2% of those who met systemwide admission standards.
As a result, some Asian Americans are nervous that they would be squeezed out to make room for others if Proposition 16 passes and allows preferential admissions on the basis of race, ethnicity and sex.
Even back during the 1980s, when affirmative action was legal in California, Asian Americans were overrepresented at UCLA and UC Berkeley and fought admission policies they believed sought to limit their enrollment at those campuses.
After Proposition 209 banned race-based preferential treatment, the gap between their high admission rates and those of other ethnic groups widened with their rigorous high school course loads, high GPAs and competitive test scores.
It's not clear what would happen if affirmative action were restored. UC regents support restoring affirmative action, saying it's needed to fully diversify campuses but recently voted to ban quotas, which the U.S. Supreme Court already has nixed. Board Chairman John A. Perez has said that race would become one of more than a dozen factors currently evaluated in applications.
At UC Berkeley, Chancellor Carol Christ has said she favors expanding capacity to make more room for everyone.
UCLA Vice Provost for Enrollment Management Youlonda Copeland-Morgan told The Times last week that she didn't know if Asian American enrollment would decline at UCLA but that the campus would continue to focus on outreach to those from less represented communities, such as Hmong, Laotian, Vietnamese and Philippine students.
Last year, UCLA offered freshman seats to eight California Hmong students compared with 1,241 Chinese Americans, who were the single largest racial or ethnic group admitted after Mexican Americans and whites, according to UC data.
Some Asian Americans try to divine the future by looking to private California colleges, which were not bound by Proposition 209's ban on affirmative action at public campuses. They are not encouraged: the top privates enroll smaller proportions of Asian American undergraduates than the 33.5% at UC campuses, including 23% at Stanford University, 21% at USC and 16.3% at Pomona College.
But experts say the impact of Proposition 16 will probably vary among Asian American subgroups.
"You can't paint Asian Americans with a broad brush," said Karthick Ramakrishnan, a UC Riverside professor of public policy and political science who founded AAPIData.com, which publishes demographic data and policy research on Asian Americans and Pacific Islanders. "Some will benefit and others stand to lose."
Overall, more Asian Americans support Proposition 16 than oppose it -- 35% vs. 21% with most of the rest undecided, according to a September survey of Asian American voters in California by AAPI Data and two other Asian American and Pacific Islander organizations.
But intriguing differences emerged in the survey.
Ethnic Chinese -- who make up a maximum of 38% of Asian American students admitted by UC, by far the largest subgroup -- opposed the measure, 37% to 30%. Among ethnic Philippine, Vietnamese, Japanese and Korean voters, support was greater than opposition but topped out at 38% with large numbers undecided at the time.
Voters of Indian descent were by far the most supportive of the affirmative action ballot measure, 58% vs. 17%. The strong support among Indian Americans is all the more striking because their share among Asian American California freshmen has surged from 7.8% in 1996 to 17.4% in 2019.
Indian Americans had one of the highest admission rates among their California ethnic Asian peers last fall, about 79%. And South Asians, who include those of Indian, Pakistani, Bangladeshi and Sri Lankan descent, made up the second-largest Asian American subgroup after ethnic Chinese.
If any subgroup loses out under Proposition 16, Ramakrishnan said, it would probably be ethnic Chinese and Indian students because of their size. Nonetheless, many Indian Americans support Proposition 16 -- and other progressive political causes -- that reflect both homeland and domestic realities, he said. Their families have adapted to affirmative action in India, which is used to grant preferential access to many colleges in India based on caste, and are sensitive to race-based bias and discrimination as a darker-skinned minority in the United States, Ramakrishnan said.
Aidan Arasasingham, a South Asian high achiever and the son of Sri Lankan immigrants, is a UCLA senior in global studies and president of the UC Student Assn. He supports Proposition 16, as do the UC Student Assn. and most of the UCLA Asian Pacific Coalition's 19 member groups. Arasasingham said all students, Asian Americans included, will be enriched by a more diverse campus -- a point affirmed by research.
One synthesis of more than 500 studies found that greater cross-racial interaction is associated with lower levels of prejudice; other researchers have found that diversity spurs growth in cognitive skills, academic engagement and innovative problem solving, according to an October analysis by William C. Kidder for the Civil Rights Project at UCLA.
"Students ... across campuses really strongly believe that diversity is an essential part of our experience," Arasasingham said. "When we graduate, we're going to be in a world where we're engaging and interacting with communities that don't look like our own and it's setting us up for failure if we don't have an understanding of them."
Some Asian Americans, however, say their personal experiences have soured them on affirmative action.
Ling Kong, a Silicon Valley design engineer and mother of two school-age children, said her ethnic Chinese family faced bias in their native Malaysia because preferential college access is given to Malays and Indians. Her failure to get into local colleges there led her to study electrical engineering at Iowa and Arizona state universities.
"I don't want my kids to be treated differently on the basis of race," she said.
Kong, who describes herself as a liberal Democrat supportive of diversity and inclusion, said she opposes Proposition 16 because she doesn't believe it will solve educational inequity. Her experience on the Milpitas Unified School District's community advisory board and her children's school site council has shown her that affordable preschools and quality K-12 schools with adequate funding in all neighborhoods are needed, she said.
"Prop. 16 doesn't address any of this," she said. "It's just a quick fix and it won't really help close the achievement gap or solve systemic inequity."
For state Assemblyman Al Muratsuchi (D-Rolling Hills Estates), the debate sparks a sense of deja vu. As a UC Berkeley student in the 1980s, he joined protests over changes in campus admission policies, such as increasing the weight of verbal over math skills, that were seen by many Asian Americans as efforts to limit their surging enrollment.
Reviews of such allegations at Berkeley and UCLA by faculty, state and federal authorities generally found that some admission policies had hurt Asian Americans but were not intentionally discriminatory.
Muratsuchi supports Proposition 16 but says he will watch to make sure that a de facto ceiling is not placed on Asian American enrollment if it passes.
In what is supposed to be a race-blind environment today, he said, UC data suggest different standards are used for Asian Americans, who were admitted into the fall 2020 California freshman class with SAT scores up to 310 points higher than those for underrepresented minority students and up to 80 points higher than those for whites. UC, however, is eliminating the use of standardized test scores in admissions decisions.
"While I support the concept of affirmative action, we need to make sure there is transparency and accountability in how it is implemented so this complicated racial balancing act doesn't result in discriminating against any racial groups," he said.

Graphic

 
PHOTO: AMY HO, a student at UCLA, supports Proposition 16. More Asian Americans support the measure than oppose it, but significant numbers are undecided.  PHOTOGRAPHER:Carolyn Cole Los Angeles Times PHOTO: ASIAN AMERICAN students rally last month on the UCLA campus in support of Proposition 16, which would repeal the statewide ban on affirmative action.  PHOTOGRAPHER:Carolyn Cole Los Angeles Times 

Load-Date: November 1, 2020


End of Document


AUTHORS MAKE A CASE FOR THE 'ART OF SKEPTICISM' UW profs push for more critical thinking in our data-rich world
Spokesman Review (Spokane, WA)
August 20, 2020 Thursday
Main Edition


Copyright 2020 Spokane Spokesman-Review
Section: C; Pg. 005
Length: 710 words
Byline: Stephanie Hammett The Spokesman-Review
Body


Carl Bergstrom and Jevin West, authors of "Calling Bullshit: The Art of Skepticism in a Data-Driven World," discussed their book with Shawn Vestal in a Northwest Passages Book Club livestream event Tuesday night. Bergstrom and West explained the origins of the project and gave advice on how to "call bullshit on lies, treachery, trickery, or injustice."
 Opening the discussion, Bergstrom, a theoretical and evolutionary biologist and professor at the University of Washington, explained how he and West, a data scientist responsible for designing and implementing UW's data science curriculum, began their collaboration.
 "A few years ago he (West) called me up and said, 'Hey, I'm going to be teaching a course on big data,' and I said 'I want to teach a course about calling bullshit on big data,'" Bergstrom said.
 The college course resulting from that conversation would become, according to West, the most important course that they could teach their students.
 "When I think about all the classes that I've taught and all the classes I took as a student, I think this is one of those things that probably should be the first thing a student takes when they get here but probably before they get to the university, and maybe the last thing," West said. He isn't concerned about students memorizing the Krebs cycle or specific dates or the ideal gas law, "but I do care about whether they (students) can look critically at any argument - arguments that come in terms of data or in any form."
 They explained that different generations display different abilities when it comes to detecting "bullshit." For example, millennials and younger students can more easily identify clickbait websites than older generations, but they may struggle when it comes to identifying whether an article uses legitimate scientific sources.
 "The source of BS that you need to be most concerned about is yourself," Bergstrom said, recalling a Neil Postman quote from the 1960s about confirmation bias.
 "As we get a little older, and ... set in our ways ... we get a little bit better at defending our beliefs and arguments. We become more and more set because we can always find ways to convince ourselves that the thing we believe before we started looking into it is true."
 For this reason, the book stresses the importance of avoiding confirmation bias by trying to be at least as critical, if not more critical of the stories and data that support one's preexisting beliefs, as one would be of stories that oppose.
 The book was completed before the rise of COVID-19, but, "You could rewrite the book using only examples from the pandemic," Bergstrom said.
 They discussed the influence of social media on confirmation bias at length.
 "Social media has paved the way for bullshit," Bergstrom said. "Instead of having professional producers and editors determine what content I see, I've got my Uncle Rob and he's a conspiracy theorist. He doesn't really even care whether it's true or not - we all have these people in our lives. And so then because we are all taking on this editorial role, this kind of curation role fundamentally changes the quality of information that people are receiving."
 Newsfeed algorithms on platforms like Facebook, they explained, are ordered towards making sure that eyes stay glued to screens so users look at ads; whatever information happens to be keeping users attention is immaterial.
 "So you see things you want to hear and what your friends want you to hear, regardless of the truth. If it's a conspiracy theory that keeps you there longer, that's what you're going to see."
 The phenomenon poses "a big problem for democracy," Bergstrom said. "People need to make informed decisions based on true information about the world. But that also requires us to be able to recognize true information."
 Bergstrom and West offered several pieces of advice on beginning to approach information critically. First, Bergstrom said, "ask who's telling me this, how do they know it, and what are they trying to sell me?"
 "If something sounds too good or too bad to be true, it's probably not true," West said. "Be very wary of emotion-evoking clickbait."
 "Calling Bullshit: The Art of Skepticism in a Data-Driven World" is available at Auntie's Bookstore.

Notes

Stephanie Hammett can be reached at (509) 459-5013 or at stephanieh@spokesman.com


Load-Date: February 3, 2021


End of Document


Amazon lists 37,000-plus jobs Company's global workforce grew by 150,500 people in 2019
Spokesman Review (Spokane, WA)
February 17, 2020 Monday
Main Edition


Copyright 2020 Spokane Spokesman-Review
Section: C; Pg. 001
Length: 1035 words
Byline: Benjamin Romano Seattle Times
Body


At one point last week, Amazon had about 37,200 job listings around the world.
 That's the most listings the Seattle company has posted on its Amazon.jobs career site in at least 15 months, a company spokesperson said - and may be the most ever, though a system update prevents easy comparisons to earlier periods.
 Amazon is seeking everyone from hourly warehouse workers to top-paid machine-learning experts, underscoring the breadth and scope of the company's operations and ambition. The listings provide a rough map of Amazon's near-term growth priorities across businesses and geographies.
 Amazon's ongoing hiring follows a year when its global workforce grew by 150,500 people, or more than 23%. It finished 2019 with 798,000 full- and part-time employees, not including contractors.
 In the United States, where the company has more than 500,000 employees, continued hiring comes against a national backdrop of strong demand for labor. Nationwide, open positions outnumber job seekers, but openings fell to a still-strong 6.4 million at the end of 2019, down nearly 15% from a year earlier and the second consecutive month of declines, according to the U.S. Bureau of Labor Statistics.
 Amazon's labor needs, particularly for in-demand technology professionals, can boggle the mind. The company had more than 10,600 jobs listings in software development, the largest single category, with an additional 7,400 listings in adjacent roles such as IT engineering and data science.
 Amazon was seeking to fill more than 4,500 project- or program-management roles. It had nearly 2,050 openings in sales, advertising and account management. There were nearly 1,800 listings in fulfillment and operations management. More than 1,600 jobs were listed in human resources, including for recruiters whose job would be recruiting more recruiters.
 "I don't think most people, even people who have worked at a large, behemoth corporation like I did at AT&T, can even fathom the idea of having that many open professional positions at one time," said Brent Heslop, who now heads the business-transformation practice in the Seattle office of consulting firm Mercer.
 Amazon's main headquarters city, where it already has more than 50,000 employees, remains the location of the plurality of job listings, and by a large margin. There were nearly 11,500 openings listed in Seattle.
 The No. 2 city was Bangalore, India, with 1,430 job listings at the time of the Tuesday snapshot. Vancouver, B.C., had 973 listings, followed by London (906). Arlington, Virginia, where Amazon is building its second headquarters, had 517 openings, while nearby Herndon had 898. New York City, the abandoned HQ2 locale, is still a growing Amazon hub. It had 864 job listings.
 Bellevue, where Amazon intends to grow to 15,000 employees in the next few years, had more than 700 openings.
 The unemployment rate in King County was 2.1% in December, not adjusting for seasonal trends. That's the lowest level measured going back to 1990, when the Washington Employment Security Department began tracking local area unemployment the way it does now, said Anneliese Vance-Sherman, regional labor economist with the state agency.
 An unemployment rate that low can be challenging for many businesses, which may struggle to attract or retain as many workers as they need. "Workers have choices," she said. "There's a lot of demand from other businesses."
 But it's a tale of two job markets in the Seattle area. For tech giants such as Amazon and Microsoft, and their Silicon Valley-based competitors with large and growing offices in the region, the low local unemployment rate is less of a concern.
 "Scale matters," Vance-Sherman said. "They are able to draw from Boston, New York, San Francisco, Mumbai, Beijing. They aren't as constrained in a manner of speaking because they're not exclusively drawing from the local labor market."
 Heslop said the brand cachet and competitive compensation offered by major tech employers help them find applicants, even in a tight labor market.
 "My assumption is that Amazon doesn't have to work as hard to source or attract candidates as the rest of us do," he said.
 Pay at Amazon starts at a minimum of $15 an hour, the wage floor it instituted for U.S. workers in 2018, and goes up to $160,000 a year for even very senior employees with only a few exceptions, though stock awards boost total compensation far higher. Median pay in the United States during 2018 was about $35,100, the company reported in its most recent proxy statement.
 The wide range in compensation underscores the broad array of businesses and roles within Amazon.
 Its lucrative and fast-growing cloud-computing business, Amazon Web Services, accounted for 13,000 openings, or more than a third of all the postings. The next closest category was Amazon's retail business, which had 2,790 openings for roles including a data scientist in Seattle to help automate research into the complexities of Amazon's economy and a financial analyst to support a team in Shenzhen, China, sourcing products for Amazon's growing private-brands business.
 Along with nearly 2,000 listings in fulfillment and operations, Amazon's sprawling logistics business, the company had 1,740 listings in operations technology. Roles included a New York-based workers-compensation-claims analyst to support efforts to reduce accidents and injuries among Amazon's third-party delivery partners and an engineer in Bellevue to evaluate battery and electric vehicles technologies to be used in its delivery operations.
 Amazon Alexa, the company's voice computing system, had 1,820 job listings. Amazon Devices, which makes Kindle readers, Echo Dot microphones and speakers, and Fire tablets and streaming media players, had 1,720 listings.
 Analyzing the job listings doesn't reveal the whole story of the company's voracious appetite for new workers. Some job listings, particularly for hourly positions in warehousing and logistics, where the majority of Amazon's employees work, advertise multiple openings. Others tout upcoming hiring fairs, like the ones Amazon held in Seattle and five other U.S. cities last fall as part of a push to fill 30,000 positions nationwide.
 Amazon

Load-Date: February 17, 2020


End of Document


Eastern and Western are economic and social engines
Spokesman Review (Spokane, WA)
March 17, 2019 Sunday
Main Edition


Copyright 2019 Spokane Spokesman-Review
Section: E; Pg. 003
Length: 707 words
Byline: Mary Cullinan and Sabah Randhawa
Body


Guest Opinion
 Eastern Washington University and Western Washington University anchor both sides of our state. While each university is distinctive, our mission and values are similar. Founded as colleges for teachers in the 19th century, our universities have evolved into institutions that transform the lives of students and work with employers and other partners to serve our growing state economy.
 Each year as we graduate thousands of students, most of them Washingtonians, our universities help to ensure that our state continues to thrive with a knowledgeable, professional workforce and an informed citizenry. Our universities are significant engines for the economy and the well-being of the state.
 Sadly, many people continue to argue that "not everyone needs to go to college." However, the degrees our students achieve benefit not only the graduates themselves but also their families, employers, and entire communities that rely on a professional workforce in order to thrive and grow.
 According to the state of Washington's Employment Security Department, 11 of the top 25 occupations in the state require a four-year degree. And all of those are by far the highest-paying jobs on the top 25 list, ranging from $72,000 to more than $126,000 a year: jobs such as software developers, computer science professionals, managers, medical and health professionals, engineers and research analysts.
 Nationwide, university graduates earn nearly 68 percent more than people with only a high school degree, and 42 percent more than those with a two-year degree. Moreover, according to the U.S. Bureau of Labor Statistics, not only do earnings increase but unemployment numbers decrease significantly for individuals with baccalaureate degrees.
 Our two universities work to ensure that our degree offerings align with student demand as well as state needs. Eastern, for instance, offers the Microsoft Professional Program in Data Science in conjunction with our baccalaureate degree in Data Analytics, to help meet a massive shortage of data managers and analysts. Western's Manufacturing and Supply Chain Management program ranks in the top 10 nationally - ahead of Stanford and MIT - and offers students hands-on learning partnerships with Boeing, Microsoft, Starbucks, PACCAR, REI, Amazon, Crane Aerospace and other leading companies.
 Another argument we often hear is "the cost of college isn't worth it." At under $7,000 per year, tuition at our institutions is less than at most universities. Since costs are lower, our students tend to borrow less than do students at more expensive universities. Almost half our students graduate with no debt. Students who do take out loans incur an average debt of $23,000, compared  with a national average of $31,000. Both our institutions, also, offer a wide array of scholarships and financial aid options.
 In a short time, a university degree from EWU or WWU more than pays for itself. But gaining a baccalaureate degree is not just about earnings. Our students gain critical thinking, teamwork and communication skills. They gain flexibility in their life choices. They are more easily able to change careers and change locations. They are often physically healthier and have longer life expectancies than people with only a high school diploma. They don't rely on the state's support services to the same extent as people without a college degree. And the children of college graduates are more apt to graduate from college and continue the family's success into the next generations.
 Every spring, our two universities graduate more than 7,000 students who enter the state's professional workforce as engineers, health and business professionals, computer scientists, teachers and civic leaders. They pay taxes and contribute to the social and cultural life as well as the economic vitality of our communities.
 Every year, several hundred thousand EWU and WWU alumni more than repay the state's investment in their success. Together, our universities are economic and social engines, crucial for the prosperity of our state and the vibrancy of our local communities.
 Mary Cullinan is president of Eastern Washington University. Sabah Randhawa is president of Western Washington University.

Load-Date: March 17, 2019


End of Document


Woman dies when car crashes into tree
Spokesman Review (Spokane, WA)
May 25, 2018 Friday
Main Edition


Copyright 2018 Spokane Spokesman-Review
Section: C; Pg. 004
Length: 431 words
Body


In brief
 A woman died early Thursday in a one-car crash in Otis Orchards.
 Emergency crews were called at about 5:30 a.m. to Wellesley Avenue and Ashton Road, said Spokane County Sheriff's Deputy Mark Gregory.
 Deputies said the  woman, driving a 1999  Honda Accord, crossed the centerline and crashed into a tree near the westbound shoulder. The person who reported the crash noticed a vehicle  in the front yard of a house.
 The woman was not wearing a seat belt. Investigators believe speed and impairment may have been factors.
 Deputies said initial information indicates the woman left the area of Barker Road and Trent Avenue sometime between midnight and 3 a.m. and traveled east on Wellesley, where she crashed.
 Anyone with information is asked to call Detective Jeff Welton at (509) 477-3237.
EWU heralds partnerships with businesses
  Eastern Washington University announced a series of initiatives and celebrated recent partnerships with businesses in the Spokane area at a ceremony Thursday.
 The university's partnership with Microsoft, for instance, is part of the school's heavy focus on developing its STEM programs.
 Through it, students at EWU can certify in Microsoft's data science program as part of a  bachelor's degree in data analytics. The Microsoft certification currently has a  10 percent completion rate.
 The university also celebrated its partnership with Avista to build the zero net energy Catalyst Building in Spokane that will house computer science, electrical engineering and visual communication programs.
 EWU President Mary Cullinan said  EWU especially wants to work with and accommodate  the needs of businesses in the region.
 "If businesses have needs, we want to be able to fill those," she said.
Nonprofit raises $317,000 for area programs
 A local nonprofit raised over $317,000 in donations, and the grant money will go to 17 local programs, including child care nutrition programs, emergency foster care and women's educational assistance.
 The nonprofit, Women Helping Women Fund, gathered 1,650 people at an event  Tuesday aimed at raising the funds, according to a news release. The guest speaker, Dr. Tererai Trent, a scholar and humanitarian, has been featured on "The Oprah Winfrey Show."
 The programs will receive grants ranging from $10,000 to $29,500. Some of the recipients are The Salvation Army, Northeast Youth Center, Women & Children's Free Restaurant & Community Kitchen, and Young Life.
 The Women Helping Women Fund started in 1992 to raise awareness about the plight of women and children living in poverty in Spokane.
 From staff reports

Load-Date: May 25, 2018


End of Document


Big Data Analysis Identifies New Cancer Risk Genes









Computer Searches Telescope Data For Evidence Of Distant Planets









Natural Product Could Lead To New Class Of Commercial Herbicide









Soil Fungi May Help Determine Resilience Of Forests To Environmental Change









Tall And Older Amazonian Forests More Resistant To Droughts









UN Chief Asks Melinda Gates And Ali Baba's Jack Ma To Head Digital Experts Panel









What Makes A Bestseller: Fiction, Thriller And A Christmas Release









Why Some Older People Are Rejecting Digital Technologies









Climate Change Is Already Affecting Global Food Production (Unequally)









Covert Action As An Intelligence Subcomponent Of The Information Instrument – Analysis









Keeping Children Safe In The 'Internet Of Things' Age









Querying Big Data Just Got Universal









Too Many Businesses Failing To Properly Embrace AI Into Processes, Not Reaping Benefits









Artificial Intelligence Can Predict Premature Death









New Satellite Keeps Close Watch On Antarctic Ice Loss









Sri Lanka: Rapidly Ageing Population, Opportunity For Business Expansion









How Healthy Will We Be In 2040?









An AI For Deciphering What Animals Do All Day









Helping Discover The Diversity In Soil









Arab Spring Success Story: Tunisians Vote – Analysis









Artificial Intelligence In Military Operations: Where Does India Stand? – Analysis









Use Of Increasingly Advanced Analytics May Present Crucial Way For P&C Insurers To Unlock Tangible Value









Why Should You Care About AI Used For Hiring?









Why Urban Planners Should Pay Attention To Restaurant-Review Sites









Artificial Intelligence On The Battlefield: Implications For Deterrence And Surprise – Analysis









New Map Of Milky Way Reveals Giant Wave Of Stellar Nurseries









Revealed The Experts And Public's Attitude Towards Gene-Edited Crops









Who's Liable? The AV Or The Human Driver?









Facebook, Cambridge Analytica And Surveillance Capitalism - OpEd









Surface Clean-Up Technology Won't Solve Ocean Plastic Problem









More Evidence Of Causal Link Between Air Pollution And Early Death









There's No End In Sight To The Zombie Economy – OpEd









Limits To Strategic Foresight: Try Wisdom Of The Crowds – Analysis









Baseball Illustrates Economics With Each Game – OpEd









More Than A Lifetime Away: World Faces 100-Year Wait For Gender Parity









Music Is Universal









Robots, Smart Helmets Deployed In Coronavirus Fight









Consider Workplace AI's Impact Before It's Too Late









Irish And UK Research Helps To Unravel Secrets Behind Game Of Thrones









Spain Using Mobile Phone Data To Study Efficacy Of Lockdown On Spread Of COVID-19









Artificial Intelligence Can Predict Students' Educational Outcomes Based On Tweets









Knowing The Model You Can Trust: The Key To Better Decision-Making









Climate Signals Detected In Global Weather









Police Stop Fewer Black Drivers At Night When 'Veil Of Darkness' Obscures Their Race









Model Beats Wall Street Analysts In Forecasting Business Financials









March Madness Bracket Analysis Shows Picking Final Four First Leads To Better Brackets









Influencing Electoral Outcomes: The Ugly Face Of Facebook - Analysis









How Climate Change Affects Crops In India









A COSMIC Approach To Nanoscale Science









Artificial Intelligence May Help Achieve UN's Sustainable Development Goals









Study Claims Reparations For Slavery Could Have Reduced COVID-19 Infections And Deaths In US









Global Ice Loss Increases At Record Rate









Forecasting Coastal Water Quality









Climate Change To Alter Position Of Earth's Tropical Rain Belt









Big Data To Analyze The Mystery Of Beethoven's Metronome









Understanding How Birds Respond To Extreme Weather Can Inform Conservation Efforts









The Danger Of Weaponizing Trade For The Environment – Analysis









Forecasting Urbanization









LOWER FEES, BETTER TECH?; NEW VOICES ARE VYING FOR SPACE IN THE FOOD-DELIVERY WORLD









How to erase the digital divide









CORONAVIRUS IN CALIFORNIA; We don't know the half of it; Did 50% of L.A. residents catch COVID-19? It all depends.









CITY BEAT; A tail-wagging tale of man and corgi









In the West, wildfire smoke accounts for more pollution; Air quality declines again after years of steady gains, study finds









Back to the drawing board, maps









Asian Americans split on affirmative action at UC; Proposition 16 divides university system's most overrepresented group of students.









AUTHORS MAKE A CASE FOR THE 'ART OF SKEPTICISM' UW profs push for more critical thinking in our data-rich world









Amazon lists 37,000-plus jobs Company's global workforce grew by 150,500 people in 2019









Eastern and Western are economic and social engines









Woman dies when car crashes into tree







